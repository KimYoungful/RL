{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60538f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import random\n",
    "import pygame\n",
    "\n",
    "# --- Constants ---\n",
    "SCREEN_WIDTH = 1200\n",
    "SCREEN_HEIGHT = 800\n",
    "FPS = 60\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (220, 50, 50)\n",
    "GREEN = (50, 220, 50)\n",
    "BLUE = (50, 50, 220)\n",
    "GRAY = (150, 150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045bbf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Episode 1 ---\n",
      "--- Episode 2 ---\n",
      "--- Episode 3 ---\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Define colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)  # Color for the trajectory\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.obstacle_num = 8\n",
    "        self.distance_threshold_goal = 0.3  # Tolerance for goal proximity\n",
    "        self.distance_threshold_obstacle = 1  # Tolerance for obstacle proximity\n",
    "        self.steps = 0\n",
    "\n",
    "        self.pre_distance = 0  # Previous distance to goal, used for reward shaping\n",
    "        self.current_distance = 0  # Current distance to goal, used for reward shaping\n",
    "        # Action space (dx, dy)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Observation space (robot_x, robot_y, goal_x, goal_y)\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size, shape=(4,), dtype=np.float32)\n",
    "\n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 50 # Pixels per grid unit\n",
    "        self.trajectory_points = [] # New: List to store past robot positions\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate((self.robot_position, self.goal_position))\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance_to_goal\": np.linalg.norm(self.robot_position - self.goal_position)\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.robot_position = np.random.uniform(0, self.grid_size, size=2)\n",
    "        self.goal_position = np.random.uniform(0, self.grid_size, size=2)\n",
    "        self.obstacle_position = [np.random.uniform(0, self.grid_size, size=2) for _ in range(self.obstacle_num)]\n",
    "        self.steps = 0\n",
    "        self.trajectory_points = [self.robot_position.copy()] # New: Reset trajectory and add initial position\n",
    "        self.pre_distance = np.linalg.norm(self.robot_position - self.goal_position)\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _reward(self):\n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.goal_position)\n",
    "        reward = (self.pre_distance - self.current_distance)*50  # Reward shaping based on distance change\n",
    "        self.pre_distance = self.current_distance\n",
    "        # penalize\n",
    "        for obs in self.obstacle_position:\n",
    "            if np.linalg.norm(self.robot_position - obs) < self.distance_threshold_obstacle:  # If too close to an obstacle\n",
    "                reward -= 75  # Add a penalty\n",
    "                # print(f\"Collision with obstacle at {obs}, increasing distance penalty.\")\n",
    "                break   \n",
    "        \n",
    "\n",
    "        # Check if the robot is close enough to the goal\n",
    "        if np.linalg.norm(self.robot_position - self.goal_position) < self.distance_threshold_goal:\n",
    "            reward += 100\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        stride = 0.5  # Base stride length\n",
    "        self.robot_position += action * stride  # Scale the action to control speed\n",
    "        self.robot_position = np.clip(self.robot_position, 0, self.grid_size)\n",
    "\n",
    "        self.trajectory_points.append(self.robot_position.copy()) # New: Add current position to trajectory\n",
    "\n",
    "        reward = self._reward()\n",
    "        # Consider a small tolerance for checking if robot_position is \"equal\" to goal_position\n",
    "        terminated = np.linalg.norm(self.robot_position - self.goal_position) < self.distance_threshold_goal\n",
    "        truncated = False\n",
    "\n",
    "        info = self._get_info()\n",
    "        observation = self._get_obs()\n",
    "        self.steps += 1\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (int(self.grid_size * self.cell_size), int(self.grid_size * self.cell_size))\n",
    "            )\n",
    "            pygame.display.set_caption(\"CustomEnv\")\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                import sys\n",
    "                sys.exit() # Exit the program\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if event.button == 1: # Left mouse button click\n",
    "                    mouse_x, mouse_y = event.pos # Get mouse position in pixels\n",
    "                    # Convert pixel coordinates to grid coordinates\n",
    "                    new_goal_x = mouse_x / self.cell_size\n",
    "                    new_goal_y = mouse_y / self.cell_size\n",
    "                    self.goal_position = np.array([new_goal_x, new_goal_y], dtype=np.float32)\n",
    "                    print(f\"New goal set at grid position: {self.goal_position}\")\n",
    "\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        canvas.fill(WHITE)\n",
    "        virus_image = pygame.image.load(\"病毒.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "        robot_image = pygame.transform.scale(virus_image, (int(self.cell_size * 0.8), int(self.cell_size * 0.8)))  # Scale the image\n",
    "        # New: Draw the trajectory\n",
    "        if len(self.trajectory_points) > 1:\n",
    "            scaled_points = []\n",
    "            for point in self.trajectory_points:\n",
    "                scaled_points.append((int(point[0] * self.cell_size), int(point[1] * self.cell_size)))\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            pygame.draw.lines(canvas, BLUE, False, scaled_points, 2) # Blue line, not closed, 2 pixels wide\n",
    "            \n",
    "            # Optionally, draw small circles at each point to emphasize\n",
    "            for point_coord in scaled_points:\n",
    "                pygame.draw.circle(canvas, BLUE, point_coord, 3) # Small blue circles\n",
    "\n",
    "        # Draw robot\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            RED,\n",
    "            (int(self.robot_position[0] * self.cell_size), int(self.robot_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.2)\n",
    "        )\n",
    "        # Draw obstacles\n",
    "        for obs in self.obstacle_position:\n",
    "            # pygame.draw.circle(\n",
    "            #     canvas,\n",
    "            #     GRAY,\n",
    "            #     (int(obs[0] * self.cell_size), int(obs[1] * self.cell_size)),\n",
    "            #     int(self.cell_size * 0.5)\n",
    "            # )\n",
    "            canvas.blit(robot_image, (int(obs[0] * self.cell_size), int(obs[1] * self.cell_size)))\n",
    "        # Draw goal\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            GREEN,\n",
    "            (int(self.goal_position[0] * self.cell_size), int(self.goal_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.3)\n",
    "        )\n",
    "\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == '__main__':\n",
    "    env = CustomEnv()\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    for episode in range(3): # Run a few episodes to see trajectory resets\n",
    "        print(f\"--- Episode {episode + 1} ---\")\n",
    "        obs, info = env.reset() # Reset environment and clear trajectory\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done and step_count < 200: # Limit steps per episode\n",
    "            action = env.action_space.sample() * 0.5 # Take a random action, scaled down for slower movement\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            env.render() # Display current frame with trajectory\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "                print(f\"Episode finished after {step_count} steps. Total reward: {total_reward:.2f}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670f1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a375fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[237], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m policy_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m      7\u001b[0m     net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mdict\u001b[39m(pi\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m], vf\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m])],\n\u001b[0;32m      8\u001b[0m     activation_fn\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU  \u001b[38;5;66;03m# 改为 ReLU，通常更适合稀疏奖励\u001b[39;00m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs,tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_custom_env_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_env()\n\u001b[0;32m     17\u001b[0m obs \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    302\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    309\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 311\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:432\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_logger:\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[0;32m    435\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_callback(callback, progress_bar)\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\utils.py:202\u001b[0m, in \u001b[0;36mconfigure_logger\u001b[1;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    199\u001b[0m save_path, format_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to log data to tensorboard but tensorboard is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     latest_run_id \u001b[38;5;241m=\u001b[39m get_latest_run_id(tensorboard_log, tb_log_name)\n",
      "\u001b[1;31mImportError\u001b[0m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[64, 128], vf=[128, 256])],\n",
    "    activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs,tensorboard_log=\"./ppo_custom_env_tensorboard/\")\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=500000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "# for i in range(1000):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, dones, info = vec_env.step(action)\n",
    "    # vec_env.render()\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3755f724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "831e9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.94845784 9.2430564  5.05651407 1.34419754]\n",
      "[-1. -1.] 7.7758927148843675\n",
      "[-1. -1.] 7.153579864232719\n",
      "[-1. -1.] 6.548503816485092\n",
      "[-1. -1.] 5.965911463899196\n",
      "[-1. -1.] 5.4130672410055904\n",
      "New goal set at grid position: [5.96 0.36]\n",
      "[-1.        -0.9270272] 6.005347975585182\n",
      "[-0.9417341  -0.96099925] 5.637439804249912\n",
      "[-0.70692027 -0.841851  ] 5.343399771780247\n",
      "[-0.5242584 -0.7220329] 5.107855669041954\n",
      "[-0.38124934 -0.5993341 ] 4.921892781592191\n",
      "[-0.27338487 -0.48949495] 4.774552089583291\n",
      "[-0.18730712 -0.40951082] 4.649422826425122\n",
      "[-0.12369078 -0.34163368] 4.541654786013297\n",
      "[-0.07939956 -0.2819771 ] 4.449631102906388\n",
      "[-0.05550933 -0.2384766 ] 4.370592196053318\n",
      "[-0.04085033 -0.2003997 ] 4.303945723502053\n",
      "[-0.02986712 -0.16882241] 4.247463090172077\n",
      "[-0.02216529 -0.1430105 ] 4.199384696228254\n",
      "[-0.01781329 -0.11984728] 4.1593873304458295\n",
      "[-0.0153974  -0.09951131] 4.126757375959005\n",
      "[-0.01326987 -0.08264637] 4.100089806207518\n",
      "[-0.01181624 -0.06861416] 4.078410801555416\n",
      "[-0.01105777 -0.05688451] 4.061000274430901\n",
      "[-0.01135851 -0.04605904] 4.047812642984789\n",
      "[-0.01113638 -0.03778425] 4.037683250609167\n",
      "[-0.01061551 -0.03142962] 4.029772017273568\n",
      "[-0.00993958 -0.02620943] 4.023586808929787\n",
      "[-0.00922191 -0.02190143] 4.018762490975261\n",
      "[-0.00845501 -0.01834659] 4.014994670135007\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda\\envs\\RL\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "obs,_ = env.reset()\n",
    "print(obs)\n",
    "for i in range(1000000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, teminated,_, info = env.step(action)\n",
    "    print(action,info[\"distance_to_goal\"])\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.6)  # Control the frame rate\n",
    "    if teminated:\n",
    "        env.close()\n",
    "        break\n",
    "        print(\"Resetting environment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
