# 快速重构指南

本文档提供了快速开始重构的步骤和最小化示例代码。

## 当前项目问题总结

### 主要问题
1. **代码重复**：存在多个版本的CustomEnv实现
2. **配置分散**：参数硬编码在代码中
3. **结构混乱**：多个目录层级，文件组织不清晰
4. **缺少模块化**：功能耦合严重
5. **缺少类型提示和文档**

### 影响
- 难以维护和扩展
- 难以复现实验结果
- 难以进行单元测试
- 代码质量不高

## 重构优先级

### 高优先级（立即执行）
1. ✅ **统一环境实现**：选择一个最佳版本，删除其他重复代码
2. ✅ **配置外置**：将硬编码参数提取到配置文件
3. ✅ **项目结构整理**：创建清晰的目录结构

### 中优先级（短期内完成）
4. ✅ **模块化重构**：分离环境、训练、部署模块
5. ✅ **添加类型提示**：提高代码可读性和可维护性
6. ✅ **创建训练脚本**：从notebook迁移到Python脚本

### 低优先级（长期改进）
7. ✅ **添加单元测试**：提高代码质量
8. ✅ **完善文档**：API文档和使用示例
9. ✅ **CI/CD集成**：自动化测试和部署

## 最小化重构方案

如果你想要快速改善代码质量，可以按照以下最小化方案进行：

### 步骤1：创建配置类（1小时）

创建 `src/env/config.py`：

```python
from dataclasses import dataclass
from typing import Tuple, Optional
import yaml
import os

@dataclass
class EnvConfig:
    """环境配置类 - 最小化版本"""
    # 基础参数
    grid_size: float = 10.0
    margin: float = 0.3
    max_steps: int = 50
    
    # 距离阈值
    distance_threshold_penalty: float = 5.0
    distance_threshold_collision: float = 1.5
    distance_threshold_arm: float = 3.0
    
    # 奖励参数
    penalty_factor: float = 5.0
    distance_reward_factor: float = 2.0
    smooth_action_penalty: float = 2.0
    reward_arm: float = -100.0
    reward_hand: float = -100.0
    reward_bound: float = -200.0
    reward_max_step: float = 200.0
    reward_step: float = 10.0
    
    # 随机化参数
    stride_robot_range: Tuple[float, float] = (1.0, 3.0)
    stride_hand_range: Tuple[float, float] = (0.6, 1.0)
    hand_move_epsilon: float = 0.1
    
    # 噪声参数
    noise_obs_sigma_range: Tuple[float, float] = (0.0, 0.1)
    noise_action_sigma_range: Tuple[float, float] = (0.0, 0.1)
    
    # 域随机化
    enable_domain_randomization: bool = True
    
    @classmethod
    def from_dict(cls, config_dict: dict) -> 'EnvConfig':
        """从字典创建配置"""
        return cls(**config_dict)
    
    @classmethod
    def from_yaml(cls, yaml_path: str) -> 'EnvConfig':
        """从YAML文件加载配置"""
        with open(yaml_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        return cls.from_dict(config_dict)
    
    def to_dict(self) -> dict:
        """转换为字典"""
        return {
            field.name: getattr(self, field.name)
            for field in self.__dataclass_fields__.values()
        }
    
    def save_yaml(self, yaml_path: str):
        """保存到YAML文件"""
        os.makedirs(os.path.dirname(yaml_path), exist_ok=True)
        with open(yaml_path, 'w') as f:
            yaml.dump(self.to_dict(), f, default_flow_style=False)
```

### 步骤2：重构环境类使用配置（2小时）

修改现有的 `CustomEnv` 类，使用 `EnvConfig`：

```python
# src/custom_env.py (重构后)
import gymnasium as gym
from gymnasium.spaces import Box
import numpy as np
import random
from typing import Optional, Tuple, Dict, Any
from src.env.config import EnvConfig

class CustomEnv(gym.Env):
    """重构后的自定义环境 - 使用配置类"""
    
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 60}
    
    def __init__(
        self,
        config: Optional[EnvConfig] = None,
        render_mode: Optional[str] = None
    ):
        super().__init__()
        # 使用配置对象而不是硬编码
        self.config = config or EnvConfig()
        self.render_mode = render_mode
        
        # 初始化状态
        self.robot_position = np.zeros(2)
        self.hand_position = np.zeros(2)
        self.last_action = np.zeros(2)
        self.current_distance = 0.0
        self.pre_distance = 0.0
        self.steps = 0
        self.trajectory_points = []
        self.fixed_point = np.zeros(2)
        self.dist_arm = 0.0
        self.distance_history = []
        
        # 定义动作和观测空间
        self.action_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)
        self.observation_space = self._create_observation_space()
        
        # 渲染相关
        self.window = None
        self.clock = None
        self.cell_size = 50
        self.random = self.config.enable_domain_randomization
    
    def _create_observation_space(self) -> Box:
        """创建观测空间"""
        obs_shape = 2 + 2 + 2 + 1 + 1 + 1 + 2 + 1 + 1
        obs_high = np.array([
            self.config.grid_size * 2,
            self.config.grid_size,
            self.config.grid_size * 2,
            self.config.grid_size,
            1.0, 1.0,
            np.sqrt(2) * self.config.grid_size,
            0.5 * self.config.grid_size,
            self.config.distance_threshold_arm,
            self.config.grid_size * 2,
            self.config.grid_size,
            self.config.stride_robot_range[1],
            self.config.stride_hand_range[1]
        ], dtype=np.float32)
        return Box(low=0, high=obs_high, shape=(obs_shape,), dtype=np.float32)
    
    def reset(self, seed=None, options=None):
        """重置环境"""
        super().reset(seed=seed)
        
        # 重置状态
        self._reset_positions()
        self._reset_dynamics()
        self.steps = 0
        self.trajectory_points = [self.robot_position.copy()]
        self.distance_history = []
        
        return self._get_obs(), self._get_info()
    
    def _reset_positions(self):
        """重置位置"""
        margin = self.config.margin
        grid_size = self.config.grid_size
        
        self.robot_position = np.random.uniform(
            margin,
            [2 * (grid_size - margin), grid_size - margin],
            size=2
        )
        self.hand_position = np.random.uniform(
            margin,
            [2 * (grid_size - margin), grid_size - margin],
            size=2
        )
        self.fixed_point = np.array([
            grid_size * random.uniform(0.2, 1.8),
            grid_size
        ])
        self.current_distance = np.linalg.norm(
            self.robot_position - self.hand_position
        )
        self.pre_distance = self.current_distance
    
    def _reset_dynamics(self):
        """重置动力学参数"""
        if self.config.enable_domain_randomization:
            self.stride_robot = np.random.uniform(*self.config.stride_robot_range)
            self.stride_hand = np.random.uniform(*self.config.stride_hand_range)
            self.noise_obs_sigma = np.random.uniform(*self.config.noise_obs_sigma_range)
            self.noise_action_sigma = np.random.uniform(*self.config.noise_action_sigma_range)
        else:
            self.stride_robot = np.mean(self.config.stride_robot_range)
            self.stride_hand = np.mean(self.config.stride_hand_range)
            self.noise_obs_sigma = 0.0
            self.noise_action_sigma = 0.0
    
    # ... 其他方法类似，使用 self.config.xxx 代替硬编码值 ...
    
    def save_args(self, path: str):
        """保存环境参数"""
        import os
        self.config.save_yaml(os.path.join(path, "env_config.yaml"))
```

### 步骤3：创建简单的训练脚本（1小时）

创建 `train_simple.py`：

```python
"""简单的训练脚本"""
import argparse
from pathlib import Path
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback
from src.custom_env import CustomEnv
from src.env.config import EnvConfig

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config/env_config.yaml")
    parser.add_argument("--log-dir", type=str, default="logs/training")
    parser.add_argument("--total-timesteps", type=int, default=400000)
    args = parser.parse_args()
    
    # 加载配置
    env_config = EnvConfig.from_yaml(args.config)
    
    # 创建环境
    def make_env():
        env = CustomEnv(config=env_config)
        return Monitor(env)
    
    env = DummyVecEnv([make_env])
    env = VecMonitor(env)
    
    # 创建模型
    model = SAC(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=args.log_dir + "/tensorboard",
        policy_kwargs=dict(net_arch=[128, 256, 256, 128])
    )
    
    # 创建回调
    eval_callback = EvalCallback(
        env,
        best_model_save_path=args.log_dir + "/best_model",
        log_path=args.log_dir,
        eval_freq=10000,
        deterministic=True,
        n_eval_episodes=10
    )
    
    # 保存配置
    Path(args.log_dir).mkdir(parents=True, exist_ok=True)
    env_config.save_yaml(args.log_dir + "/env_config.yaml")
    
    # 训练
    model.learn(
        total_timesteps=args.total_timesteps,
        callback=eval_callback
    )
    
    print(f"训练完成！模型保存在: {args.log_dir}")

if __name__ == "__main__":
    main()
```

### 步骤4：创建配置文件（30分钟）

创建 `config/env_config.yaml`：

```yaml
grid_size: 10.0
margin: 0.3
max_steps: 50

distance_threshold_penalty: 5.0
distance_threshold_collision: 1.5
distance_threshold_arm: 3.0

penalty_factor: 5.0
distance_reward_factor: 2.0
smooth_action_penalty: 2.0
reward_arm: -100.0
reward_hand: -100.0
reward_bound: -200.0
reward_max_step: 200.0
reward_step: 10.0

stride_robot_range: [1.0, 3.0]
stride_hand_range: [0.6, 1.0]
hand_move_epsilon: 0.1

noise_obs_sigma_range: [0.0, 0.1]
noise_action_sigma_range: [0.0, 0.1]

enable_domain_randomization: true
```

## 使用重构后的代码

### 训练模型

```bash
python train_simple.py --config config/env_config.yaml --log-dir logs/exp1
```

### 加载模型

```python
from src.custom_env import CustomEnv
from src.env.config import EnvConfig
from stable_baselines3 import SAC

# 加载配置
config = EnvConfig.from_yaml("config/env_config.yaml")
env = CustomEnv(config=config)

# 加载模型
model = SAC.load("logs/exp1/best_model/best_model.zip", env=env)

# 使用模型
obs, info = env.reset()
action, _ = model.predict(obs, deterministic=True)
```

## 重构 checklist

### 第一阶段：基础重构（1-2天）
- [ ] 创建 `EnvConfig` 配置类
- [ ] 重构 `CustomEnv` 使用配置
- [ ] 创建配置文件 `env_config.yaml`
- [ ] 创建简单的训练脚本
- [ ] 测试训练流程

### 第二阶段：代码整理（1天）
- [ ] 删除重复的环境实现
- [ ] 整理项目目录结构
- [ ] 移动文件到正确位置
- [ ] 更新导入路径

### 第三阶段：功能完善（2-3天）
- [ ] 添加类型提示
- [ ] 添加文档字符串
- [ ] 改进错误处理
- [ ] 添加日志记录

## 预期收益

完成最小化重构后，你将获得：

1. **配置管理**：所有参数集中在配置文件中
2. **代码复用**：环境类可以在不同场景下复用
3. **易于实验**：通过修改配置文件快速尝试不同参数
4. **可维护性**：代码结构更清晰，易于维护
5. **可扩展性**：易于添加新功能和参数

## 下一步

完成最小化重构后，可以考虑：

1. 进一步模块化（分离环境、训练、部署）
2. 添加单元测试
3. 完善文档
4. 优化代码性能
5. 添加更多功能（如可视化、评估指标等）

## 注意事项

1. **备份代码**：在开始重构前，确保备份原有代码
2. **逐步迁移**：不要一次性改变所有代码，逐步迁移更安全
3. **测试验证**：每个步骤完成后，进行测试验证
4. **版本控制**：使用Git管理代码变更，便于回滚

