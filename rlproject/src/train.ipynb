{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840609b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc81cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete,Tuple\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Define colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)  # Color for the trajectory\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self,render_mode=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "\n",
    "        self.env_width = self.grid_size*1.5\n",
    "        self.env_height = self.grid_size\n",
    "\n",
    "        self.pause = False\n",
    "        self.domain_randomization = False\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define two separate thresholds for obstacle handling\n",
    "        self.distance_threshold_penalty = 5  # Penalty zone threshold (larger value)\n",
    "        self.distance_threshold_collision = 3  # Collision threshold (smaller value)\n",
    "        self.distance_threshold_arm = 3  # Arm threshold (smaller value)\n",
    "        self.penalty_factor = 5  # Penalty scaling factor\n",
    "        self.distance_reward_factor = 2\n",
    "        self.smooth_action_penalty = 2\n",
    "        self.steps = 0\n",
    "        self.margin = 0.3\n",
    "        self.reward_arm = -100\n",
    "        self.reward_hand = -100\n",
    "        self.reward_bound = -200\n",
    "        self.reward_max_step = 200\n",
    "        self.reward_step = 10\n",
    "        self.stride_robot_random = [1,3]\n",
    "        self.stride_hand_random = [0.6,1]\n",
    "        self.hand_move_epsilon = 0.1\n",
    "\n",
    "\n",
    "        self.current_distance = 0  # Current distance to goal, used for reward shaping\n",
    "        self.max_steps = 50  # Set a maximum number of steps to prevent infinite loops\n",
    "        # Action space (dx, dy)\n",
    "        self.action_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Observation space (robot_x, robot_y, goal_x, goal_y)\n",
    "        self.observation_shape = 2+2+2+1+1+1+2+1+1+2 # Robot position, hand position, velocity_hand,radius_hand, and distance to hand\n",
    "\n",
    "        self.observation_space = Box(low=0, high=np.array([self.env_width,self.env_height, self.env_width, self.env_height,1,1 , (2**0.5)*self.grid_size,0.5*self.grid_size,0.5*self.grid_size,2*self.grid_size,self.grid_size,self.stride_robot_random[1],self.stride_hand_random[1],self.env_width,self.env_height]), \n",
    "                                     shape=(self.observation_shape,), dtype=np.float32)\n",
    "\n",
    "        self.random = True\n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 50 # Pixels per grid unit\n",
    "        self.trajectory_points = [] # New: List to store past robot positions\n",
    "        self.dist_arm = 0\n",
    "\n",
    "\n",
    "    def dist_point_to_segment_correct(self,P, A, B, eps=1e-12):\n",
    "        P = np.asarray(P, dtype=float)\n",
    "        A = np.asarray(A, dtype=float)\n",
    "        B = np.asarray(B, dtype=float)\n",
    "        v = B - A\n",
    "        w = P - A\n",
    "        vv = np.dot(v, v)\n",
    "        if vv <= eps:\n",
    "            # A and B coincide: treat as point A\n",
    "            C = A.copy()\n",
    "            d = np.linalg.norm(P - A)\n",
    "            t = 0.0\n",
    "            case = 'endpoint_A'\n",
    "        else:\n",
    "            t = np.dot(w, v) / vv\n",
    "            if t < 0.0:\n",
    "                C = A\n",
    "                d = np.linalg.norm(P - A)\n",
    "                case = 'before_A'\n",
    "            elif t > 1.0:\n",
    "                C = B\n",
    "                d = np.linalg.norm(P - B)\n",
    "                case = 'after_B'\n",
    "            else:\n",
    "                C = A + t * v\n",
    "                d = np.linalg.norm(P - C)\n",
    "                case = 'on_segment'\n",
    "        return float(d), C, float(t), case\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return np.concatenate(([self.robot_position]+ \n",
    "                               [self.hand_position]+\n",
    "                               [self.last_action]+\n",
    "                               [np.array([self.current_distance])]+\n",
    "                               [np.array([min(self.robot_position[0],\n",
    "                                              self.robot_position[1],\n",
    "                                              self.env_width-self.robot_position[0],\n",
    "                                              self.env_height-self.robot_position[1])])]+\n",
    "                                [np.array([self.dist_arm])]+\n",
    "                                [self.fixed_point]+\n",
    "                                [np.array([self.stride_robot])]+\n",
    "                                [np.array([self.stride_hand])]+\n",
    "                                [np.array([self.env_width,self.env_height])]))\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance_to_hand\": self.current_distance,\n",
    "            \"robot_position\": self.robot_position,\n",
    "            \"hand_position\": self.hand_position,\n",
    "            'distance_arm':self.dist_arm,\n",
    "            \"fix_point\":self.fixed_point,\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        super().reset()\n",
    "        self.distance = []\n",
    "        self.stride_robot = np.random.uniform(*self.stride_robot_random)  # Randomize stride length\n",
    "        self.stride_hand = np.random.uniform(*self.stride_hand_random)  # Randomize stride length\n",
    "        # self.stride_robot = 1  # Randomize stride length\n",
    "        self.distance_threshold_collision = np.random.uniform(2,3)  # Randomize collision threshold\n",
    "        self.distance_threshold_penalty = np.random.uniform(3, 4)  # Randomize penalty threshold\n",
    "        \n",
    "        \n",
    "        self.noise_obs_sigma = np.random.uniform(0, 0.1)  # Add some noise to observation to make it more realistic\n",
    "        self.noise_action_sigma = np.random.uniform(0,0.1)  # Add some noise to action to make it more realistic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.robot_position = np.random.uniform(self.margin, [self.env_width-self.margin,self.env_height-self.margin])  # Randomize robot position\n",
    "        self.hand_position = np.random.uniform(self.margin, [self.env_width-self.margin,self.env_height-self.margin])  # Randomize hand position\n",
    "        # self.hand_position = np.clip(self.hand_position, self.margin, self.grid_size-self.margin)  # Ensure hand stays within grid bounds\n",
    "        \n",
    "        # self.hand_move_mode = 'random' if np.random.rand() < 0.1 else 'towards_robot'  # Randomize hand movement mode\n",
    "        # self.hand_move_mode = 'towards_robot'\n",
    "        \n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.pre_distance = self.current_distance\n",
    "        self.last_action = np.zeros(2)\n",
    "        self.steps = 0\n",
    "        self.trajectory_points = [self.robot_position.copy()] # New: Reset trajectory and add initial position\n",
    "        \n",
    "        self.fixed_point = np.array([self.grid_size*random.uniform(0.2,1.3),self.grid_size])\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _reward(self,action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0  # Initialize reward\n",
    "        done_reason = None  # Initialize done reason\n",
    "\n",
    "        # action regulation penalty\n",
    "        # reward -= 0.5 * np.sum(np.square(action))  # Penalty for large actions\n",
    "\n",
    "        self.dist_arm = self.dist_point_to_segment_correct(self.robot_position,self.hand_position, self.fixed_point)[0]\n",
    "        if self.dist_arm < self.distance_threshold_arm:\n",
    "            reward += self.reward_arm \n",
    "            terminated = True  # Truncate if arm is too short\n",
    "\n",
    "        # boundary penalty\n",
    "        if np.any(self.robot_position <= self.margin) or (self.env_height-self.robot_position[1] <=self.margin) or self.env_width-self.robot_position[0] <=self.margin:\n",
    "            reward += self.reward_bound\n",
    "            terminated = True  # Truncate if robot goes out of bounds\n",
    "            done_reason = \"out of bounds\"\n",
    "\n",
    "    \n",
    "        # Auxiliary Rewards -  distance to hand\n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.distance.append(self.current_distance)\n",
    "        reward += (self.current_distance-self.pre_distance)*self.distance_reward_factor  # Reward shaping based on distance change\n",
    "        self.pre_distance = self.current_distance\n",
    "\n",
    "        # Obstacle handling with two thresholds\n",
    "        if self.current_distance < self.distance_threshold_collision:\n",
    "            reward += self.reward_hand\n",
    "            terminated = True  # Terminate if too close to obstacles\n",
    "            done_reason = \"collision with obstacle\"\n",
    "        elif self.current_distance < self.distance_threshold_penalty:\n",
    "            reward -= self.penalty_factor * (self.distance_threshold_penalty - self.current_distance)  # Penalty for being too close to obstacles\n",
    "\n",
    "        reward -= self.smooth_action_penalty * np.linalg.norm(action - self.last_action)\n",
    "\n",
    "        # Small reward for each step taken to encourage exploration\n",
    "        reward+= self.reward_step \n",
    "\n",
    "        # Truncate if max steps reached and give max step reward\n",
    "        if self.steps >= self.max_steps:\n",
    "            reward += self.reward_max_step\n",
    "            truncated = True  \n",
    "\n",
    "        return reward,terminated,truncated,done_reason\n",
    "\n",
    "    def _get_hand_movement(self):\n",
    "\n",
    "        # if self.hand_move_mode == 'random':\n",
    "        #     move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        # elif self.hand_move_mode == 'towards_robot':\n",
    "        #     dir_vector = self.robot_position - self.hand_position\n",
    "        #     if np.linalg.norm(dir_vector) > 0:\n",
    "        #         dir_vector /= np.linalg.norm(dir_vector)\n",
    "        #     move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        if random.random() < self.hand_move_epsilon:\n",
    "            move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        else:\n",
    "            dir_vector = self.robot_position - self.hand_position\n",
    "            if np.linalg.norm(dir_vector) > 0:\n",
    "                dir_vector /= np.linalg.norm(dir_vector)\n",
    "            move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        \n",
    "        return move_hand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.random:\n",
    "            action+=np.random.normal(0,self.noise_action_sigma,size=self.action_space.shape)  # Add some noise to action to make it more realistic\n",
    "\n",
    "        move_hand = self._get_hand_movement()\n",
    "        self.hand_position += move_hand  # Update hand position\n",
    "        self.hand_position = np.clip(self.hand_position, self.margin, [self.env_width-self.margin,self.env_height-self.margin])  # Ensure hand stays within grid bounds\n",
    "        # self.fixed_point+= np.array([np.,0])  # Randomize fixed point position\n",
    "\n",
    "        self.robot_position += action * self.stride_robot  # Scale the action to control speed\n",
    "        self.trajectory_points.append(self.robot_position.copy()) # New: Add current position to trajectory\n",
    "        self.steps += 1\n",
    "        \n",
    "\n",
    "        reward,terminated,truncated,done_reason = self._reward(action)\n",
    "        info = self._get_info()\n",
    "        info['done_reason'] = done_reason\n",
    "        info['distance_mean'] = np.mean(self.distance)\n",
    "        observation = self._get_obs()\n",
    "        if self.random:\n",
    "            observation += np.random.normal(0, self.noise_obs_sigma, size=self.observation_shape)  # Add some noise to observation to make it more realistic\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    " \n",
    "        pygame.display.init()\n",
    "        self.window = pygame.display.set_mode(\n",
    "                (int(self.grid_size * self.cell_size), int(self.grid_size * self.cell_size))\n",
    "            )\n",
    "        pygame.display.set_caption(\"CustomEnv\")\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                import sys\n",
    "                sys.exit() # Exit the program\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mouse_x, mouse_y = event.pos\n",
    "                self.hand_position = np.array([mouse_x/self.cell_size, mouse_y/self.cell_size])\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:  # 空格键切换暂停\n",
    "                    self.pause = not self.pause\n",
    "\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        canvas.fill(WHITE)\n",
    "        virus_image = pygame.image.load(\"hand.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "        robot_image = pygame.transform.scale(virus_image, (int(self.cell_size * 2), int(self.cell_size * 2)))  # Scale the image\n",
    "        # New: Draw the trajectory\n",
    "        if len(self.trajectory_points) > 1:\n",
    "            scaled_points = []\n",
    "            for point in self.trajectory_points:\n",
    "                scaled_points.append((int(point[0] * self.cell_size), int(point[1] * self.cell_size)))\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            pygame.draw.lines(canvas, BLUE, False, scaled_points, 2) # Blue line, not closed, 2 pixels wide\n",
    "            \n",
    "            # Optionally, draw small circles at each point to emphasize\n",
    "            for point_coord in scaled_points:\n",
    "                pygame.draw.circle(canvas, BLUE, point_coord, 3) # Small blue circles\n",
    "\n",
    "        # Draw robot\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            RED,\n",
    "            (int(self.robot_position[0] * self.cell_size), int(self.robot_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.2)\n",
    "        )\n",
    "        # Draw obstacles\n",
    "\n",
    "        canvas.blit(robot_image, (int((self.hand_position[0]-1) * self.cell_size), int((self.hand_position[1]-1) * self.cell_size+1)))\n",
    "        pygame.draw.circle(canvas,\n",
    "                            GREEN, \n",
    "                            (int((self.hand_position[0]) * self.cell_size), \n",
    "                            int((self.hand_position[1]) * self.cell_size+1)), \n",
    "        int(self.cell_size * 0.2)\n",
    "        )\n",
    "\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def load_args(self, args):\n",
    "        pass\n",
    "\n",
    "    def save_args(self,path):\n",
    "        env_args = {\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"distance_threshold_penalty\":self.distance_threshold_penalty,\n",
    "            \"distance_threshold_collision\":self.distance_threshold_collision,\n",
    "            \"penalty_factor\":self.penalty_factor,\n",
    "            \"distance_reward_factor\":self.distance_reward_factor,\n",
    "            \"smooth_action_penalty\":self.smooth_action_penalty,\n",
    "            \"max_steps\":self.max_steps,\n",
    "            \"margin\":self.margin,\n",
    "            \"reward_step\":self.reward_step,\n",
    "            \"reward_max_step\":self.reward_max_step,\n",
    "            \"reward_bound\":self.reward_bound,\n",
    "            \"reward_arm\":self.reward_arm,\n",
    "            \"reward_hand\":self.reward_hand,\n",
    "            \"stride_robot_range\":self.stride_robot_random,\n",
    "            \"stride_hand_range\":self.stride_hand_random,\n",
    "            \"move_hand_epsilon\":self.hand_move_epsilon,\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "        with open(os.path.join(path, \"env_args.json\"), \"w\") as f:\n",
    "            json.dump(env_args, f,indent=4)\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        pygame.display.quit()\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3798d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=10000, n_episodes=1, log_freq=10000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        # 用deque统计最近N个done的终止原因，避免内存爆炸\n",
    "        self.termination_reasons = deque(maxlen=1000)  # 统计最近1000次终止\n",
    "        self.env_to_render = env\n",
    "        self.render_freq = render_freq\n",
    "        self.n_episodes = n_episodes\n",
    "        self.distance_mean = deque(maxlen=1000) \n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 先从env info里读取终止原因\n",
    "        # print((self.locals.keys()))\n",
    "        infos = self.locals.get('infos', None)\n",
    "\n",
    "        dones = self.locals.get('dones', None)\n",
    "        if infos is not None and dones is not None:\n",
    "            for done, info in zip(dones, infos):\n",
    "                if done and info is not None and 'done_reason' in info:\n",
    "                    self.termination_reasons.append(info['done_reason'])\n",
    "                    self.distance_mean.append(info['distance_mean'])\n",
    "\n",
    "        # 每log_freq步打印信息\n",
    "        # if self.num_timesteps % self.render_freq == 0 and self.verbose:\n",
    "        #     for ep in range(self.n_episodes):\n",
    "        #         obs = self.env_to_render.reset()\n",
    "        #         done = False\n",
    "        #         while not done:\n",
    "        #             action, _states = self.model.predict(obs, deterministic=True)\n",
    "        #             obs, rewards, done, info = self.env_to_render.step(action)\n",
    "        #             self.env_to_render.render()\n",
    "        #             time.sleep(0.6)\n",
    "\n",
    "        #             if done:\n",
    "\n",
    "        #                 self.env_to_render.close()\n",
    "        #                 break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.num_timesteps % self.log_freq == 0 and self.verbose:\n",
    "            log = self.model.logger.name_to_value\n",
    "            ep_rew = log.get('rollout/ep_rew_mean', None)\n",
    "            ep_len = log.get('rollout/ep_len_mean', None)\n",
    "            loss = log.get('train/loss', None)\n",
    "            v_loss = log.get('train/value_loss', None)\n",
    "            p_loss = log.get('train/policy_gradient_loss', None)\n",
    "            ent_loss = log.get('train/entropy_loss', None)\n",
    "            kl = log.get('train/approx_kl', None)\n",
    "\n",
    "            # 统计终止原因比例\n",
    "            total = len(self.termination_reasons)\n",
    "            if total > 0:\n",
    "                count_hand = sum(1 for r in self.termination_reasons if r == 'out of bounds')\n",
    "                ratio_hand = count_hand / total\n",
    "            else:\n",
    "                ratio_hand = 0.0\n",
    "            distance_mean = sum(self.distance_mean) / len(self.distance_mean) if len(self.distance_mean) > 0 else 0.0\n",
    "\n",
    "            # print(f\"[{self.num_timesteps:7d}] ep_rew_mean={ep_rew}, ep_len_mean={ep_len}, loss={loss:.3f}, \"\n",
    "            #       f\"v_loss={v_loss:.3f}, p_loss={p_loss:.3f}, ent_loss={ent_loss:.3f}, kl={kl:.4f}, \"\n",
    "            #       f\"termination_reason_hand_ratio={ratio_hand:.3f}\")\n",
    "            self.logger.record(\"custom/termination_reason_ratio\", ratio_hand)\n",
    "\n",
    "            self.logger.record(\"custom/distance_mean\", distance_mean)\n",
    "            self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f08397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camera_calibration.camera_calibration import CameraCalibration\n",
    "from robot_control.ur_control import URControl  \n",
    "from cv.hand_detect import HandDetection\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from cv.get_workspace import get_workspace\n",
    "\n",
    "\n",
    "\n",
    "desired_width = 2592 \n",
    "desired_height = 1944\n",
    "\n",
    "\n",
    "cv_model = YOLO('runs/detect/train3/weights/best.onnx')\n",
    "hand_detector = HandDetection()\n",
    "cali = CameraCalibration()\n",
    "robot_ip = \"192.168.1.2\"\n",
    "\n",
    "robot_control = URControl(robot_ip)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)  # 创建一个窗口来显示矫正后的图像\n",
    "cv2.namedWindow('edges', cv2.WINDOW_NORMAL)  \n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera for demonstration.\")\n",
    "    exit()\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, desired_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, desired_height)\n",
    "\n",
    "w_env, h_env = 15, 10  # 环境的宽度和高度\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a310e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "class BufferRecord:\n",
    "    # obs: list = []\n",
    "    # action: list = []\n",
    "    # reward: list = []\n",
    "    # next_obs: list = []\n",
    "    # done: list = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.action = []\n",
    "        self.reward = []\n",
    "        self.next_obs = []\n",
    "        self.done = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        self.obs.extend(obs)\n",
    "        self.action.extend(action)\n",
    "        self.reward.extend(reward)\n",
    "        self.next_obs.extend(next_obs)\n",
    "        self.done.extend(done)\n",
    "\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"obs\": self.obs,\n",
    "            \"action\": self.action,\n",
    "            \"reward\": self.reward,\n",
    "            \"next_obs\": self.next_obs,\n",
    "            \"done\": self.done\n",
    "        }\n",
    "    def save_to_npz(self, file_path):\n",
    "        np.savez(file_path, **self.to_dict())\n",
    "    \n",
    "    def load_from_npz(self, file_path):\n",
    "        data = np.load(file_path)\n",
    "        self.obs = data[\"obs\"]\n",
    "        self.action = data[\"action\"]\n",
    "        self.reward = data[\"reward\"]\n",
    "        self.next_obs = data[\"next_obs\"]\n",
    "        self.done = data[\"done\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\RL\\lib\\site-packages\\gymnasium\\spaces\\box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./sac_custom_env_tensorboard/best_model_sac103\\run_1\n",
      "step: 2\n",
      "Loading runs/detect/train3/weights/best.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CPUExecutionProvider\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | 3.87     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    total timesteps | 240      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -377     |\n",
      "|    critic_loss     | 1.12e+04 |\n",
      "|    ent_coef        | 1.73     |\n",
      "|    ent_coef_loss   | 0.279    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 889947   |\n",
      "---------------------------------\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "*************done*************\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.8     |\n",
      "|    ep_rew_mean     | -27.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    total timesteps | 254      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 9.91e+03 |\n",
      "|    ent_coef        | 1.72     |\n",
      "|    ent_coef_loss   | 0.147    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 889963   |\n",
      "---------------------------------\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -42.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    total timesteps | 263      |\n",
      "---------------------------------\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "*************done*************\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.4     |\n",
      "|    ep_rew_mean     | -43.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    total timesteps | 294      |\n",
      "---------------------------------\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | -38.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    total timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 1.24e+04 |\n",
      "|    ent_coef        | 1.72     |\n",
      "|    ent_coef_loss   | 0.0162   |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 889995   |\n",
      "---------------------------------\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "training\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "*************done*************\n",
      "step: 2\n",
      "*************done*************\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | -39.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    total timesteps | 496      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 9.38e+03 |\n",
      "|    ent_coef        | 1.72     |\n",
      "|    ent_coef_loss   | -0.0132  |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 890027   |\n",
      "---------------------------------\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n",
      "step: 2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import os\n",
    "from stable_baselines3 import PPO,SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.commom.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import safe_mean\n",
    "\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "env_raw = CustomEnv()\n",
    "save_path = \"logs/best_model_sac\"\n",
    "i=1\n",
    "while os.path.exists(save_path+str(i)):\n",
    "    i+=1\n",
    "save_path = f\"{save_path}{i}\"\n",
    "os.makedirs(save_path)\n",
    "\n",
    "tensorboard_dir = \"./sac_custom_env_tensorboard\"\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "tensorboard_log_dir = tensorboard_dir + '/'+ f'best_model_sac{i}'\n",
    "# print(os.getcwd())\n",
    "# load_model = r\"C:\\Users\\admin\\Desktop\\huifeng\\RL\\src\\logs\\best_model_sac88\\best_model.zip\"\n",
    "\n",
    "load_model = r\"C:\\Users\\admin\\Desktop\\huifeng\\RL\\rlproject\\src\\model\\model_500step.zip\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env = Monitor(env_raw)  # Wrap the environment with Monitor for logging\n",
    "env = env_raw\n",
    "\n",
    "# env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128,256,256,128], qf=[128,256,256,128])],\n",
    "    # activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[128,256,256,128],\n",
    "    # activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1,ent_coef='auto',buffer_size=10_000,policy_kwargs=policy_kwargs,tensorboard_log=tensorboard_log_dir)\n",
    "\n",
    "model = SAC.load(load_model, env, verbose=1,ent_coef='auto',learning_rate=0.0001,tensorboard_log=tensorboard_log_dir)\n",
    "replay_buffer = model.replay_buffer\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=save_path,\n",
    "    log_path = './logs/',\n",
    "    eval_freq=10,  # 每1000步评估一次\n",
    "    deterministic=True,\n",
    "    render=True,\n",
    "    n_eval_episodes=10,  # 每次评估5个episode\n",
    ")\n",
    "\n",
    "last_hand = 0\n",
    "\n",
    "position_hand_env = [0,0]\n",
    "last_action = np.array([0,0],dtype=np.float32)\n",
    "action = np.array([0,0],dtype=np.float32)\n",
    "h,w =0,0\n",
    "trajectory_robot = deque(maxlen=20)\n",
    "def get_obs(frame):\n",
    "    global last_hand, position_hand_env,last_action,h,w,position_robot_pixel,action\n",
    "    # ret, frame = cap.read()\n",
    "    # if not ret:\n",
    "    #     print(\"Error: Could not read frame from camera for demonstration.\")\n",
    "    # 使用 cv2.undistort 对每一帧进行畸变矫正\n",
    "\n",
    "    undistorted_frame = cali.undistort_frame(frame)\n",
    "    undistorted_frame = get_workspace(undistorted_frame)\n",
    "    h , w = undistorted_frame.shape[:2]\n",
    "    img_gray = cv2.cvtColor(undistorted_frame, cv2.COLOR_BGR2GRAY)\n",
    "    results = cv_model.predict(undistorted_frame, conf=0.7, save=False,imgsz=640,verbose=False)\n",
    "    undistorted_frame, hand_positions = hand_detector.process_frame(undistorted_frame)\n",
    "    cx,cy = 0,0\n",
    "    for i, r in enumerate(results):\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            # 提取边界框坐标\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            if x2-x1 > 100:\n",
    "                continue\n",
    "            # 计算中心点\n",
    "            cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            # 在图像上绘制边界框和中心点\n",
    "            cv2.rectangle(undistorted_frame, (x1, y1), (x2, y2), (255, 0, 255), 6)\n",
    "            # cv2.circle(undistorted_frame, (cx, cy), 5, (255, 0, 0), -1)\n",
    "    \n",
    "    trajectory_robot.append([cx,cy])\n",
    "    if len(trajectory_robot) >= 2:\n",
    "        i = 2\n",
    "        for j in range(1, len(trajectory_robot)):\n",
    "            cv2.line(undistorted_frame, trajectory_robot[j - 1], trajectory_robot[j], (0, 255, 255), int(i//2))\n",
    "            i+=0.2\n",
    "\n",
    "    if hand_positions:\n",
    "        position_hand_env = hand_positions[0]/np.array([w/h_env,h/w_env])\n",
    "        position_hand_env = position_hand_env[1],h_env - position_hand_env[0]\n",
    "    \n",
    "    hsv = cv2.cvtColor(undistorted_frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # 设置肤色范围（适用于常见肤色，可根据光照微调）\n",
    "    lower_skin = np.array([0, 30, 60], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 150, 255], dtype=np.uint8)\n",
    "\n",
    "    # 根据阈值生成mask\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    # 例如右边伸出\n",
    "    try:\n",
    "        idx = np.argmin(xs)\n",
    "        tip = (xs[idx], ys[idx])\n",
    "        cv2.circle(undistorted_frame, tip, 10, (0, 0, 255), -1)\n",
    "        fixed_point = [tip[1]*w_env/h,h_env]\n",
    "    except:\n",
    "        fixed_point = [10,10]\n",
    "    \n",
    "    *position_robot_world,z,rx,ry,rz = robot_control.get_robot_pose()  # 使用更新后的类名\n",
    "\n",
    "\n",
    "\n",
    "    position_robot_pixel = cx,cy\n",
    "\n",
    "    # position_robot_pixel = cali.world_to_pixel(position_robot_world)\n",
    "\n",
    "    # print(position_robot_world)\n",
    "    \n",
    "    position_robot_env =  position_robot_pixel[1]*w_env / h, h_env - position_robot_pixel[0]*h_env /w \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    robot = np.array([position_robot_env],dtype=np.float32)\n",
    "    hand = np.array([position_hand_env],dtype=np.float32)\n",
    "    stride_hand = np.linalg.norm(hand - last_hand)\n",
    "    # print(stride_hand)\n",
    "    distance_to_object = np.linalg.norm(robot - hand)\n",
    "    distance = np.array([distance_to_object],dtype=np.float32)\n",
    "    # print(distance)\n",
    "    boundary = np.array([min(position_robot_env[0],position_robot_env[1],w_env-position_robot_env[0],h_env-position_robot_env[1])],dtype=np.float32)\n",
    "    # last_action = np.array([last_action],dtype=np.float32)\n",
    "\n",
    "\n",
    "    dist_arm = env_raw.dist_point_to_segment_correct(robot.flatten(),hand.flatten(),[15,10])[0]\n",
    "    # fixed_point = [tip[1]*20/h,10]\n",
    "    # stride_robot = 2\n",
    "    env.hand_position = hand.flatten()\n",
    "    env.robot_position = robot.flatten()\n",
    "    env.fixed_point = fixed_point\n",
    "    last_hand = hand\n",
    "    last_action = action\n",
    "    obs = np.concatenate((robot.flatten(),hand.flatten(),last_action.flatten(),distance.flatten(),boundary.flatten(),np.array([dist_arm],dtype=np.float32),np.array(fixed_point,dtype=np.float32),np.array([env.stride_robot]),np.array([stride_hand]),np.array([15,10])))\n",
    "    # print(obs)\n",
    "    return obs,undistorted_frame\n",
    "\n",
    "debug_callback = DebugCallback(env=env,log_freq=10000, verbose=1)\n",
    "# debug_callback = None\n",
    "callback = CallbackList([eval_callback, debug_callback])\n",
    "\n",
    "\n",
    "\n",
    "model._setup_learn(total_timesteps=400000,callback=callback)\n",
    "callback.on_training_start(locals(), globals())\n",
    "# model.learn(total_timesteps=400000,callback=callback)\n",
    "obs = env.reset()\n",
    "env.stride_robot = 2\n",
    "\n",
    "done = False\n",
    "rewards = []\n",
    "best_eval_metric = 0\n",
    "info_buffer = deque(maxlen=100)\n",
    "cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "records = BufferRecord()\n",
    "for step in range(1000):\n",
    "    model.num_timesteps+=1\n",
    "    # print(f\"step: {env.stride_robot}\")\n",
    "\n",
    "    # get obs\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame from camera for demonstration.\")\n",
    "        break\n",
    "    \n",
    "\n",
    "    obs,frame = get_obs(frame)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    action, _ = model.predict(obs, deterministic=False)\n",
    "    # print(f\"action: {action}\")\n",
    "    # next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "    #execute action\n",
    "    stride_robot = env.stride_robot\n",
    "    action_pixel = action * np.array([h/w_env,w/h_env])*stride_robot\n",
    "    action_pixel = -action_pixel[1],action_pixel[0]\n",
    "    rx,ry,rz = 0.085,-0.027,4.637\n",
    "    position_robot_pixel += np.array([action_pixel[0],action_pixel[1]])\n",
    "    position_robot_world = cali.pixel_to_world(position_robot_pixel)\n",
    "\n",
    "    # print(position_robot_world)\n",
    "    robot_control.move_robot([position_robot_world[0],position_robot_world[1],0.12,rx,ry,rz],0)\n",
    "    time.sleep(0.3)\n",
    "\n",
    "\n",
    "    # compute reward,done,info\n",
    "\n",
    "    reward,terminated,truncated,done_reason = env._reward(action)\n",
    "    done = terminated or truncated\n",
    "    info= [{\"terminated\":terminated,\"truncated\":truncated,\"done_reason\":done_reason}]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # get next obs\n",
    "    ret, frame = cap.read()\n",
    "    next_obs,frame = get_obs(frame)\n",
    "    # print(next_obs[:2])\n",
    "\n",
    "    # store transition in replay buffer\n",
    "    replay_buffer.add(obs,next_obs, action, reward, done, info)\n",
    "    rewards.append(reward)\n",
    "\n",
    "\n",
    "    records.add([obs], [action], [reward], [next_obs], [done])\n",
    "    if step % 50 == 0 and replay_buffer.size() > 64:\n",
    "        # time.sleep(0.5)\n",
    "        print(\"training\")\n",
    "        model.train(batch_size=64, gradient_steps=16)\n",
    "\n",
    "    \n",
    "    if done:\n",
    "        print(\"*************done*************\")\n",
    "        # time.sleep(0.5)\n",
    "        model._episode_num += 1\n",
    "        info_buffer.append({\n",
    "            \"r\": safe_mean(rewards),\n",
    "            \"l\": len(rewards),\n",
    "        })\n",
    "        # print(f\"Episode {model._episode_num} mean reward: {safe_mean(rewards)}\")\n",
    "        # print(f\"Episode {model._episode_num} episode length: {len(rewards)}\")\n",
    "        rewards = []\n",
    "\n",
    "        if model._episode_num % 4 == 0:\n",
    "            # model.dump_logs()\n",
    "            model.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in info_buffer]))\n",
    "            model.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in info_buffer]))\n",
    "            model.logger.record(\"time/episodes\", model._episode_num)\n",
    "            model.logger.record(\"time/total timesteps\", model.num_timesteps)\n",
    "            model.logger.dump(step=model.num_timesteps)\n",
    "        # obs = env.reset()\n",
    "        done = False\n",
    "    # callback.on_step()\n",
    "\n",
    "    frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.waitKey(1)\n",
    "# cali.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# env.save(\"vec_env.pkl\")  # Save the environment state\n",
    "records.save_to_npz(\"replay_buffer.npz\")\n",
    "model.save('./model/model_final.zip') \n",
    "settings = {\n",
    "    'load_model':load_model,\n",
    "    'tensorboard_log' :tensorboard_log_dir,\n",
    "           }\n",
    "env_raw.save_args(save_path)\n",
    "with open(os.path.join(save_path, \"settings.json\"), \"w\") as f:\n",
    "    json.dump(settings, f)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9be3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
