{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60538f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import random\n",
    "import pygame\n",
    "\n",
    "# --- Constants ---\n",
    "SCREEN_WIDTH = 1200\n",
    "SCREEN_HEIGHT = 800\n",
    "FPS = 60\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (220, 50, 50)\n",
    "GREEN = (50, 220, 50)\n",
    "BLUE = (50, 50, 220)\n",
    "GRAY = (150, 150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "045bbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete,Tuple\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Define colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)  # Color for the trajectory\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.obstacle_num = 8\n",
    "        self.distance_threshold_goal = 0.3  # Tolerance for goal proximity\n",
    "\n",
    "        # Define two separate thresholds for obstacle handling\n",
    "        self.distance_threshold_penalty = 1  # Penalty zone threshold (larger value)\n",
    "        self.distance_threshold_collision = 0.5  # Collision threshold (smaller value)\n",
    "        self.penalty_factor = 25  # Penalty scaling factor\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.pre_distance = 0  # Previous distance to goal, used for reward shaping\n",
    "        self.current_distance = 0  # Current distance to goal, used for reward shaping\n",
    "        # Action space (dx, dy)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Observation space (robot_x, robot_y, goal_x, goal_y)\n",
    "        self.observation_shape = 2 + 2 + self.obstacle_num * 2 + 1 # Robot position, goal position, and obstacle positions\n",
    "        self.observation_space = Box(low=0, high=np.array([self.grid_size for _ in range(self.observation_shape-1)]+[1]), shape=(self.observation_shape,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 50 # Pixels per grid unit\n",
    "        self.trajectory_points = [] # New: List to store past robot positions\n",
    "\n",
    "    def _get_obs(self):\n",
    "\n",
    "        return np.concatenate(([self.robot_position]+ [self.goal_position]+self.obstacle_position+[np.array(self.steps)]),dtype=np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance_to_goal\": np.linalg.norm(self.robot_position - self.goal_position)\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.robot_position = np.random.uniform(0, self.grid_size, size=2)\n",
    "        self.goal_position = np.random.uniform(0, self.grid_size, size=2)\n",
    "        self.obstacle_position = [np.random.uniform(0, self.grid_size, size=2) for _ in range(self.obstacle_num)]\n",
    "        self.steps = 0\n",
    "        self.trajectory_points = [self.robot_position.copy()] # New: Reset trajectory and add initial position\n",
    "        self.pre_distance = np.linalg.norm(self.robot_position - self.goal_position)\n",
    "        self.max_steps = 1000  # Set a maximum number of steps to prevent infinite loops\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _reward(self):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        if self.steps >= self.max_steps:\n",
    "            reward -= 50\n",
    "            truncated = True  # Truncate if max steps reached\n",
    "        # Auxiliary Rewards -  distance to goal \n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.goal_position)\n",
    "        reward = (self.pre_distance - self.current_distance)*30  # Reward shaping based on distance change\n",
    "        self.pre_distance = self.current_distance\n",
    "\n",
    "        # Obstacle handling with two thresholds\n",
    "        for obs in self.obstacle_position:\n",
    "            distance = np.linalg.norm(self.robot_position - obs)\n",
    "            if distance < self.distance_threshold_collision:\n",
    "                # Game over - collision detected\n",
    "                reward -= 100  # Large negative reward for collision\n",
    "                terminated = True\n",
    "                break\n",
    "            elif distance < self.distance_threshold_penalty:\n",
    "                # Apply penalty for being too close to obstacle\n",
    "                reward -= (self.distance_threshold_penalty - distance) * self.penalty_factor\n",
    "        # 是否越界\n",
    "        if np.any(self.robot_position == 0) or np.any(self.robot_position == self.grid_size):\n",
    "            reward -= 100\n",
    "            terminated = True  # Out of bounds\n",
    "        if np.linalg.norm(self.robot_position - self.goal_position) < self.distance_threshold_goal:\n",
    "            reward += 100\n",
    "            terminated = True  # Reached the goal\n",
    "        \n",
    "        reward-=  0.5  # Small penaty for  each step to encourage efficiency\n",
    "\n",
    "        return reward, terminated,truncated\n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        stride = 0.3  # Base stride length\n",
    "        self.robot_position += action * stride  # Scale the action to control speed\n",
    "        self.robot_position = np.clip(self.robot_position, 0, self.grid_size)\n",
    "\n",
    "        self.trajectory_points.append(self.robot_position.copy()) # New: Add current position to trajectory\n",
    "\n",
    "        reward,terminated,truncated = self._reward()\n",
    "        info = self._get_info()\n",
    "        observation = self._get_obs()\n",
    "        self.steps += 0.01\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (int(self.grid_size * self.cell_size), int(self.grid_size * self.cell_size))\n",
    "            )\n",
    "            pygame.display.set_caption(\"CustomEnv\")\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                import sys\n",
    "                sys.exit() # Exit the program\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if event.button == 1: # Left mouse button click\n",
    "                    mouse_x, mouse_y = event.pos # Get mouse position in pixels\n",
    "                    # Convert pixel coordinates to grid coordinates\n",
    "                    new_goal_x = mouse_x / self.cell_size\n",
    "                    new_goal_y = mouse_y / self.cell_size\n",
    "                    self.goal_position = np.array([new_goal_x, new_goal_y], dtype=np.float32)\n",
    "                    print(f\"New goal set at grid position: {self.goal_position}\")\n",
    "\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        canvas.fill(WHITE)\n",
    "        virus_image = pygame.image.load(\"病毒.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "        robot_image = pygame.transform.scale(virus_image, (int(self.cell_size * 0.8), int(self.cell_size * 0.8)))  # Scale the image\n",
    "        # New: Draw the trajectory\n",
    "        if len(self.trajectory_points) > 1:\n",
    "            scaled_points = []\n",
    "            for point in self.trajectory_points:\n",
    "                scaled_points.append((int(point[0] * self.cell_size), int(point[1] * self.cell_size)))\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            pygame.draw.lines(canvas, BLUE, False, scaled_points, 2) # Blue line, not closed, 2 pixels wide\n",
    "            \n",
    "            # Optionally, draw small circles at each point to emphasize\n",
    "            for point_coord in scaled_points:\n",
    "                pygame.draw.circle(canvas, BLUE, point_coord, 3) # Small blue circles\n",
    "\n",
    "        # Draw robot\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            RED,\n",
    "            (int(self.robot_position[0] * self.cell_size), int(self.robot_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.2)\n",
    "        )\n",
    "        # Draw obstacles\n",
    "        for obs in self.obstacle_position:\n",
    "            # pygame.draw.circle(\n",
    "            #     canvas,\n",
    "            #     GRAY,\n",
    "            #     (int(obs[0] * self.cell_size), int(obs[1] * self.cell_size)),\n",
    "            #     int(self.cell_size * 0.5)\n",
    "            # )\n",
    "            canvas.blit(robot_image, (int(obs[0] * self.cell_size), int(obs[1] * self.cell_size)))\n",
    "        # Draw goal\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            GREEN,\n",
    "            (int(self.goal_position[0] * self.cell_size), int(self.goal_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.3)\n",
    "        )\n",
    "\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670f1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a375fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 10 has 0 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 25\u001b[0m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ent_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs,tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_custom_env_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(\n\u001b[0;32m     16\u001b[0m     env,\n\u001b[0;32m     17\u001b[0m     best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs/best_model/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# 每次评估5个episode\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# vec_env = model.get_env()\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# obs = vec_env.reset()\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# for i in range(1000):\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     action, _states = model.predict(obs, deterministic=True)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#     obs, reward, dones, info = vec_env.step(action)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# vec_env.render()\u001b[39;00m\n\u001b[0;32m     35\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    302\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    309\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 311\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:424\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:78\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     77\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 78\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds[env_idx], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmaybe_options)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[136], line 61\u001b[0m, in \u001b[0;36mCustomEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_distance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot_position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_position)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Set a maximum number of steps to prevent infinite loops\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_info()\n",
      "Cell \u001b[1;32mIn[136], line 45\u001b[0m, in \u001b[0;36mCustomEnv._get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot_position\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal_position\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobstacle_position\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 10 has 0 dimension(s)"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "env = CustomEnv()\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[64, 128,64], vf=[128, 256,128])],\n",
    "    # activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, ent_coef=0.01,policy_kwargs=policy_kwargs,tensorboard_log=\"./ppo_custom_env_tensorboard/\")\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path='./logs/best_model/',\n",
    "    log_path = './logs/',\n",
    "    eval_freq=100000,  # 每1000步评估一次\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    n_eval_episodes=5,  # 每次评估5个episode\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=3000000)\n",
    "\n",
    "# vec_env = model.get_env()\n",
    "# obs = vec_env.reset()\n",
    "# for i in range(1000):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, dones, info = vec_env.step(action)\n",
    "    # vec_env.render()\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3755f724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "831e9e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Box high must be a np.ndarray, integer, or float, actual type=<class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mCustomEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m obs,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n",
      "Cell \u001b[1;32mIn[117], line 34\u001b[0m, in \u001b[0;36mCustomEnv.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Observation space (robot_x, robot_y, goal_x, goal_y)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobstacle_num \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Robot position, goal position, and obstacle positions\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m \u001b[43mBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_shape\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# For rendering\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\gymnasium\\spaces\\box.py:148\u001b[0m, in \u001b[0;36mBox.__init__\u001b[1;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Cast `low` and `high` to ndarray for the dtype min and max for out of range tests\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbounded_below \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_low(low, dtype_min)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbounded_above \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cast_high\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# recheck shape for case where shape and (low or high) are provided\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m shape:\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\gymnasium\\spaces\\box.py:271\u001b[0m, in \u001b[0;36mBox._cast_high\u001b[1;34m(self, high, dtype_max)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(high, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox high must be a np.ndarray, integer, or float, actual type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(high)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m         )\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    275\u001b[0m         np\u001b[38;5;241m.\u001b[39missubdtype(high\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating)\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(high\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger)\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m high\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m    278\u001b[0m     ):\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox high must be a floating or integer dtype, actual dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhigh\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Box high must be a np.ndarray, integer, or float, actual type=<class 'list'>"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "obs,_ = env.reset()\n",
    "print(obs)\n",
    "for i in range(1000000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, teminated,_, info = env.step(action)\n",
    "    print(action,info[\"distance_to_goal\"])\n",
    "    print(obs)\n",
    "    print(action)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(env.robot_position)\n",
    "    env.render()\n",
    "    time.sleep(0.6)  # Control the frame rate\n",
    "    if teminated:\n",
    "        env.close()\n",
    "        break\n",
    "        print(\"Resetting environment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
