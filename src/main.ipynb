{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60538f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import random\n",
    "import pygame\n",
    "from loguru import logger\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Constants ---\n",
    "SCREEN_WIDTH = 1200\n",
    "SCREEN_HEIGHT = 800\n",
    "FPS = 60\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (220, 50, 50)\n",
    "GREEN = (50, 220, 50)\n",
    "BLUE = (50, 50, 220)\n",
    "GRAY = (150, 150, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045bbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete,Tuple\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Define colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)  # Color for the trajectory\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self,render_mode=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.pause = False\n",
    "        self.domain_randomization = False\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define two separate thresholds for obstacle handling\n",
    "        self.distance_threshold_penalty = 5  # Penalty zone threshold (larger value)\n",
    "        self.distance_threshold_collision = 1.5  # Collision threshold (smaller value)\n",
    "        self.distance_threshold_arm = 3  # Arm threshold (smaller value)\n",
    "        self.penalty_factor = 5  # Penalty scaling factor\n",
    "        self.distance_reward_factor = 2\n",
    "        self.smooth_action_penalty = 2\n",
    "        self.steps = 0\n",
    "        self.margin = 0.3\n",
    "        self.reward_arm = -100\n",
    "        self.reward_hand = -100\n",
    "        self.reward_bound = -200\n",
    "        self.reward_max_step = 200\n",
    "        self.reward_step = 10\n",
    "        self.stride_robot_random = [1,3]\n",
    "        self.stride_hand_random = [0.6,1]\n",
    "        self.hand_move_epsilon = 0.1\n",
    "\n",
    "\n",
    "        self.current_distance = 0  # Current distance to goal, used for reward shaping\n",
    "        self.max_steps = 50  # Set a maximum number of steps to prevent infinite loops\n",
    "        # Action space (dx, dy)\n",
    "        self.action_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Observation space (robot_x, robot_y, goal_x, goal_y)\n",
    "        self.observation_shape = 2+2+2+1+1+1+2+1+1 # Robot position, hand position, velocity_hand,radius_hand, and distance to hand\n",
    "\n",
    "        self.observation_space = Box(low=0, high=np.array([self.grid_size*2,self.grid_size, self.grid_size*2, self.grid_size,1,1 , (2**0.5)*self.grid_size,0.5*self.grid_size,0.5*self.grid_size,2*self.grid_size,self.grid_size,self.stride_robot_random[1],self.stride_hand_random[1]]), \n",
    "                                     shape=(self.observation_shape,), dtype=np.float32)\n",
    "\n",
    "        self.random = True\n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 50 # Pixels per grid unit\n",
    "        self.trajectory_points = [] # New: List to store past robot positions\n",
    "        self.dist_arm = 0\n",
    "\n",
    "\n",
    "    def dist_point_to_segment_correct(self,P, A, B, eps=1e-12):\n",
    "        P = np.asarray(P, dtype=float)\n",
    "        A = np.asarray(A, dtype=float)\n",
    "        B = np.asarray(B, dtype=float)\n",
    "        v = B - A\n",
    "        w = P - A\n",
    "        vv = np.dot(v, v)\n",
    "        if vv <= eps:\n",
    "            # A and B coincide: treat as point A\n",
    "            C = A.copy()\n",
    "            d = np.linalg.norm(P - A)\n",
    "            t = 0.0\n",
    "            case = 'endpoint_A'\n",
    "        else:\n",
    "            t = np.dot(w, v) / vv\n",
    "            if t < 0.0:\n",
    "                C = A\n",
    "                d = np.linalg.norm(P - A)\n",
    "                case = 'before_A'\n",
    "            elif t > 1.0:\n",
    "                C = B\n",
    "                d = np.linalg.norm(P - B)\n",
    "                case = 'after_B'\n",
    "            else:\n",
    "                C = A + t * v\n",
    "                d = np.linalg.norm(P - C)\n",
    "                case = 'on_segment'\n",
    "        return float(d), C, float(t), case\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return np.concatenate(([self.robot_position]+ [self.hand_position]+[self.last_action]+\n",
    "                               [np.array([self.current_distance])]+\n",
    "                               [np.array([min(self.robot_position[0],\n",
    "                                              self.robot_position[1],\n",
    "                                              self.grid_size-self.robot_position[0],\n",
    "                                              self.grid_size-self.robot_position[1]) ])]+\n",
    "                                              [np.array([self.dist_arm])]+\n",
    "                                               [self.fixed_point]+[np.array([self.stride_robot])]+[np.array([self.stride_hand])]))\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance_to_hand\": self.current_distance,\n",
    "            \"robot_position\": self.robot_position,\n",
    "            \"hand_position\": self.hand_position,\n",
    "            'distance_arm':self.dist_arm,\n",
    "            \"fix_point\":self.fixed_point,\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        super().reset()\n",
    "        self.distance = []\n",
    "        self.stride_robot = np.random.uniform(*self.stride_robot_random)  # Randomize stride length\n",
    "        self.stride_hand = np.random.uniform(*self.stride_hand_random)  # Randomize stride length\n",
    "        # self.stride_robot = 1  # Randomize stride length\n",
    "        self.distance_threshold_collision = np.random.uniform(2,3)  # Randomize collision threshold\n",
    "        self.distance_threshold_penalty = np.random.uniform(3, 4)  # Randomize penalty threshold\n",
    "        \n",
    "        \n",
    "        self.noise_obs_sigma = np.random.uniform(0, 0.1)  # Add some noise to observation to make it more realistic\n",
    "        self.noise_action_sigma = np.random.uniform(0,0.1)  # Add some noise to action to make it more realistic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.robot_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        self.hand_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        # self.hand_position = np.clip(self.hand_position, self.margin, self.grid_size-self.margin)  # Ensure hand stays within grid bounds\n",
    "        \n",
    "        # self.hand_move_mode = 'random' if np.random.rand() < 0.1 else 'towards_robot'  # Randomize hand movement mode\n",
    "        # self.hand_move_mode = 'towards_robot'\n",
    "        \n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.pre_distance = self.current_distance\n",
    "        self.last_action = np.zeros(2)\n",
    "        self.steps = 0\n",
    "        self.trajectory_points = [self.robot_position.copy()] # New: Reset trajectory and add initial position\n",
    "        \n",
    "        self.fixed_point = np.array([self.grid_size*random.uniform(0.2,1.8),self.grid_size])\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _reward(self,action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0  # Initialize reward\n",
    "        done_reason = None  # Initialize done reason\n",
    "\n",
    "        # action regulation penalty\n",
    "        # reward -= 0.5 * np.sum(np.square(action))  # Penalty for large actions\n",
    "\n",
    "        self.dist_arm = self.dist_point_to_segment_correct(self.robot_position,self.hand_position, self.fixed_point)[0]\n",
    "        if self.dist_arm < self.distance_threshold_arm:\n",
    "            reward += self.reward_arm \n",
    "            terminated = True  # Truncate if arm is too short\n",
    "\n",
    "        # boundary penalty\n",
    "        if np.any(self.robot_position <= self.margin) or (self.grid_size-self.robot_position[1] <=self.margin) or (2*self.grid_size-self.robot_position[0] <=self.margin):\n",
    "            reward += self.reward_bound\n",
    "            terminated = True  # Truncate if robot goes out of bounds\n",
    "            done_reason = \"out of bounds\"\n",
    "\n",
    "    \n",
    "        # Auxiliary Rewards -  distance to hand\n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.distance.append(self.current_distance)\n",
    "        reward += (self.current_distance-self.pre_distance)*self.distance_reward_factor  # Reward shaping based on distance change\n",
    "        self.pre_distance = self.current_distance\n",
    "\n",
    "        # Obstacle handling with two thresholds\n",
    "        if self.current_distance < self.distance_threshold_collision:\n",
    "            reward += self.reward_hand\n",
    "            terminated = True  # Terminate if too close to obstacles\n",
    "            done_reason = \"collision with obstacle\"\n",
    "        elif self.current_distance < self.distance_threshold_penalty:\n",
    "            reward -= self.penalty_factor * (self.distance_threshold_penalty - self.current_distance)  # Penalty for being too close to obstacles\n",
    "\n",
    "        reward -= self.smooth_action_penalty * np.linalg.norm(action - self.last_action)\n",
    "\n",
    "        # Small reward for each step taken to encourage exploration\n",
    "        reward+= self.reward_step \n",
    "\n",
    "        # Truncate if max steps reached and give max step reward\n",
    "        if self.steps >= self.max_steps:\n",
    "            reward += self.reward_max_step\n",
    "            truncated = True  \n",
    "\n",
    "        return reward,terminated,truncated,done_reason\n",
    "\n",
    "    def _get_hand_movement(self):\n",
    "\n",
    "        # if self.hand_move_mode == 'random':\n",
    "        #     move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        # elif self.hand_move_mode == 'towards_robot':\n",
    "        #     dir_vector = self.robot_position - self.hand_position\n",
    "        #     if np.linalg.norm(dir_vector) > 0:\n",
    "        #         dir_vector /= np.linalg.norm(dir_vector)\n",
    "        #     move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        if random.random() < self.hand_move_epsilon:\n",
    "            move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        else:\n",
    "            dir_vector = self.robot_position - self.hand_position\n",
    "            if np.linalg.norm(dir_vector) > 0:\n",
    "                dir_vector /= np.linalg.norm(dir_vector)\n",
    "            move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        \n",
    "        return move_hand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.random:\n",
    "            action+=np.random.normal(0,self.noise_action_sigma,size=self.action_space.shape)  # Add some noise to action to make it more realistic\n",
    "\n",
    "        move_hand = self._get_hand_movement()\n",
    "        self.hand_position += move_hand  # Update hand position\n",
    "        self.hand_position = np.clip(self.hand_position, self.margin, [self.grid_size*2,self.grid_size-self.margin])  # Ensure hand stays within grid bounds\n",
    "        # self.fixed_point+= np.array([np.,0])  # Randomize fixed point position\n",
    "\n",
    "        self.robot_position += action * self.stride_robot  # Scale the action to control speed\n",
    "        self.trajectory_points.append(self.robot_position.copy()) # New: Add current position to trajectory\n",
    "        self.steps += 1\n",
    "        \n",
    "\n",
    "        reward,terminated,truncated,done_reason = self._reward(action)\n",
    "        info = self._get_info()\n",
    "        info['done_reason'] = done_reason\n",
    "        info['distance_mean'] = np.mean(self.distance)\n",
    "        observation = self._get_obs()\n",
    "        if self.random:\n",
    "            observation += np.random.normal(0, self.noise_obs_sigma, size=self.observation_shape)  # Add some noise to observation to make it more realistic\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    " \n",
    "        pygame.display.init()\n",
    "        self.window = pygame.display.set_mode(\n",
    "                (int(self.grid_size * self.cell_size), int(self.grid_size * self.cell_size))\n",
    "            )\n",
    "        pygame.display.set_caption(\"CustomEnv\")\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                import sys\n",
    "                sys.exit() # Exit the program\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mouse_x, mouse_y = event.pos\n",
    "                self.hand_position = np.array([mouse_x/self.cell_size, mouse_y/self.cell_size])\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:  # 空格键切换暂停\n",
    "                    self.pause = not self.pause\n",
    "\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        canvas.fill(WHITE)\n",
    "        virus_image = pygame.image.load(\"hand.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "        robot_image = pygame.transform.scale(virus_image, (int(self.cell_size * 2), int(self.cell_size * 2)))  # Scale the image\n",
    "        # New: Draw the trajectory\n",
    "        if len(self.trajectory_points) > 1:\n",
    "            scaled_points = []\n",
    "            for point in self.trajectory_points:\n",
    "                scaled_points.append((int(point[0] * self.cell_size), int(point[1] * self.cell_size)))\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            pygame.draw.lines(canvas, BLUE, False, scaled_points, 2) # Blue line, not closed, 2 pixels wide\n",
    "            \n",
    "            # Optionally, draw small circles at each point to emphasize\n",
    "            for point_coord in scaled_points:\n",
    "                pygame.draw.circle(canvas, BLUE, point_coord, 3) # Small blue circles\n",
    "\n",
    "        # Draw robot\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            RED,\n",
    "            (int(self.robot_position[0] * self.cell_size), int(self.robot_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.2)\n",
    "        )\n",
    "        # Draw obstacles\n",
    "\n",
    "        canvas.blit(robot_image, (int((self.hand_position[0]-1) * self.cell_size), int((self.hand_position[1]-1) * self.cell_size+1)))\n",
    "        pygame.draw.circle(canvas,\n",
    "                            GREEN, \n",
    "                            (int((self.hand_position[0]) * self.cell_size), \n",
    "                            int((self.hand_position[1]) * self.cell_size+1)), \n",
    "        int(self.cell_size * 0.2)\n",
    "        )\n",
    "\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def load_args(self, args):\n",
    "        pass\n",
    "\n",
    "    def save_args(self,path):\n",
    "        env_args = {\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"distance_threshold_penalty\":self.distance_threshold_penalty,\n",
    "            \"distance_threshold_collision\":self.distance_threshold_collision,\n",
    "            \"penalty_factor\":self.penalty_factor,\n",
    "            \"distance_reward_factor\":self.distance_reward_factor,\n",
    "            \"smooth_action_penalty\":self.smooth_action_penalty,\n",
    "            \"max_steps\":self.max_steps,\n",
    "            \"margin\":self.margin,\n",
    "            \"reward_step\":self.reward_step,\n",
    "            \"reward_max_step\":self.reward_max_step,\n",
    "            \"reward_bound\":self.reward_bound,\n",
    "            \"reward_arm\":self.reward_arm,\n",
    "            \"reward_hand\":self.reward_hand,\n",
    "            \"stride_robot_range\":self.stride_robot_random,\n",
    "            \"stride_hand_range\":self.stride_hand_random,\n",
    "            \"move_hand_epsilon\":self.hand_move_epsilon,\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "        with open(os.path.join(path, \"env_args.json\"), \"w\") as f:\n",
    "            json.dump(env_args, f,indent=4)\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        pygame.display.quit()\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49588c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete,Tuple\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Define colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)  # Color for the trajectory\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self,render_mode=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.pause = False\n",
    "        self.domain_randomization = False\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define two separate thresholds for obstacle handling\n",
    "        self.distance_threshold_penalty = 5  # Penalty zone threshold (larger value)\n",
    "        self.distance_threshold_collision = 1.5  # Collision threshold (smaller value)\n",
    "        self.distance_threshold_arm = 3  # Arm threshold (smaller value)\n",
    "        self.penalty_factor = 5  # Penalty scaling factor\n",
    "        self.distance_reward_factor = 2\n",
    "        self.smooth_action_penalty = 2\n",
    "        self.steps = 0\n",
    "        self.margin = 0.3\n",
    "        self.reward_arm = -100\n",
    "        self.reward_hand = -100\n",
    "        self.reward_bound = -200\n",
    "        self.reward_max_step = 200\n",
    "        self.reward_step = 10\n",
    "        self.stride_robot = 2\n",
    "        self.stride_hand_random = [0.6,0.8]\n",
    "        self.hand_move_epsilon = 0.1\n",
    "\n",
    "\n",
    "        self.current_distance = 0  # Current distance to goal, used for reward shaping\n",
    "        self.max_steps = 50  # Set a maximum number of steps to prevent infinite loops\n",
    "        # Action space (dx, dy)\n",
    "        self.action_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Observation space (robot_x, robot_y, goal_x, goal_y)\n",
    "        self.observation_shape = 2+2+2+1+1+1+2+1  # Robot position, hand position, velocity_hand,radius_hand, and distance to hand\n",
    "        self.observation_space = Box(low=0, high=np.array([self.grid_size*2,self.grid_size, self.grid_size*2, self.grid_size,1,1 , (2**0.5)*self.grid_size,0.5*self.grid_size,0.5*self.grid_size,2*self.grid_size,self.grid_size]), \n",
    "                                     shape=(self.observation_shape,), dtype=np.float32)\n",
    "\n",
    "        self.random = True\n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 50 # Pixels per grid unit\n",
    "        self.trajectory_points = [] # New: List to store past robot positions\n",
    "        self.dist_arm = 0\n",
    "\n",
    "\n",
    "    def dist_point_to_segment_correct(self,P, A, B, eps=1e-12):\n",
    "        P = np.asarray(P, dtype=float)\n",
    "        A = np.asarray(A, dtype=float)\n",
    "        B = np.asarray(B, dtype=float)\n",
    "        v = B - A\n",
    "        w = P - A\n",
    "        vv = np.dot(v, v)\n",
    "        if vv <= eps:\n",
    "            # A and B coincide: treat as point A\n",
    "            C = A.copy()\n",
    "            d = np.linalg.norm(P - A)\n",
    "            t = 0.0\n",
    "            case = 'endpoint_A'\n",
    "        else:\n",
    "            t = np.dot(w, v) / vv\n",
    "            if t < 0.0:\n",
    "                C = A\n",
    "                d = np.linalg.norm(P - A)\n",
    "                case = 'before_A'\n",
    "            elif t > 1.0:\n",
    "                C = B\n",
    "                d = np.linalg.norm(P - B)\n",
    "                case = 'after_B'\n",
    "            else:\n",
    "                C = A + t * v\n",
    "                d = np.linalg.norm(P - C)\n",
    "                case = 'on_segment'\n",
    "        return float(d), C, float(t), case\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return np.concatenate(([self.robot_position]+ [self.hand_position]+[self.last_action]+\n",
    "                               [np.array([self.current_distance])]+\n",
    "                               [np.array([min(self.robot_position[0],\n",
    "                                              self.robot_position[1],\n",
    "                                              self.grid_size-self.robot_position[0],\n",
    "                                              self.grid_size-self.robot_position[1]) ])]+\n",
    "                                              [np.array([self.dist_arm])]+\n",
    "                                               [self.fixed_point]))\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance_to_hand\": self.current_distance,\n",
    "            \"robot_position\": self.robot_position,\n",
    "            \"hand_position\": self.hand_position,\n",
    "            'distance_arm':self.dist_arm,\n",
    "            \"fix_point\":self.fixed_point,\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        super().reset()\n",
    "        self.distance = []\n",
    "        self.stride_hand = np.random.uniform(*self.stride_hand_random)  # Randomize stride length\n",
    "        # self.stride_robot = 1  # Randomize stride length\n",
    "        self.distance_threshold_collision = np.random.uniform(2,3)  # Randomize collision threshold\n",
    "        self.distance_threshold_penalty = np.random.uniform(3, 4)  # Randomize penalty threshold\n",
    "        \n",
    "        \n",
    "        self.noise_obs_sigma = np.random.uniform(0, 0.1)  # Add some noise to observation to make it more realistic\n",
    "        self.noise_action_sigma = np.random.uniform(0,0.1)  # Add some noise to action to make it more realistic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.robot_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        self.hand_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        # self.hand_position = np.clip(self.hand_position, self.margin, self.grid_size-self.margin)  # Ensure hand stays within grid bounds\n",
    "        \n",
    "        # self.hand_move_mode = 'random' if np.random.rand() < 0.1 else 'towards_robot'  # Randomize hand movement mode\n",
    "        # self.hand_move_mode = 'towards_robot'\n",
    "        \n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.pre_distance = self.current_distance\n",
    "        self.last_action = np.zeros(2)\n",
    "        self.steps = 0\n",
    "        self.trajectory_points = [self.robot_position.copy()] # New: Reset trajectory and add initial position\n",
    "        \n",
    "        self.fixed_point = np.array([self.grid_size*random.uniform(0.2,1.8),self.grid_size])\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _reward(self,action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0  # Initialize reward\n",
    "        done_reason = None  # Initialize done reason\n",
    "\n",
    "        # action regulation penalty\n",
    "        # reward -= 0.5 * np.sum(np.square(action))  # Penalty for large actions\n",
    "\n",
    "        self.dist_arm = self.dist_point_to_segment_correct(self.robot_position,self.hand_position, self.fixed_point)[0]\n",
    "        if self.dist_arm < self.distance_threshold_arm:\n",
    "            reward += self.reward_arm \n",
    "            terminated = True  # Truncate if arm is too short\n",
    "\n",
    "        # boundary penalty\n",
    "        if np.any(self.robot_position <= self.margin) or (self.grid_size-self.robot_position[1] <=self.margin) or (2*self.grid_size-self.robot_position[0] <=self.margin):\n",
    "            reward += self.reward_bound\n",
    "            terminated = True  # Truncate if robot goes out of bounds\n",
    "            done_reason = \"out of bounds\"\n",
    "\n",
    "    \n",
    "        # Auxiliary Rewards -  distance to hand\n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.distance.append(self.current_distance)\n",
    "        reward += (self.current_distance-self.pre_distance)*self.distance_reward_factor  # Reward shaping based on distance change\n",
    "        self.pre_distance = self.current_distance\n",
    "\n",
    "        # Obstacle handling with two thresholds\n",
    "        if self.current_distance < self.distance_threshold_collision:\n",
    "            reward += self.reward_hand\n",
    "            terminated = True  # Terminate if too close to obstacles\n",
    "            done_reason = \"collision with obstacle\"\n",
    "        elif self.current_distance < self.distance_threshold_penalty:\n",
    "            reward -= self.penalty_factor * (self.distance_threshold_penalty - self.current_distance)  # Penalty for being too close to obstacles\n",
    "\n",
    "        reward -= self.smooth_action_penalty * np.linalg.norm(action - self.last_action)\n",
    "\n",
    "        # Small reward for each step taken to encourage exploration\n",
    "        reward+= self.reward_step \n",
    "\n",
    "        # Truncate if max steps reached and give max step reward\n",
    "        if self.steps >= self.max_steps:\n",
    "            reward += self.reward_max_step\n",
    "            truncated = True  \n",
    "\n",
    "        return reward,terminated,truncated,done_reason\n",
    "\n",
    "    def _get_hand_movement(self):\n",
    "\n",
    "        # if self.hand_move_mode == 'random':\n",
    "        #     move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        # elif self.hand_move_mode == 'towards_robot':\n",
    "        #     dir_vector = self.robot_position - self.hand_position\n",
    "        #     if np.linalg.norm(dir_vector) > 0:\n",
    "        #         dir_vector /= np.linalg.norm(dir_vector)\n",
    "        #     move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        if random.random() < self.hand_move_epsilon:\n",
    "            move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        else:\n",
    "            dir_vector = self.robot_position - self.hand_position\n",
    "            if np.linalg.norm(dir_vector) > 0:\n",
    "                dir_vector /= np.linalg.norm(dir_vector)\n",
    "            move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        \n",
    "        return move_hand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_obs(self, action):\n",
    "        pass\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.random:\n",
    "            action+=np.random.normal(0,self.noise_action_sigma,size=self.action_space.shape)  # Add some noise to action to make it more realistic\n",
    "\n",
    "        self.update_obs(action)\n",
    "\n",
    "\n",
    "        self.trajectory_points.append(self.robot_position.copy()) # New: Add current position to trajectory\n",
    "        self.steps += 1\n",
    "        \n",
    "\n",
    "        reward,terminated,truncated,done_reason = self._reward(action)\n",
    "        info = self._get_info()\n",
    "        info['done_reason'] = done_reason\n",
    "        info['distance_mean'] = np.mean(self.distance)\n",
    "        observation = self._get_obs()\n",
    "        if self.random:\n",
    "            observation += np.random.normal(0, self.noise_obs_sigma, size=self.observation_shape)  # Add some noise to observation to make it more realistic\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    " \n",
    "        pygame.display.init()\n",
    "        self.window = pygame.display.set_mode(\n",
    "                (int(self.grid_size * self.cell_size), int(self.grid_size * self.cell_size))\n",
    "            )\n",
    "        pygame.display.set_caption(\"CustomEnv\")\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                import sys\n",
    "                sys.exit() # Exit the program\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mouse_x, mouse_y = event.pos\n",
    "                self.hand_position = np.array([mouse_x/self.cell_size, mouse_y/self.cell_size])\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:  # 空格键切换暂停\n",
    "                    self.pause = not self.pause\n",
    "\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        canvas.fill(WHITE)\n",
    "        virus_image = pygame.image.load(\"hand.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "        robot_image = pygame.transform.scale(virus_image, (int(self.cell_size * 2), int(self.cell_size * 2)))  # Scale the image\n",
    "        # New: Draw the trajectory\n",
    "        if len(self.trajectory_points) > 1:\n",
    "            scaled_points = []\n",
    "            for point in self.trajectory_points:\n",
    "                scaled_points.append((int(point[0] * self.cell_size), int(point[1] * self.cell_size)))\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            pygame.draw.lines(canvas, BLUE, False, scaled_points, 2) # Blue line, not closed, 2 pixels wide\n",
    "            \n",
    "            # Optionally, draw small circles at each point to emphasize\n",
    "            for point_coord in scaled_points:\n",
    "                pygame.draw.circle(canvas, BLUE, point_coord, 3) # Small blue circles\n",
    "\n",
    "        # Draw robot\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            RED,\n",
    "            (int(self.robot_position[0] * self.cell_size), int(self.robot_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.2)\n",
    "        )\n",
    "        # Draw obstacles\n",
    "\n",
    "        canvas.blit(robot_image, (int((self.hand_position[0]-1) * self.cell_size), int((self.hand_position[1]-1) * self.cell_size+1)))\n",
    "        pygame.draw.circle(canvas,\n",
    "                            GREEN, \n",
    "                            (int((self.hand_position[0]) * self.cell_size), \n",
    "                            int((self.hand_position[1]) * self.cell_size+1)), \n",
    "        int(self.cell_size * 0.2)\n",
    "        )\n",
    "\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def load_args(self, args):\n",
    "        pass\n",
    "\n",
    "    def save_args(self,path):\n",
    "        env_args = {\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"distance_threshold_penalty\":self.distance_threshold_penalty,\n",
    "            \"distance_threshold_collision\":self.distance_threshold_collision,\n",
    "            \"penalty_factor\":self.penalty_factor,\n",
    "            \"distance_reward_factor\":self.distance_reward_factor,\n",
    "            \"smooth_action_penalty\":self.smooth_action_penalty,\n",
    "            \"max_steps\":self.max_steps,\n",
    "            \"margin\":self.margin,\n",
    "            \"reward_step\":self.reward_step,\n",
    "            \"reward_max_step\":self.reward_max_step,\n",
    "            \"reward_bound\":self.reward_bound,\n",
    "            \"reward_arm\":self.reward_arm,\n",
    "            \"reward_hand\":self.reward_hand,\n",
    "            \"stride_robot\":self.stride_robot,\n",
    "            \"stride_hand_range\":self.stride_hand_random,\n",
    "            \"move_hand_epsilon\":self.hand_move_epsilon,\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "        with open(os.path.join(path, \"env_args.json\"), \"w\") as f:\n",
    "            json.dump(env_args, f,indent=4)\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        pygame.display.quit()\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa40b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_environment(robot_position, hand_position, fix_point,trajectory_points, grid_size=10, cell_size=50):\n",
    "    WHITE = (255, 255, 255)\n",
    "    RED = (255, 0, 0)\n",
    "    GREEN = (0, 255, 0)\n",
    "    BLUE = (0, 0, 255)\n",
    "    pygame.init()\n",
    "    window = pygame.display.set_mode((grid_size * cell_size*2, grid_size * cell_size))\n",
    "    canvas = pygame.Surface((grid_size * cell_size*2, grid_size * cell_size))\n",
    "    canvas.fill(WHITE)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            import sys\n",
    "            sys.exit() # Exit the program\n",
    "\n",
    "        elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            mouse_x, mouse_y = event.pos\n",
    "            hand_position = np.array([mouse_x/cell_size, mouse_y/cell_size])\n",
    "\n",
    "    if len(trajectory_points) > 1:\n",
    "        scaled_points = [(int(point[0] * cell_size), int(point[1] * cell_size)) for point in trajectory_points]\n",
    "        pygame.draw.lines(canvas, BLUE, False, scaled_points, 2)\n",
    "        for point_coord in scaled_points:\n",
    "            pygame.draw.circle(canvas, BLUE, point_coord, 3)\n",
    "\n",
    "    pygame.draw.lines(canvas, (255, 224, 189), False,[hand_position*cell_size, [fix_point[0]*cell_size,fix_point[1]*cell_size]],width=25)\n",
    "\n",
    "    virus_image = pygame.image.load(\"../hand.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "    robot_image = pygame.transform.scale(virus_image, (int(cell_size * 2), int(cell_size * 2)))  # Scale the \n",
    "    pygame.draw.circle(canvas, RED, (int(robot_position[0] * cell_size), int(robot_position[1] * cell_size)), int(cell_size * 0.2))\n",
    "    pygame.draw.circle(canvas, GREEN, (int(hand_position[0] * cell_size), int(hand_position[1] * cell_size)), int(cell_size * 0.2))\n",
    "    canvas.blit(robot_image, (int((hand_position[0]-1) * cell_size), int((hand_position[1]-1) * cell_size)))\n",
    "    font = pygame.font.Font(None, 24)\n",
    "    text = font.render(f\"{hand_position[0]},{hand_position[1]}\", True, BLUE)\n",
    "    text_rect = text.get_rect()\n",
    "    text_rect.center = (int(hand_position[0] * cell_size), int(hand_position[1] * cell_size))\n",
    "    window.blit(canvas, canvas.get_rect())\n",
    "    # window.blit(text, text_rect)\n",
    "    pygame.display.flip()\n",
    "\n",
    "    return hand_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8670f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=10000, n_episodes=1, log_freq=10000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        # 用deque统计最近N个done的终止原因，避免内存爆炸\n",
    "        self.termination_reasons = deque(maxlen=1000)  # 统计最近1000次终止\n",
    "        self.env_to_render = env\n",
    "        self.render_freq = render_freq\n",
    "        self.n_episodes = n_episodes\n",
    "        self.distance_mean = deque(maxlen=1000) \n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 先从env info里读取终止原因\n",
    "        # print((self.locals.keys()))\n",
    "        infos = self.locals.get('infos', None)\n",
    "\n",
    "        dones = self.locals.get('dones', None)\n",
    "        if infos is not None and dones is not None:\n",
    "            for done, info in zip(dones, infos):\n",
    "                if done and info is not None and 'done_reason' in info:\n",
    "                    self.termination_reasons.append(info['done_reason'])\n",
    "                    self.distance_mean.append(info['distance_mean'])\n",
    "\n",
    "        # 每log_freq步打印信息\n",
    "        # if self.num_timesteps % self.render_freq == 0 and self.verbose:\n",
    "        #     for ep in range(self.n_episodes):\n",
    "        #         obs = self.env_to_render.reset()\n",
    "        #         done = False\n",
    "        #         while not done:\n",
    "        #             action, _states = self.model.predict(obs, deterministic=True)\n",
    "        #             obs, rewards, done, info = self.env_to_render.step(action)\n",
    "        #             self.env_to_render.render()\n",
    "        #             time.sleep(0.6)\n",
    "\n",
    "        #             if done:\n",
    "\n",
    "        #                 self.env_to_render.close()\n",
    "        #                 break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.num_timesteps % self.log_freq == 0 and self.verbose:\n",
    "            log = self.model.logger.name_to_value\n",
    "            ep_rew = log.get('rollout/ep_rew_mean', None)\n",
    "            ep_len = log.get('rollout/ep_len_mean', None)\n",
    "            loss = log.get('train/loss', None)\n",
    "            v_loss = log.get('train/value_loss', None)\n",
    "            p_loss = log.get('train/policy_gradient_loss', None)\n",
    "            ent_loss = log.get('train/entropy_loss', None)\n",
    "            kl = log.get('train/approx_kl', None)\n",
    "\n",
    "            # 统计终止原因比例\n",
    "            total = len(self.termination_reasons)\n",
    "            if total > 0:\n",
    "                count_hand = sum(1 for r in self.termination_reasons if r == 'out of bounds')\n",
    "                ratio_hand = count_hand / total\n",
    "            else:\n",
    "                ratio_hand = 0.0\n",
    "            distance_mean = sum(self.distance_mean) / len(self.distance_mean) if len(self.distance_mean) > 0 else 0.0\n",
    "\n",
    "            # print(f\"[{self.num_timesteps:7d}] ep_rew_mean={ep_rew}, ep_len_mean={ep_len}, loss={loss:.3f}, \"\n",
    "            #       f\"v_loss={v_loss:.3f}, p_loss={p_loss:.3f}, ent_loss={ent_loss:.3f}, kl={kl:.4f}, \"\n",
    "            #       f\"termination_reason_hand_ratio={ratio_hand:.3f}\")\n",
    "            self.logger.record(\"custom/termination_reason_ratio\", ratio_hand)\n",
    "\n",
    "            self.logger.record(\"custom/distance_mean\", distance_mean)\n",
    "            self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aba57ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1385666487.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 31\u001b[1;36m\u001b[0m\n\u001b[1;33m    direction =\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def metric(trajectory):\n",
    "    \"\"\"\n",
    "    trajectory: list of tuples, each tuple contains (observation, action, hand_movement, reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(trajectory, list):\n",
    "        raise TypeError(\"trajectory should be a list of tuples\")\n",
    "\n",
    "    # distance = [x[6] for x in trajectory]\n",
    "    # return distance\n",
    "    \n",
    "    # plt.hist(distance, bins=10, density=True,edgecolor='black', alpha=0.7,color='skyblue')\n",
    "    # plt.xlabel('distance')\n",
    "    # plt.ylabel('density')\n",
    "    # plt.title('distance distribution')\n",
    "    # plt.pause(0.1)\n",
    "\n",
    "    # distance_aproximity\n",
    "    for item in trajectory:\n",
    "        obs = item[0]\n",
    "        distance = obs[0:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # direction_alignment\n",
    "    for item in trajectory:\n",
    "        obs = item[0]\n",
    "        direction = \n",
    "        hand_movement = item[2]\n",
    "\n",
    "        position_robot,position_hand = obs[0:2],obs[2:4]\n",
    "        direction = position_robot - position_hand\n",
    "        consine_angle = np.dot(direction, hand_movement) / (np.linalg.norm(direction) * np.linalg.norm(hand_movement))\n",
    "        angle = np.arccos(consine_angle)\n",
    "\n",
    "    # reaction_time\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a375fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda\\envs\\RL\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Box high.shape doesn't match provided shape, high.shape=(11,), shape=(12,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m configure\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m safe_mean\n\u001b[1;32m---> 11\u001b[0m env_raw \u001b[38;5;241m=\u001b[39m \u001b[43mCustomEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/best_model_sac\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 47\u001b[0m, in \u001b[0;36mCustomEnv.__init__\u001b[1;34m(self, render_mode)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Observation space (robot_x, robot_y, goal_x, goal_y)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Robot position, hand position, velocity_hand,radius_hand, and distance to hand\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m \u001b[43mBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# For rendering\u001b[39;00m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\gymnasium\\spaces\\box.py:156\u001b[0m, in \u001b[0;36mBox.__init__\u001b[1;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox low.shape doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match provided shape, low.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m shape:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox high.shape doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match provided shape, high.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# check that low <= high\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh):\n",
      "\u001b[1;31mValueError\u001b[0m: Box high.shape doesn't match provided shape, high.shape=(11,), shape=(12,)"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from stable_baselines3 import PPO,SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.commom.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import safe_mean\n",
    "\n",
    "env_raw = CustomEnv()\n",
    "save_path = \"logs/best_model_sac\"\n",
    "i=1\n",
    "while os.path.exists(save_path+str(i)):\n",
    "    i+=1\n",
    "save_path = f\"{save_path}{i}\"\n",
    "os.makedirs(save_path)\n",
    "\n",
    "tensorboard_dir = \"./sac_custom_env_tensorboard\"\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "tensorboard_log_dir = tensorboard_dir + '/'+ f'best_model_sac{i}'\n",
    "load_model = \"logs/best_model_sac44/best_model.zip\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env =DummyVecEnv([lambda: Monitor(env_raw)])  # Wrap the environment with Monitor for logging\n",
    "\n",
    "# env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128,256,256,128], qf=[128,256,256,128])],\n",
    "    # activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[128,256,256,128],\n",
    "    # activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1,policy_kwargs=policy_kwargs,tensorboard_log=\"./ppo_custom_env_tensorboard/\")\n",
    "# model = PPO.load(\"logs/best_model4/best_model.zip\",env=env)  # Load the best model\n",
    "# model.learning_rate = 0.0008\n",
    "# model.ent_coef = 0.02  # Set a lower learning rate for fine-tuning\n",
    "\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1,ent_coef='auto',policy_kwargs=policy_kwargs,tensorboard_log=tensorboard_log_dir)\n",
    "replay_buffer = model.replay_buffer\n",
    "# model = SAC.load(load_model, env, verbose=1,ent_coef='auto',learning_rate=0.0001,tensorboard_log=tensorboard_log_dir)\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=save_path,\n",
    "    log_path = './logs/',\n",
    "    eval_freq=10,  # 每1000步评估一次\n",
    "    deterministic=True,\n",
    "    render=True,\n",
    "    n_eval_episodes=10,  # 每次评估5个episode\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "debug_callback = DebugCallback(env=env,log_freq=10000, verbose=1)\n",
    "callback = CallbackList([eval_callback, debug_callback])\n",
    "\n",
    "# model._logger = configure(\"./logs/manual_sac_train/\", [\"stdout\", \"tensorboard\"])\n",
    "# model._current_progress_remaining = 1.0\n",
    "\n",
    "model._setup_learn(total_timesteps=400000,callback=callback)\n",
    "callback.on_training_start(locals(), globals())\n",
    "# model.learn(total_timesteps=400000,callback=callback)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "s = 0\n",
    "info_buffer = deque(maxlen=100)\n",
    "for step in range(1000000):\n",
    "    model.num_timesteps+=1\n",
    "\n",
    "\n",
    "    # get obs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    # next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "    #execute action\n",
    "\n",
    "    # compute reward,done,info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # get next obs\n",
    "\n",
    "\n",
    "    replay_buffer.add(obs,next_obs, action, reward, done, info)\n",
    "\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        model.train(batch_size=64, gradient_steps=1)\n",
    "    s+=1\n",
    "    if done:\n",
    "        model._episode_num += 1\n",
    "        info_buffer.append([s])\n",
    "        s = 0\n",
    "        if model._episode_num % 4 == 0:\n",
    "            # model.dump_logs()\n",
    "            # model.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in model.ep_info_buffer]))\n",
    "            # model.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[0] for ep_info in info_buffer]))\n",
    "            model.logger.dump(step=model.num_timesteps)\n",
    "        # obs = env.reset()\n",
    "        done = False\n",
    "    callback.on_step()\n",
    "callback.on_training_end()\n",
    "# env.save(\"vec_env.pkl\")  # Save the environment state\n",
    "\n",
    "settings = {\n",
    "    'load_model':load_model,\n",
    "    'tensorboard_log' :tensorboard_log_dir,\n",
    "           }\n",
    "env_raw.save_args(save_path)\n",
    "with open(os.path.join(save_path, \"settings.json\"), \"w\") as f:\n",
    "    json.dump(settings, f)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Observation spaces do not match: Box(0.0, [20.       10.       20.       10.        1.        1.       14.142136\n  5.        5.       20.       10.      ], (11,), float32) != Box(0.0, [20.       10.       20.       10.        1.        1.       14.142136\n  5.        5.       20.       10.        3.        1.      ], (13,), float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m env \u001b[38;5;241m=\u001b[39m CustomEnv()\n\u001b[0;32m      9\u001b[0m env\u001b[38;5;241m.\u001b[39mrandom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model\u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogs/best_model_sac44/best_model.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m obs,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     15\u001b[0m env\u001b[38;5;241m.\u001b[39mstride_hand \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:717\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_env(env, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# Check if given env is valid\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m \u001b[43mcheck_for_correct_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;66;03m# Discard `_last_obs`, this will force the env to reset before training\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;66;03m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reset \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\utils.py:232\u001b[0m, in \u001b[0;36mcheck_for_correct_spaces\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03mChecks that the environment has same spaces as provided ones. Used by BaseAlgorithm to check if\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mspaces match after loading the model with given env.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m:param action_space: Action space to check against\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Observation spaces do not match: Box(0.0, [20.       10.       20.       10.        1.        1.       14.142136\n  5.        5.       20.       10.      ], (11,), float32) != Box(0.0, [20.       10.       20.       10.        1.        1.       14.142136\n  5.        5.       20.       10.        3.        1.      ], (13,), float32)"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import SAC,PPO\n",
    "# env =DummyVecEnv([lambda: Monitor(CustomEnv())])  # Wrap the environment with Monitor for logging\n",
    "\n",
    "# env = VecNormalize.load('vec_norm.pkl',env)\n",
    "# env = VecNormalize(env)  # Apply normalization to the environment\n",
    "# seed = np.random.randint(0,1000)\n",
    "# from custom_env import CustomEnv\n",
    "env = CustomEnv()\n",
    "env.random = False\n",
    "\n",
    "\n",
    "model= SAC.load(\"logs/best_model_sac59/best_model.zip\",env=env)  # Load the best model\n",
    "\n",
    "obs,_ = env.reset()\n",
    "env.stride_hand = 1\n",
    "env.stride_robot = 1.2\n",
    "state_history = []\n",
    "max_steps = 40000\n",
    "\n",
    "for i in range(max_steps):\n",
    "    obs = env._get_obs()\n",
    "    info = env._get_info()\n",
    "\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, teminated,_, info = env.step(action)\n",
    "    state_history.append(obs)\n",
    "    # print(f\"obs:{obs}\")\n",
    "    # print(f\"action:{action}\")\n",
    "\n",
    "\n",
    "\n",
    "    # print(info[\"robot_position\"],info[\"hand_position\"])\n",
    "    # print(\"Reward:\", reward)\n",
    "    # print(\"distance_arm:\",info['distance_arm'])\n",
    "    # env.render()\n",
    "    env.hand_position = render_environment(info[\"robot_position\"], info[\"hand_position\"],info[\"fix_point\"], trajectory_points=env.trajectory_points)\n",
    "    time.sleep(0.6)  # Control the frame rate\n",
    "    if teminated:\n",
    "        env.hand_position = render_environment(info[\"robot_position\"], info[\"hand_position\"],info[\"fix_point\"], trajectory_points=env.trajectory_points)\n",
    "        time.sleep(0.6)\n",
    "        env.close()\n",
    "        break\n",
    "        print(\"Resetting environment\")\n",
    "metric(state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb9ee739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['grid_size', 'pause', 'domain_randomization', 'render_mode', 'distance_threshold_penalty', 'distance_threshold_collision', 'distance_threshold_arm', 'penalty_factor', 'distance_reward_factor', 'smooth_action_penalty', 'steps', 'margin', 'reward_arm', 'reward_hand', 'reward_bound', 'reward_max_step', 'reward_step', 'stride_robot_random', 'stride_hand_random', 'hand_move_epsilon', 'current_distance', 'max_steps', 'action_space', 'observation_shape', 'observation_space', 'random', 'window', 'clock', 'cell_size', 'trajectory_points', 'dist_arm'])\n",
      "dict_keys(['grid_size', 'pause', 'domain_randomization', 'render_mode', 'distance_threshold_penalty', 'distance_threshold_collision', 'distance_threshold_arm', 'penalty_factor', 'distance_reward_factor', 'smooth_action_penalty', 'steps', 'margin', 'reward_arm', 'reward_hand', 'reward_bound', 'reward_max_step', 'reward_step', 'stride_robot_random', 'stride_hand_random', 'hand_move_epsilon', 'current_distance', 'max_steps', 'action_space', 'observation_shape', 'observation_space', 'random', 'window', 'clock', 'cell_size', 'trajectory_points', 'dist_arm', 'stride_hand'])\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "print(env.__dict__.keys())\n",
    "env.stride_hand = 1\n",
    "print(env.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143107ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b93ec99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric(state_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
