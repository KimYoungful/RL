{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60538f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import random\n",
    "import pygame\n",
    "from loguru import logger\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Constants ---\n",
    "SCREEN_WIDTH = 1200\n",
    "SCREEN_HEIGHT = 800\n",
    "FPS = 60\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (220, 50, 50)\n",
    "GREEN = (50, 220, 50)\n",
    "BLUE = (50, 50, 220)\n",
    "GRAY = (150, 150, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045bbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete,Tuple\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Define colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)  # Color for the trajectory\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self,render_mode=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.pause = False\n",
    "        self.domain_randomization = False\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define two separate thresholds for obstacle handling\n",
    "        self.distance_threshold_penalty = 5  # Penalty zone threshold (larger value)\n",
    "        self.distance_threshold_collision = 1.5  # Collision threshold (smaller value)\n",
    "        self.distance_threshold_arm = 3  # Arm threshold (smaller value)\n",
    "        self.penalty_factor = 5  # Penalty scaling factor\n",
    "        self.distance_reward_factor = 2\n",
    "        self.smooth_action_penalty = 2\n",
    "        self.steps = 0\n",
    "        self.margin = 0.3\n",
    "        self.reward_arm = -100\n",
    "        self.reward_hand = -100\n",
    "        self.reward_bound = -200\n",
    "        self.reward_max_step = 200\n",
    "        self.reward_step = 10\n",
    "        self.stride_robot_random = [1,3]\n",
    "        self.stride_hand_random = [0.6,1]\n",
    "        self.hand_move_epsilon = 0.1\n",
    "\n",
    "\n",
    "        self.current_distance = 0  # Current distance to goal, used for reward shaping\n",
    "        self.max_steps = 50  # Set a maximum number of steps to prevent infinite loops\n",
    "        # Action space (dx, dy)\n",
    "        self.action_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Observation space (robot_x, robot_y, goal_x, goal_y)\n",
    "        self.observation_shape = 2+2+2+1+1+1+2+1+1 # Robot position, hand position, velocity_hand,radius_hand, and distance to hand\n",
    "\n",
    "        self.observation_space = Box(low=0, high=np.array([self.grid_size*2,self.grid_size, self.grid_size*2, self.grid_size,1,1 , (2**0.5)*self.grid_size,0.5*self.grid_size,0.5*self.grid_size,2*self.grid_size,self.grid_size,self.stride_robot_random[1],self.stride_hand_random[1]]), \n",
    "                                     shape=(self.observation_shape,), dtype=np.float32)\n",
    "\n",
    "        self.random = True\n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 50 # Pixels per grid unit\n",
    "        self.trajectory_points = [] # New: List to store past robot positions\n",
    "        self.dist_arm = 0\n",
    "\n",
    "\n",
    "    def dist_point_to_segment_correct(self,P, A, B, eps=1e-12):\n",
    "        P = np.asarray(P, dtype=float)\n",
    "        A = np.asarray(A, dtype=float)\n",
    "        B = np.asarray(B, dtype=float)\n",
    "        v = B - A\n",
    "        w = P - A\n",
    "        vv = np.dot(v, v)\n",
    "        if vv <= eps:\n",
    "            # A and B coincide: treat as point A\n",
    "            C = A.copy()\n",
    "            d = np.linalg.norm(P - A)\n",
    "            t = 0.0\n",
    "            case = 'endpoint_A'\n",
    "        else:\n",
    "            t = np.dot(w, v) / vv\n",
    "            if t < 0.0:\n",
    "                C = A\n",
    "                d = np.linalg.norm(P - A)\n",
    "                case = 'before_A'\n",
    "            elif t > 1.0:\n",
    "                C = B\n",
    "                d = np.linalg.norm(P - B)\n",
    "                case = 'after_B'\n",
    "            else:\n",
    "                C = A + t * v\n",
    "                d = np.linalg.norm(P - C)\n",
    "                case = 'on_segment'\n",
    "        return float(d), C, float(t), case\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return np.concatenate(([self.robot_position]+ [self.hand_position]+[self.last_action]+\n",
    "                               [np.array([self.current_distance])]+\n",
    "                               [np.array([min(self.robot_position[0],\n",
    "                                              self.robot_position[1],\n",
    "                                              self.grid_size-self.robot_position[0],\n",
    "                                              self.grid_size-self.robot_position[1]) ])]+\n",
    "                                              [np.array([self.dist_arm])]+\n",
    "                                               [self.fixed_point]+[np.array([self.stride_robot])]+[np.array([self.stride_hand])]))\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance_to_hand\": self.current_distance,\n",
    "            \"robot_position\": self.robot_position,\n",
    "            \"hand_position\": self.hand_position,\n",
    "            'distance_arm':self.dist_arm,\n",
    "            \"fix_point\":self.fixed_point,\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        super().reset()\n",
    "        self.distance = []\n",
    "        self.stride_robot = np.random.uniform(*self.stride_robot_random)  # Randomize stride length\n",
    "        self.stride_hand = np.random.uniform(*self.stride_hand_random)  # Randomize stride length\n",
    "        # self.stride_robot = 1  # Randomize stride length\n",
    "        self.distance_threshold_collision = np.random.uniform(2,3)  # Randomize collision threshold\n",
    "        self.distance_threshold_penalty = np.random.uniform(3, 4)  # Randomize penalty threshold\n",
    "        \n",
    "        \n",
    "        self.noise_obs_sigma = np.random.uniform(0, 0.1)  # Add some noise to observation to make it more realistic\n",
    "        self.noise_action_sigma = np.random.uniform(0,0.1)  # Add some noise to action to make it more realistic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.robot_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        self.hand_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        # self.hand_position = np.clip(self.hand_position, self.margin, self.grid_size-self.margin)  # Ensure hand stays within grid bounds\n",
    "        \n",
    "        # self.hand_move_mode = 'random' if np.random.rand() < 0.1 else 'towards_robot'  # Randomize hand movement mode\n",
    "        # self.hand_move_mode = 'towards_robot'\n",
    "        \n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.pre_distance = self.current_distance\n",
    "        self.last_action = np.zeros(2)\n",
    "        self.steps = 0\n",
    "        self.trajectory_points = [self.robot_position.copy()] # New: Reset trajectory and add initial position\n",
    "        \n",
    "        self.fixed_point = np.array([self.grid_size*random.uniform(0.2,1.8),self.grid_size])\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _reward(self,action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0  # Initialize reward\n",
    "        done_reason = None  # Initialize done reason\n",
    "\n",
    "        # action regulation penalty\n",
    "        # reward -= 0.5 * np.sum(np.square(action))  # Penalty for large actions\n",
    "\n",
    "        self.dist_arm = self.dist_point_to_segment_correct(self.robot_position,self.hand_position, self.fixed_point)[0]\n",
    "        if self.dist_arm < self.distance_threshold_arm:\n",
    "            reward += self.reward_arm \n",
    "            terminated = True  # Truncate if arm is too short\n",
    "\n",
    "        # boundary penalty\n",
    "        if np.any(self.robot_position <= self.margin) or (self.grid_size-self.robot_position[1] <=self.margin) or (2*self.grid_size-self.robot_position[0] <=self.margin):\n",
    "            reward += self.reward_bound\n",
    "            terminated = True  # Truncate if robot goes out of bounds\n",
    "            done_reason = \"out of bounds\"\n",
    "\n",
    "    \n",
    "        # Auxiliary Rewards -  distance to hand\n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.distance.append(self.current_distance)\n",
    "        reward += (self.current_distance-self.pre_distance)*self.distance_reward_factor  # Reward shaping based on distance change\n",
    "        self.pre_distance = self.current_distance\n",
    "\n",
    "        # Obstacle handling with two thresholds\n",
    "        if self.current_distance < self.distance_threshold_collision:\n",
    "            reward += self.reward_hand\n",
    "            terminated = True  # Terminate if too close to obstacles\n",
    "            done_reason = \"collision with obstacle\"\n",
    "        elif self.current_distance < self.distance_threshold_penalty:\n",
    "            reward -= self.penalty_factor * (self.distance_threshold_penalty - self.current_distance)  # Penalty for being too close to obstacles\n",
    "\n",
    "        reward -= self.smooth_action_penalty * np.linalg.norm(action - self.last_action)\n",
    "\n",
    "        # Small reward for each step taken to encourage exploration\n",
    "        reward+= self.reward_step \n",
    "\n",
    "        # Truncate if max steps reached and give max step reward\n",
    "        if self.steps >= self.max_steps:\n",
    "            reward += self.reward_max_step\n",
    "            truncated = True  \n",
    "\n",
    "        return reward,terminated,truncated,done_reason\n",
    "\n",
    "    def _get_hand_movement(self):\n",
    "\n",
    "        # if self.hand_move_mode == 'random':\n",
    "        #     move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        # elif self.hand_move_mode == 'towards_robot':\n",
    "        #     dir_vector = self.robot_position - self.hand_position\n",
    "        #     if np.linalg.norm(dir_vector) > 0:\n",
    "        #         dir_vector /= np.linalg.norm(dir_vector)\n",
    "        #     move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        if random.random() < self.hand_move_epsilon:\n",
    "            move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        else:\n",
    "            dir_vector = self.robot_position - self.hand_position\n",
    "            if np.linalg.norm(dir_vector) > 0:\n",
    "                dir_vector /= np.linalg.norm(dir_vector)\n",
    "            move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        \n",
    "        return move_hand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.random:\n",
    "            action+=np.random.normal(0,self.noise_action_sigma,size=self.action_space.shape)  # Add some noise to action to make it more realistic\n",
    "\n",
    "        move_hand = self._get_hand_movement()\n",
    "        self.hand_position += move_hand  # Update hand position\n",
    "        self.hand_position = np.clip(self.hand_position, self.margin, [self.grid_size*2,self.grid_size-self.margin])  # Ensure hand stays within grid bounds\n",
    "        # self.fixed_point+= np.array([np.,0])  # Randomize fixed point position\n",
    "\n",
    "        self.robot_position += action * self.stride_robot  # Scale the action to control speed\n",
    "        self.trajectory_points.append(self.robot_position.copy()) # New: Add current position to trajectory\n",
    "        self.steps += 1\n",
    "        \n",
    "\n",
    "        reward,terminated,truncated,done_reason = self._reward(action)\n",
    "        info = self._get_info()\n",
    "        info['done_reason'] = done_reason\n",
    "        info['distance_mean'] = np.mean(self.distance)\n",
    "        observation = self._get_obs()\n",
    "        if self.random:\n",
    "            observation += np.random.normal(0, self.noise_obs_sigma, size=self.observation_shape)  # Add some noise to observation to make it more realistic\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    " \n",
    "        pygame.display.init()\n",
    "        self.window = pygame.display.set_mode(\n",
    "                (int(self.grid_size * self.cell_size), int(self.grid_size * self.cell_size))\n",
    "            )\n",
    "        pygame.display.set_caption(\"CustomEnv\")\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                import sys\n",
    "                sys.exit() # Exit the program\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mouse_x, mouse_y = event.pos\n",
    "                self.hand_position = np.array([mouse_x/self.cell_size, mouse_y/self.cell_size])\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:  # 空格键切换暂停\n",
    "                    self.pause = not self.pause\n",
    "\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        canvas.fill(WHITE)\n",
    "        virus_image = pygame.image.load(\"hand.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "        robot_image = pygame.transform.scale(virus_image, (int(self.cell_size * 2), int(self.cell_size * 2)))  # Scale the image\n",
    "        # New: Draw the trajectory\n",
    "        if len(self.trajectory_points) > 1:\n",
    "            scaled_points = []\n",
    "            for point in self.trajectory_points:\n",
    "                scaled_points.append((int(point[0] * self.cell_size), int(point[1] * self.cell_size)))\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            pygame.draw.lines(canvas, BLUE, False, scaled_points, 2) # Blue line, not closed, 2 pixels wide\n",
    "            \n",
    "            # Optionally, draw small circles at each point to emphasize\n",
    "            for point_coord in scaled_points:\n",
    "                pygame.draw.circle(canvas, BLUE, point_coord, 3) # Small blue circles\n",
    "\n",
    "        # Draw robot\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            RED,\n",
    "            (int(self.robot_position[0] * self.cell_size), int(self.robot_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.2)\n",
    "        )\n",
    "        # Draw obstacles\n",
    "\n",
    "        canvas.blit(robot_image, (int((self.hand_position[0]-1) * self.cell_size), int((self.hand_position[1]-1) * self.cell_size+1)))\n",
    "        pygame.draw.circle(canvas,\n",
    "                            GREEN, \n",
    "                            (int((self.hand_position[0]) * self.cell_size), \n",
    "                            int((self.hand_position[1]) * self.cell_size+1)), \n",
    "        int(self.cell_size * 0.2)\n",
    "        )\n",
    "\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def load_args(self, args):\n",
    "        pass\n",
    "\n",
    "    def save_args(self,path):\n",
    "        env_args = {\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"distance_threshold_penalty\":self.distance_threshold_penalty,\n",
    "            \"distance_threshold_collision\":self.distance_threshold_collision,\n",
    "            \"penalty_factor\":self.penalty_factor,\n",
    "            \"distance_reward_factor\":self.distance_reward_factor,\n",
    "            \"smooth_action_penalty\":self.smooth_action_penalty,\n",
    "            \"max_steps\":self.max_steps,\n",
    "            \"margin\":self.margin,\n",
    "            \"reward_step\":self.reward_step,\n",
    "            \"reward_max_step\":self.reward_max_step,\n",
    "            \"reward_bound\":self.reward_bound,\n",
    "            \"reward_arm\":self.reward_arm,\n",
    "            \"reward_hand\":self.reward_hand,\n",
    "            \"stride_robot_range\":self.stride_robot_random,\n",
    "            \"stride_hand_range\":self.stride_hand_random,\n",
    "            \"move_hand_epsilon\":self.hand_move_epsilon,\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "        with open(os.path.join(path, \"env_args.json\"), \"w\") as f:\n",
    "            json.dump(env_args, f,indent=4)\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        pygame.display.quit()\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49588c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete,Tuple\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Define colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)  # Color for the trajectory\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self,render_mode=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.pause = False\n",
    "        self.domain_randomization = False\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define two separate thresholds for obstacle handling\n",
    "        self.distance_threshold_penalty = 5  # Penalty zone threshold (larger value)\n",
    "        self.distance_threshold_collision = 1.5  # Collision threshold (smaller value)\n",
    "        self.distance_threshold_arm = 3  # Arm threshold (smaller value)\n",
    "        self.penalty_factor = 5  # Penalty scaling factor\n",
    "        self.distance_reward_factor = 2\n",
    "        self.smooth_action_penalty = 2\n",
    "        self.steps = 0\n",
    "        self.margin = 0.3\n",
    "        self.reward_arm = -100\n",
    "        self.reward_hand = -100\n",
    "        self.reward_bound = -200\n",
    "        self.reward_max_step = 200\n",
    "        self.reward_step = 10\n",
    "        self.stride_robot = 2\n",
    "        self.stride_hand_random = [0.6,0.8]\n",
    "        self.hand_move_epsilon = 0.1\n",
    "\n",
    "\n",
    "        self.current_distance = 0  # Current distance to goal, used for reward shaping\n",
    "        self.max_steps = 50  # Set a maximum number of steps to prevent infinite loops\n",
    "        # Action space (dx, dy)\n",
    "        self.action_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Observation space (robot_x, robot_y, goal_x, goal_y)\n",
    "        self.observation_shape = 2+2+2+1+1+1+2+1  # Robot position, hand position, velocity_hand,radius_hand, and distance to hand\n",
    "        self.observation_space = Box(low=0, high=np.array([self.grid_size*2,self.grid_size, self.grid_size*2, self.grid_size,1,1 , (2**0.5)*self.grid_size,0.5*self.grid_size,0.5*self.grid_size,2*self.grid_size,self.grid_size]), \n",
    "                                     shape=(self.observation_shape,), dtype=np.float32)\n",
    "\n",
    "        self.random = True\n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 50 # Pixels per grid unit\n",
    "        self.trajectory_points = [] # New: List to store past robot positions\n",
    "        self.dist_arm = 0\n",
    "\n",
    "\n",
    "    def dist_point_to_segment_correct(self,P, A, B, eps=1e-12):\n",
    "        P = np.asarray(P, dtype=float)\n",
    "        A = np.asarray(A, dtype=float)\n",
    "        B = np.asarray(B, dtype=float)\n",
    "        v = B - A\n",
    "        w = P - A\n",
    "        vv = np.dot(v, v)\n",
    "        if vv <= eps:\n",
    "            # A and B coincide: treat as point A\n",
    "            C = A.copy()\n",
    "            d = np.linalg.norm(P - A)\n",
    "            t = 0.0\n",
    "            case = 'endpoint_A'\n",
    "        else:\n",
    "            t = np.dot(w, v) / vv\n",
    "            if t < 0.0:\n",
    "                C = A\n",
    "                d = np.linalg.norm(P - A)\n",
    "                case = 'before_A'\n",
    "            elif t > 1.0:\n",
    "                C = B\n",
    "                d = np.linalg.norm(P - B)\n",
    "                case = 'after_B'\n",
    "            else:\n",
    "                C = A + t * v\n",
    "                d = np.linalg.norm(P - C)\n",
    "                case = 'on_segment'\n",
    "        return float(d), C, float(t), case\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return np.concatenate(([self.robot_position]+ [self.hand_position]+[self.last_action]+\n",
    "                               [np.array([self.current_distance])]+\n",
    "                               [np.array([min(self.robot_position[0],\n",
    "                                              self.robot_position[1],\n",
    "                                              self.grid_size-self.robot_position[0],\n",
    "                                              self.grid_size-self.robot_position[1]) ])]+\n",
    "                                              [np.array([self.dist_arm])]+\n",
    "                                               [self.fixed_point]))\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance_to_hand\": self.current_distance,\n",
    "            \"robot_position\": self.robot_position,\n",
    "            \"hand_position\": self.hand_position,\n",
    "            'distance_arm':self.dist_arm,\n",
    "            \"fix_point\":self.fixed_point,\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        super().reset()\n",
    "        self.distance = []\n",
    "        self.stride_hand = np.random.uniform(*self.stride_hand_random)  # Randomize stride length\n",
    "        # self.stride_robot = 1  # Randomize stride length\n",
    "        self.distance_threshold_collision = np.random.uniform(2,3)  # Randomize collision threshold\n",
    "        self.distance_threshold_penalty = np.random.uniform(3, 4)  # Randomize penalty threshold\n",
    "        \n",
    "        \n",
    "        self.noise_obs_sigma = np.random.uniform(0, 0.1)  # Add some noise to observation to make it more realistic\n",
    "        self.noise_action_sigma = np.random.uniform(0,0.1)  # Add some noise to action to make it more realistic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.robot_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        self.hand_position = np.random.uniform(self.margin, [2*(self.grid_size-self.margin),self.grid_size-self.margin], size=2)\n",
    "        # self.hand_position = np.clip(self.hand_position, self.margin, self.grid_size-self.margin)  # Ensure hand stays within grid bounds\n",
    "        \n",
    "        # self.hand_move_mode = 'random' if np.random.rand() < 0.1 else 'towards_robot'  # Randomize hand movement mode\n",
    "        # self.hand_move_mode = 'towards_robot'\n",
    "        \n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.pre_distance = self.current_distance\n",
    "        self.last_action = np.zeros(2)\n",
    "        self.steps = 0\n",
    "        self.trajectory_points = [self.robot_position.copy()] # New: Reset trajectory and add initial position\n",
    "        \n",
    "        self.fixed_point = np.array([self.grid_size*random.uniform(0.2,1.8),self.grid_size])\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _reward(self,action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0  # Initialize reward\n",
    "        done_reason = None  # Initialize done reason\n",
    "\n",
    "        # action regulation penalty\n",
    "        # reward -= 0.5 * np.sum(np.square(action))  # Penalty for large actions\n",
    "\n",
    "        self.dist_arm = self.dist_point_to_segment_correct(self.robot_position,self.hand_position, self.fixed_point)[0]\n",
    "        if self.dist_arm < self.distance_threshold_arm:\n",
    "            reward += self.reward_arm \n",
    "            terminated = True  # Truncate if arm is too short\n",
    "\n",
    "        # boundary penalty\n",
    "        if np.any(self.robot_position <= self.margin) or (self.grid_size-self.robot_position[1] <=self.margin) or (2*self.grid_size-self.robot_position[0] <=self.margin):\n",
    "            reward += self.reward_bound\n",
    "            terminated = True  # Truncate if robot goes out of bounds\n",
    "            done_reason = \"out of bounds\"\n",
    "\n",
    "    \n",
    "        # Auxiliary Rewards -  distance to hand\n",
    "        self.current_distance = np.linalg.norm(self.robot_position - self.hand_position)\n",
    "        self.distance.append(self.current_distance)\n",
    "        reward += (self.current_distance-self.pre_distance)*self.distance_reward_factor  # Reward shaping based on distance change\n",
    "        self.pre_distance = self.current_distance\n",
    "\n",
    "        # Obstacle handling with two thresholds\n",
    "        if self.current_distance < self.distance_threshold_collision:\n",
    "            reward += self.reward_hand\n",
    "            terminated = True  # Terminate if too close to obstacles\n",
    "            done_reason = \"collision with obstacle\"\n",
    "        elif self.current_distance < self.distance_threshold_penalty:\n",
    "            reward -= self.penalty_factor * (self.distance_threshold_penalty - self.current_distance)  # Penalty for being too close to obstacles\n",
    "\n",
    "        reward -= self.smooth_action_penalty * np.linalg.norm(action - self.last_action)\n",
    "\n",
    "        # Small reward for each step taken to encourage exploration\n",
    "        reward+= self.reward_step \n",
    "\n",
    "        # Truncate if max steps reached and give max step reward\n",
    "        if self.steps >= self.max_steps:\n",
    "            reward += self.reward_max_step\n",
    "            truncated = True  \n",
    "\n",
    "        return reward,terminated,truncated,done_reason\n",
    "\n",
    "    def _get_hand_movement(self):\n",
    "\n",
    "        # if self.hand_move_mode == 'random':\n",
    "        #     move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        # elif self.hand_move_mode == 'towards_robot':\n",
    "        #     dir_vector = self.robot_position - self.hand_position\n",
    "        #     if np.linalg.norm(dir_vector) > 0:\n",
    "        #         dir_vector /= np.linalg.norm(dir_vector)\n",
    "        #     move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        if random.random() < self.hand_move_epsilon:\n",
    "            move_hand = np.random.uniform(-1, 1, size=2)  # Randomly move the hand position slightly\n",
    "        else:\n",
    "            dir_vector = self.robot_position - self.hand_position\n",
    "            if np.linalg.norm(dir_vector) > 0:\n",
    "                dir_vector /= np.linalg.norm(dir_vector)\n",
    "            move_hand = dir_vector * self.stride_hand  # Move hand towards robot position\n",
    "        \n",
    "        return move_hand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_obs(self, action):\n",
    "        pass\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.random:\n",
    "            action+=np.random.normal(0,self.noise_action_sigma,size=self.action_space.shape)  # Add some noise to action to make it more realistic\n",
    "\n",
    "        self.update_obs(action)\n",
    "\n",
    "\n",
    "        self.trajectory_points.append(self.robot_position.copy()) # New: Add current position to trajectory\n",
    "        self.steps += 1\n",
    "        \n",
    "\n",
    "        reward,terminated,truncated,done_reason = self._reward(action)\n",
    "        info = self._get_info()\n",
    "        info['done_reason'] = done_reason\n",
    "        info['distance_mean'] = np.mean(self.distance)\n",
    "        observation = self._get_obs()\n",
    "        if self.random:\n",
    "            observation += np.random.normal(0, self.noise_obs_sigma, size=self.observation_shape)  # Add some noise to observation to make it more realistic\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    " \n",
    "        pygame.display.init()\n",
    "        self.window = pygame.display.set_mode(\n",
    "                (int(self.grid_size * self.cell_size), int(self.grid_size * self.cell_size))\n",
    "            )\n",
    "        pygame.display.set_caption(\"CustomEnv\")\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                import sys\n",
    "                sys.exit() # Exit the program\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mouse_x, mouse_y = event.pos\n",
    "                self.hand_position = np.array([mouse_x/self.cell_size, mouse_y/self.cell_size])\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:  # 空格键切换暂停\n",
    "                    self.pause = not self.pause\n",
    "\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        canvas.fill(WHITE)\n",
    "        virus_image = pygame.image.load(\"hand.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "        robot_image = pygame.transform.scale(virus_image, (int(self.cell_size * 2), int(self.cell_size * 2)))  # Scale the image\n",
    "        # New: Draw the trajectory\n",
    "        if len(self.trajectory_points) > 1:\n",
    "            scaled_points = []\n",
    "            for point in self.trajectory_points:\n",
    "                scaled_points.append((int(point[0] * self.cell_size), int(point[1] * self.cell_size)))\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            pygame.draw.lines(canvas, BLUE, False, scaled_points, 2) # Blue line, not closed, 2 pixels wide\n",
    "            \n",
    "            # Optionally, draw small circles at each point to emphasize\n",
    "            for point_coord in scaled_points:\n",
    "                pygame.draw.circle(canvas, BLUE, point_coord, 3) # Small blue circles\n",
    "\n",
    "        # Draw robot\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            RED,\n",
    "            (int(self.robot_position[0] * self.cell_size), int(self.robot_position[1] * self.cell_size)),\n",
    "            int(self.cell_size * 0.2)\n",
    "        )\n",
    "        # Draw obstacles\n",
    "\n",
    "        canvas.blit(robot_image, (int((self.hand_position[0]-1) * self.cell_size), int((self.hand_position[1]-1) * self.cell_size+1)))\n",
    "        pygame.draw.circle(canvas,\n",
    "                            GREEN, \n",
    "                            (int((self.hand_position[0]) * self.cell_size), \n",
    "                            int((self.hand_position[1]) * self.cell_size+1)), \n",
    "        int(self.cell_size * 0.2)\n",
    "        )\n",
    "\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def load_args(self, args):\n",
    "        pass\n",
    "\n",
    "    def save_args(self,path):\n",
    "        env_args = {\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"distance_threshold_penalty\":self.distance_threshold_penalty,\n",
    "            \"distance_threshold_collision\":self.distance_threshold_collision,\n",
    "            \"penalty_factor\":self.penalty_factor,\n",
    "            \"distance_reward_factor\":self.distance_reward_factor,\n",
    "            \"smooth_action_penalty\":self.smooth_action_penalty,\n",
    "            \"max_steps\":self.max_steps,\n",
    "            \"margin\":self.margin,\n",
    "            \"reward_step\":self.reward_step,\n",
    "            \"reward_max_step\":self.reward_max_step,\n",
    "            \"reward_bound\":self.reward_bound,\n",
    "            \"reward_arm\":self.reward_arm,\n",
    "            \"reward_hand\":self.reward_hand,\n",
    "            \"stride_robot\":self.stride_robot,\n",
    "            \"stride_hand_range\":self.stride_hand_random,\n",
    "            \"move_hand_epsilon\":self.hand_move_epsilon,\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "        with open(os.path.join(path, \"env_args.json\"), \"w\") as f:\n",
    "            json.dump(env_args, f,indent=4)\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        pygame.display.quit()\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa40b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_environment(robot_position, hand_position, fix_point,trajectory_points, grid_size=10, cell_size=50):\n",
    "    WHITE = (255, 255, 255)\n",
    "    RED = (255, 0, 0)\n",
    "    GREEN = (0, 255, 0)\n",
    "    BLUE = (0, 0, 255)\n",
    "    pygame.init()\n",
    "    window = pygame.display.set_mode((grid_size * cell_size*2, grid_size * cell_size))\n",
    "    canvas = pygame.Surface((grid_size * cell_size*2, grid_size * cell_size))\n",
    "    canvas.fill(WHITE)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            import sys\n",
    "            sys.exit() # Exit the program\n",
    "\n",
    "        elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            mouse_x, mouse_y = event.pos\n",
    "            hand_position = np.array([mouse_x/cell_size, mouse_y/cell_size])\n",
    "\n",
    "    if len(trajectory_points) > 1:\n",
    "        scaled_points = [(int(point[0] * cell_size), int(point[1] * cell_size)) for point in trajectory_points]\n",
    "        pygame.draw.lines(canvas, BLUE, False, scaled_points, 2)\n",
    "        for point_coord in scaled_points:\n",
    "            pygame.draw.circle(canvas, BLUE, point_coord, 3)\n",
    "\n",
    "    pygame.draw.lines(canvas, (255, 224, 189), False,[hand_position*cell_size, [fix_point[0]*cell_size,fix_point[1]*cell_size]],width=25)\n",
    "\n",
    "    virus_image = pygame.image.load(\"../hand.png\").convert_alpha()  # Load an image if needed, but not used here\n",
    "    robot_image = pygame.transform.scale(virus_image, (int(cell_size * 2), int(cell_size * 2)))  # Scale the \n",
    "    pygame.draw.circle(canvas, RED, (int(robot_position[0] * cell_size), int(robot_position[1] * cell_size)), int(cell_size * 0.2))\n",
    "    pygame.draw.circle(canvas, GREEN, (int(hand_position[0] * cell_size), int(hand_position[1] * cell_size)), int(cell_size * 0.2))\n",
    "    canvas.blit(robot_image, (int((hand_position[0]-1) * cell_size), int((hand_position[1]-1) * cell_size)))\n",
    "    font = pygame.font.Font(None, 24)\n",
    "    text = font.render(f\"{hand_position[0]},{hand_position[1]}\", True, BLUE)\n",
    "    text_rect = text.get_rect()\n",
    "    text_rect.center = (int(hand_position[0] * cell_size), int(hand_position[1] * cell_size))\n",
    "    window.blit(canvas, canvas.get_rect())\n",
    "    # window.blit(text, text_rect)\n",
    "    pygame.display.flip()\n",
    "\n",
    "    return hand_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8670f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=10000, n_episodes=1, log_freq=10000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        # 用deque统计最近N个done的终止原因，避免内存爆炸\n",
    "        self.termination_reasons = deque(maxlen=1000)  # 统计最近1000次终止\n",
    "        self.env_to_render = env\n",
    "        self.render_freq = render_freq\n",
    "        self.n_episodes = n_episodes\n",
    "        self.distance_mean = deque(maxlen=1000) \n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 先从env info里读取终止原因\n",
    "        # print((self.locals.keys()))\n",
    "        infos = self.locals.get('infos', None)\n",
    "\n",
    "        dones = self.locals.get('dones', None)\n",
    "        if infos is not None and dones is not None:\n",
    "            for done, info in zip(dones, infos):\n",
    "                if done and info is not None and 'done_reason' in info:\n",
    "                    self.termination_reasons.append(info['done_reason'])\n",
    "                    self.distance_mean.append(info['distance_mean'])\n",
    "\n",
    "        # 每log_freq步打印信息\n",
    "        # if self.num_timesteps % self.render_freq == 0 and self.verbose:\n",
    "        #     for ep in range(self.n_episodes):\n",
    "        #         obs = self.env_to_render.reset()\n",
    "        #         done = False\n",
    "        #         while not done:\n",
    "        #             action, _states = self.model.predict(obs, deterministic=True)\n",
    "        #             obs, rewards, done, info = self.env_to_render.step(action)\n",
    "        #             self.env_to_render.render()\n",
    "        #             time.sleep(0.6)\n",
    "\n",
    "        #             if done:\n",
    "\n",
    "        #                 self.env_to_render.close()\n",
    "        #                 break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.num_timesteps % self.log_freq == 0 and self.verbose:\n",
    "            log = self.model.logger.name_to_value\n",
    "            ep_rew = log.get('rollout/ep_rew_mean', None)\n",
    "            ep_len = log.get('rollout/ep_len_mean', None)\n",
    "            loss = log.get('train/loss', None)\n",
    "            v_loss = log.get('train/value_loss', None)\n",
    "            p_loss = log.get('train/policy_gradient_loss', None)\n",
    "            ent_loss = log.get('train/entropy_loss', None)\n",
    "            kl = log.get('train/approx_kl', None)\n",
    "\n",
    "            # 统计终止原因比例\n",
    "            total = len(self.termination_reasons)\n",
    "            if total > 0:\n",
    "                count_hand = sum(1 for r in self.termination_reasons if r == 'out of bounds')\n",
    "                ratio_hand = count_hand / total\n",
    "            else:\n",
    "                ratio_hand = 0.0\n",
    "            distance_mean = sum(self.distance_mean) / len(self.distance_mean) if len(self.distance_mean) > 0 else 0.0\n",
    "\n",
    "            # print(f\"[{self.num_timesteps:7d}] ep_rew_mean={ep_rew}, ep_len_mean={ep_len}, loss={loss:.3f}, \"\n",
    "            #       f\"v_loss={v_loss:.3f}, p_loss={p_loss:.3f}, ent_loss={ent_loss:.3f}, kl={kl:.4f}, \"\n",
    "            #       f\"termination_reason_hand_ratio={ratio_hand:.3f}\")\n",
    "            self.logger.record(\"custom/termination_reason_ratio\", ratio_hand)\n",
    "\n",
    "            self.logger.record(\"custom/distance_mean\", distance_mean)\n",
    "            self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def metric(trajectory):\n",
    "    \"\"\"\n",
    "    trajectory: list of tuples, each tuple contains (observation, action, hand_movement, reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(trajectory, list):\n",
    "        raise TypeError(\"trajectory should be a list of tuples\")\n",
    "\n",
    "    # distance = [x[6] for x in trajectory]\n",
    "    # return distance\n",
    "    \n",
    "    # plt.hist(distance, bins=10, density=True,edgecolor='black', alpha=0.7,color='skyblue')\n",
    "    # plt.xlabel('distance')\n",
    "    # plt.ylabel('density')\n",
    "    # plt.title('distance distribution')\n",
    "    # plt.pause(0.1)\n",
    "\n",
    "    # distance_aproximity\n",
    "    for item in trajectory:\n",
    "        obs = item[0]\n",
    "        distance = obs[0:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # direction_alignment\n",
    "    for item in trajectory:\n",
    "        obs = item[0]\n",
    "        direction = \n",
    "        hand_movement = item[2]\n",
    "\n",
    "        position_robot,position_hand = obs[0:2],obs[2:4]\n",
    "        direction = position_robot - position_hand\n",
    "        consine_angle = np.dot(direction, hand_movement) / (np.linalg.norm(direction) * np.linalg.norm(hand_movement))\n",
    "        angle = np.arccos(consine_angle)\n",
    "\n",
    "    # reaction_time\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./sac_custom_env_tensorboard/best_model_sac80\\run_2\n",
      "Eval num_timesteps=10, episode_reward=-135.33 +/- 68.85\n",
      "Episode length: 6.40 +/- 4.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.987   |\n",
      "|    critic_loss     | 3.66e+04 |\n",
      "|    ent_coef        | 1        |\n",
      "|    ent_coef_loss   | 0        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1        |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20, episode_reward=-138.10 +/- 54.21\n",
      "Episode length: 3.80 +/- 4.62\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 3.8       |\n",
      "|    mean_reward     | -138      |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 20        |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.926    |\n",
      "|    critic_loss     | 1.16e+04  |\n",
      "|    ent_coef        | 1         |\n",
      "|    ent_coef_loss   | -0.000979 |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2         |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30, episode_reward=-103.93 +/- 71.40\n",
      "Episode length: 4.50 +/- 5.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.998   |\n",
      "|    critic_loss     | 8.15e+03 |\n",
      "|    ent_coef        | 0.999    |\n",
      "|    ent_coef_loss   | -0.00205 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3        |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | -0.877   |\n",
      "|    critic_loss   | 4.69e+03 |\n",
      "|    ent_coef      | 0.999    |\n",
      "|    ent_coef_loss | -0.00302 |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 4        |\n",
      "-------------------------------\n",
      "Eval num_timesteps=40, episode_reward=-101.77 +/- 50.31\n",
      "Episode length: 4.60 +/- 4.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | -0.832   |\n",
      "|    critic_loss   | 6.67e+03 |\n",
      "|    ent_coef      | 0.999    |\n",
      "|    ent_coef_loss | -0.00405 |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 5        |\n",
      "-------------------------------\n",
      "Eval num_timesteps=50, episode_reward=-139.75 +/- 69.38\n",
      "Episode length: 4.80 +/- 4.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60, episode_reward=-102.48 +/- 75.98\n",
      "Episode length: 5.60 +/- 4.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.734   |\n",
      "|    critic_loss     | 6.39e+03 |\n",
      "|    ent_coef        | 0.999    |\n",
      "|    ent_coef_loss   | -0.00508 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6        |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70, episode_reward=-128.80 +/- 51.57\n",
      "Episode length: 3.70 +/- 3.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.527   |\n",
      "|    critic_loss     | 7.8e+03  |\n",
      "|    ent_coef        | 0.998    |\n",
      "|    ent_coef_loss   | -0.00592 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7        |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80, episode_reward=-71.74 +/- 77.10\n",
      "Episode length: 7.10 +/- 6.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -71.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.564   |\n",
      "|    critic_loss     | 5.07e+03 |\n",
      "|    ent_coef        | 0.998    |\n",
      "|    ent_coef_loss   | -0.00716 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8        |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | -0.343   |\n",
      "|    critic_loss   | 4.94e+03 |\n",
      "|    ent_coef      | 0.998    |\n",
      "|    ent_coef_loss | -0.00797 |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 9        |\n",
      "-------------------------------\n",
      "Eval num_timesteps=90, episode_reward=-99.36 +/- 72.37\n",
      "Episode length: 7.40 +/- 5.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | -99.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100, episode_reward=-80.73 +/- 72.98\n",
      "Episode length: 8.20 +/- 7.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | -80.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.22    |\n",
      "|    critic_loss     | 4.76e+03 |\n",
      "|    ent_coef        | 0.997    |\n",
      "|    ent_coef_loss   | -0.00893 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | -0.2     |\n",
      "|    critic_loss   | 4.94e+03 |\n",
      "|    ent_coef      | 0.997    |\n",
      "|    ent_coef_loss | -0.0101  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 11       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=110, episode_reward=-101.90 +/- 69.35\n",
      "Episode length: 7.10 +/- 5.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120, episode_reward=-110.77 +/- 46.82\n",
      "Episode length: 4.80 +/- 3.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0814  |\n",
      "|    critic_loss     | 4.09e+03 |\n",
      "|    ent_coef        | 0.997    |\n",
      "|    ent_coef_loss   | -0.0111  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 0.0501   |\n",
      "|    critic_loss   | 8.2e+03  |\n",
      "|    ent_coef      | 0.996    |\n",
      "|    ent_coef_loss | -0.0122  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 13       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=130, episode_reward=-65.87 +/- 108.53\n",
      "Episode length: 10.40 +/- 8.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.4     |\n",
      "|    mean_reward     | -65.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 0.226    |\n",
      "|    critic_loss   | 4.92e+03 |\n",
      "|    ent_coef      | 0.996    |\n",
      "|    ent_coef_loss | -0.013   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 14       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=140, episode_reward=-106.89 +/- 64.76\n",
      "Episode length: 6.40 +/- 6.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150, episode_reward=-91.10 +/- 66.21\n",
      "Episode length: 7.30 +/- 4.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | -91.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.347    |\n",
      "|    critic_loss     | 8.54e+03 |\n",
      "|    ent_coef        | 0.996    |\n",
      "|    ent_coef_loss   | -0.0142  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160, episode_reward=-101.90 +/- 71.35\n",
      "Episode length: 7.20 +/- 6.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.589    |\n",
      "|    critic_loss     | 6.57e+03 |\n",
      "|    ent_coef        | 0.996    |\n",
      "|    ent_coef_loss   | -0.0151  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 0.747    |\n",
      "|    critic_loss   | 3.33e+03 |\n",
      "|    ent_coef      | 0.995    |\n",
      "|    ent_coef_loss | -0.0157  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 17       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=170, episode_reward=-103.02 +/- 72.60\n",
      "Episode length: 7.20 +/- 6.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180, episode_reward=-65.93 +/- 68.41\n",
      "Episode length: 9.00 +/- 6.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -65.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.764    |\n",
      "|    critic_loss     | 3.59e+03 |\n",
      "|    ent_coef        | 0.995    |\n",
      "|    ent_coef_loss   | -0.0174  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 1.13     |\n",
      "|    critic_loss   | 6.26e+03 |\n",
      "|    ent_coef      | 0.995    |\n",
      "|    ent_coef_loss | -0.0182  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 19       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=190, episode_reward=-96.86 +/- 47.59\n",
      "Episode length: 7.90 +/- 3.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200, episode_reward=-121.96 +/- 110.06\n",
      "Episode length: 7.00 +/- 7.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.46     |\n",
      "|    critic_loss     | 2.46e+03 |\n",
      "|    ent_coef        | 0.994    |\n",
      "|    ent_coef_loss   | -0.0191  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 1.65     |\n",
      "|    critic_loss   | 2.09e+03 |\n",
      "|    ent_coef      | 0.994    |\n",
      "|    ent_coef_loss | -0.0203  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 21       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=210, episode_reward=-127.30 +/- 100.05\n",
      "Episode length: 5.40 +/- 3.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 1.96     |\n",
      "|    critic_loss   | 3.58e+03 |\n",
      "|    ent_coef      | 0.994    |\n",
      "|    ent_coef_loss | -0.0211  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 22       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=220, episode_reward=-141.13 +/- 41.51\n",
      "Episode length: 5.90 +/- 4.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 2.15     |\n",
      "|    critic_loss   | 3.13e+03 |\n",
      "|    ent_coef      | 0.993    |\n",
      "|    ent_coef_loss | -0.0224  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 23       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=230, episode_reward=-117.05 +/- 63.68\n",
      "Episode length: 4.30 +/- 4.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240, episode_reward=-97.58 +/- 14.67\n",
      "Episode length: 2.80 +/- 3.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | -97.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.35     |\n",
      "|    critic_loss     | 3.65e+03 |\n",
      "|    ent_coef        | 0.993    |\n",
      "|    ent_coef_loss   | -0.0231  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 2.58     |\n",
      "|    critic_loss   | 3.32e+03 |\n",
      "|    ent_coef      | 0.993    |\n",
      "|    ent_coef_loss | -0.0242  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 25       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=250, episode_reward=-99.73 +/- 76.65\n",
      "Episode length: 8.50 +/- 7.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.5      |\n",
      "|    mean_reward     | -99.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260, episode_reward=-90.49 +/- 86.70\n",
      "Episode length: 6.20 +/- 5.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -90.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.11     |\n",
      "|    critic_loss     | 6.97e+03 |\n",
      "|    ent_coef        | 0.993    |\n",
      "|    ent_coef_loss   | -0.0252  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270, episode_reward=-99.04 +/- 54.49\n",
      "Episode length: 6.30 +/- 4.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -99      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.3      |\n",
      "|    critic_loss     | 3.01e+03 |\n",
      "|    ent_coef        | 0.992    |\n",
      "|    ent_coef_loss   | -0.026   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 3.74     |\n",
      "|    critic_loss   | 3.7e+03  |\n",
      "|    ent_coef      | 0.992    |\n",
      "|    ent_coef_loss | -0.027   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 28       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=280, episode_reward=-78.78 +/- 41.95\n",
      "Episode length: 6.60 +/- 5.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -78.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290, episode_reward=-101.30 +/- 62.01\n",
      "Episode length: 6.00 +/- 7.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.27     |\n",
      "|    critic_loss     | 4.94e+03 |\n",
      "|    ent_coef        | 0.992    |\n",
      "|    ent_coef_loss   | -0.0285  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 4.66     |\n",
      "|    critic_loss   | 4.93e+03 |\n",
      "|    ent_coef      | 0.991    |\n",
      "|    ent_coef_loss | -0.0295  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 30       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=300, episode_reward=-103.97 +/- 70.18\n",
      "Episode length: 7.00 +/- 5.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 5.09     |\n",
      "|    critic_loss   | 2.24e+03 |\n",
      "|    ent_coef      | 0.991    |\n",
      "|    ent_coef_loss | -0.0302  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 31       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=310, episode_reward=-106.26 +/- 28.77\n",
      "Episode length: 4.50 +/- 4.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320, episode_reward=-102.42 +/- 55.79\n",
      "Episode length: 6.00 +/- 4.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.65     |\n",
      "|    critic_loss     | 5.64e+03 |\n",
      "|    ent_coef        | 0.991    |\n",
      "|    ent_coef_loss   | -0.0311  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330, episode_reward=-69.94 +/- 56.01\n",
      "Episode length: 6.50 +/- 4.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -69.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.47     |\n",
      "|    critic_loss     | 4.21e+03 |\n",
      "|    ent_coef        | 0.99     |\n",
      "|    ent_coef_loss   | -0.0319  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340, episode_reward=-105.10 +/- 75.80\n",
      "Episode length: 5.70 +/- 5.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.75     |\n",
      "|    critic_loss     | 3.34e+03 |\n",
      "|    ent_coef        | 0.99     |\n",
      "|    ent_coef_loss   | -0.0329  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 7.84     |\n",
      "|    critic_loss   | 4.29e+03 |\n",
      "|    ent_coef      | 0.99     |\n",
      "|    ent_coef_loss | -0.0342  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 35       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=350, episode_reward=-104.00 +/- 56.28\n",
      "Episode length: 3.50 +/- 2.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 8.33     |\n",
      "|    critic_loss   | 2.71e+03 |\n",
      "|    ent_coef      | 0.99     |\n",
      "|    ent_coef_loss | -0.0355  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 36       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=360, episode_reward=-99.47 +/- 65.49\n",
      "Episode length: 5.10 +/- 5.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -99.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370, episode_reward=-99.44 +/- 85.32\n",
      "Episode length: 6.20 +/- 5.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -99.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.96     |\n",
      "|    critic_loss     | 4.86e+03 |\n",
      "|    ent_coef        | 0.989    |\n",
      "|    ent_coef_loss   | -0.0365  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380, episode_reward=-88.04 +/- 48.90\n",
      "Episode length: 5.30 +/- 4.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -88      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.7     |\n",
      "|    critic_loss     | 5.66e+03 |\n",
      "|    ent_coef        | 0.989    |\n",
      "|    ent_coef_loss   | -0.0378  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 11.4     |\n",
      "|    critic_loss   | 6.59e+03 |\n",
      "|    ent_coef      | 0.989    |\n",
      "|    ent_coef_loss | -0.0381  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 39       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=390, episode_reward=-64.90 +/- 76.31\n",
      "Episode length: 8.50 +/- 5.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.5      |\n",
      "|    mean_reward     | -64.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400, episode_reward=-164.37 +/- 65.89\n",
      "Episode length: 2.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -164     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 6.93e+03 |\n",
      "|    ent_coef        | 0.988    |\n",
      "|    ent_coef_loss   | -0.0394  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 14.6     |\n",
      "|    critic_loss   | 3.52e+03 |\n",
      "|    ent_coef      | 0.988    |\n",
      "|    ent_coef_loss | -0.04    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 41       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=410, episode_reward=-104.77 +/- 81.91\n",
      "Episode length: 7.00 +/- 6.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420, episode_reward=-106.86 +/- 67.22\n",
      "Episode length: 6.30 +/- 6.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 4.2e+03  |\n",
      "|    ent_coef        | 0.988    |\n",
      "|    ent_coef_loss   | -0.041   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.6     |\n",
      "|    critic_loss   | 5.01e+03 |\n",
      "|    ent_coef      | 0.987    |\n",
      "|    ent_coef_loss | -0.0419  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 43       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=430, episode_reward=-88.64 +/- 112.80\n",
      "Episode length: 10.00 +/- 6.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -88.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440, episode_reward=-105.89 +/- 69.82\n",
      "Episode length: 5.40 +/- 4.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 4.91e+03 |\n",
      "|    ent_coef        | 0.987    |\n",
      "|    ent_coef_loss   | -0.0433  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 20.2     |\n",
      "|    critic_loss   | 3.75e+03 |\n",
      "|    ent_coef      | 0.987    |\n",
      "|    ent_coef_loss | -0.044   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 45       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=450, episode_reward=-77.34 +/- 65.68\n",
      "Episode length: 5.20 +/- 5.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -77.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460, episode_reward=-120.97 +/- 60.92\n",
      "Episode length: 5.00 +/- 4.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 4.49e+03 |\n",
      "|    ent_coef        | 0.987    |\n",
      "|    ent_coef_loss   | -0.0461  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 22.7     |\n",
      "|    critic_loss   | 2.84e+03 |\n",
      "|    ent_coef      | 0.986    |\n",
      "|    ent_coef_loss | -0.0466  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 47       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=470, episode_reward=-97.32 +/- 40.72\n",
      "Episode length: 5.40 +/- 4.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -97.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480, episode_reward=-85.77 +/- 74.12\n",
      "Episode length: 5.70 +/- 5.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -85.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.1     |\n",
      "|    critic_loss     | 3.73e+03 |\n",
      "|    ent_coef        | 0.986    |\n",
      "|    ent_coef_loss   | -0.0471  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490, episode_reward=-102.09 +/- 63.07\n",
      "Episode length: 7.10 +/- 5.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.8     |\n",
      "|    critic_loss     | 3.62e+03 |\n",
      "|    ent_coef        | 0.986    |\n",
      "|    ent_coef_loss   | -0.0485  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.1     |\n",
      "|    critic_loss   | 2.94e+03 |\n",
      "|    ent_coef      | 0.985    |\n",
      "|    ent_coef_loss | -0.0489  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 50       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=500, episode_reward=-144.42 +/- 66.71\n",
      "Episode length: 4.30 +/- 4.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510, episode_reward=-99.72 +/- 79.25\n",
      "Episode length: 7.40 +/- 6.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | -99.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23       |\n",
      "|    critic_loss     | 4.45e+03 |\n",
      "|    ent_coef        | 0.985    |\n",
      "|    ent_coef_loss   | -0.0508  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 51       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520, episode_reward=-97.19 +/- 53.73\n",
      "Episode length: 4.30 +/- 3.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.5     |\n",
      "|    critic_loss     | 4.63e+03 |\n",
      "|    ent_coef        | 0.985    |\n",
      "|    ent_coef_loss   | -0.0512  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.2     |\n",
      "|    critic_loss   | 4.3e+03  |\n",
      "|    ent_coef      | 0.985    |\n",
      "|    ent_coef_loss | -0.0524  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 53       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=530, episode_reward=-131.14 +/- 105.24\n",
      "Episode length: 4.90 +/- 4.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540, episode_reward=-94.44 +/- 58.34\n",
      "Episode length: 5.70 +/- 6.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.9     |\n",
      "|    critic_loss     | 3.94e+03 |\n",
      "|    ent_coef        | 0.984    |\n",
      "|    ent_coef_loss   | -0.0533  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 54       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 20.8     |\n",
      "|    critic_loss   | 2.26e+03 |\n",
      "|    ent_coef      | 0.984    |\n",
      "|    ent_coef_loss | -0.0547  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 55       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=550, episode_reward=-110.71 +/- 46.20\n",
      "Episode length: 4.90 +/- 4.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.8     |\n",
      "|    critic_loss   | 3.5e+03  |\n",
      "|    ent_coef      | 0.984    |\n",
      "|    ent_coef_loss | -0.0554  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 56       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=560, episode_reward=-102.59 +/- 58.81\n",
      "Episode length: 3.50 +/- 2.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570, episode_reward=-93.70 +/- 48.28\n",
      "Episode length: 6.80 +/- 4.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.7     |\n",
      "|    critic_loss     | 4.4e+03  |\n",
      "|    ent_coef        | 0.983    |\n",
      "|    ent_coef_loss   | -0.0564  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 57       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.5     |\n",
      "|    critic_loss   | 4.22e+03 |\n",
      "|    ent_coef      | 0.983    |\n",
      "|    ent_coef_loss | -0.0573  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 58       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=580, episode_reward=-97.38 +/- 62.50\n",
      "Episode length: 4.20 +/- 4.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.2     |\n",
      "|    critic_loss   | 6.62e+03 |\n",
      "|    ent_coef      | 0.983    |\n",
      "|    ent_coef_loss | -0.0585  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 59       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=590, episode_reward=-101.68 +/- 96.57\n",
      "Episode length: 5.80 +/- 7.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600, episode_reward=-100.31 +/- 42.34\n",
      "Episode length: 6.00 +/- 4.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 2.91e+03 |\n",
      "|    ent_coef        | 0.982    |\n",
      "|    ent_coef_loss   | -0.0595  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610, episode_reward=-127.60 +/- 70.71\n",
      "Episode length: 4.00 +/- 3.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 3.85e+03 |\n",
      "|    ent_coef        | 0.982    |\n",
      "|    ent_coef_loss   | -0.0607  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 61       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 18.4     |\n",
      "|    critic_loss   | 3.85e+03 |\n",
      "|    ent_coef      | 0.982    |\n",
      "|    ent_coef_loss | -0.0603  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 62       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=620, episode_reward=-123.52 +/- 47.17\n",
      "Episode length: 4.60 +/- 4.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 18.8     |\n",
      "|    critic_loss   | 3.33e+03 |\n",
      "|    ent_coef      | 0.982    |\n",
      "|    ent_coef_loss | -0.0624  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 63       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=630, episode_reward=-91.02 +/- 50.72\n",
      "Episode length: 7.10 +/- 6.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -91      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640, episode_reward=-75.87 +/- 49.07\n",
      "Episode length: 5.50 +/- 4.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -75.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 4.94e+03 |\n",
      "|    ent_coef        | 0.981    |\n",
      "|    ent_coef_loss   | -0.0628  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 64       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650, episode_reward=-88.52 +/- 47.41\n",
      "Episode length: 5.00 +/- 5.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -88.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 2.61e+03 |\n",
      "|    ent_coef        | 0.981    |\n",
      "|    ent_coef_loss   | -0.0645  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 65       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.1     |\n",
      "|    critic_loss   | 4.27e+03 |\n",
      "|    ent_coef      | 0.981    |\n",
      "|    ent_coef_loss | -0.0652  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 66       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=660, episode_reward=-73.36 +/- 83.28\n",
      "Episode length: 9.00 +/- 8.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -73.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.5     |\n",
      "|    critic_loss   | 4.3e+03  |\n",
      "|    ent_coef      | 0.98     |\n",
      "|    ent_coef_loss | -0.0671  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 67       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=670, episode_reward=-93.43 +/- 56.88\n",
      "Episode length: 5.70 +/- 6.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -93.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680, episode_reward=-95.01 +/- 73.31\n",
      "Episode length: 6.80 +/- 6.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 2.4e+03  |\n",
      "|    ent_coef        | 0.98     |\n",
      "|    ent_coef_loss   | -0.0669  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.5     |\n",
      "|    critic_loss   | 5.47e+03 |\n",
      "|    ent_coef      | 0.98     |\n",
      "|    ent_coef_loss | -0.0681  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 69       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=690, episode_reward=-100.73 +/- 76.56\n",
      "Episode length: 6.10 +/- 3.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700, episode_reward=-108.14 +/- 83.55\n",
      "Episode length: 6.20 +/- 5.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 3.85e+03 |\n",
      "|    ent_coef        | 0.98     |\n",
      "|    ent_coef_loss   | -0.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 70       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710, episode_reward=-104.11 +/- 43.71\n",
      "Episode length: 4.40 +/- 3.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 2.81e+03 |\n",
      "|    ent_coef        | 0.979    |\n",
      "|    ent_coef_loss   | -0.0708  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 71       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 17.1     |\n",
      "|    critic_loss   | 4.38e+03 |\n",
      "|    ent_coef      | 0.979    |\n",
      "|    ent_coef_loss | -0.0716  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 72       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=720, episode_reward=-79.78 +/- 60.77\n",
      "Episode length: 5.00 +/- 4.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -79.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730, episode_reward=-111.67 +/- 55.41\n",
      "Episode length: 3.60 +/- 2.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 4.56e+03 |\n",
      "|    ent_coef        | 0.979    |\n",
      "|    ent_coef_loss   | -0.0724  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 73       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740, episode_reward=-125.29 +/- 103.70\n",
      "Episode length: 6.70 +/- 5.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.9     |\n",
      "|    critic_loss     | 4.2e+03  |\n",
      "|    ent_coef        | 0.978    |\n",
      "|    ent_coef_loss   | -0.0738  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 74       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 18.2     |\n",
      "|    critic_loss   | 2.73e+03 |\n",
      "|    ent_coef      | 0.978    |\n",
      "|    ent_coef_loss | -0.0735  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 75       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=750, episode_reward=-137.47 +/- 47.91\n",
      "Episode length: 2.90 +/- 2.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 18.3     |\n",
      "|    critic_loss   | 4e+03    |\n",
      "|    ent_coef      | 0.978    |\n",
      "|    ent_coef_loss | -0.0758  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 76       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=760, episode_reward=-84.56 +/- 63.88\n",
      "Episode length: 7.90 +/- 5.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | -84.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770, episode_reward=-87.80 +/- 42.18\n",
      "Episode length: 5.00 +/- 3.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -87.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 5.16e+03 |\n",
      "|    ent_coef        | 0.977    |\n",
      "|    ent_coef_loss   | -0.0759  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 77       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780, episode_reward=-88.15 +/- 51.82\n",
      "Episode length: 5.40 +/- 5.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -88.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.2     |\n",
      "|    critic_loss     | 3.96e+03 |\n",
      "|    ent_coef        | 0.977    |\n",
      "|    ent_coef_loss   | -0.0756  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 78       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 20.3     |\n",
      "|    critic_loss   | 4.52e+03 |\n",
      "|    ent_coef      | 0.977    |\n",
      "|    ent_coef_loss | -0.0785  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 79       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=790, episode_reward=-99.06 +/- 41.29\n",
      "Episode length: 2.70 +/- 3.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | -99.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800, episode_reward=-54.43 +/- 71.30\n",
      "Episode length: 10.10 +/- 7.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | -54.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.2     |\n",
      "|    critic_loss     | 3.85e+03 |\n",
      "|    ent_coef        | 0.977    |\n",
      "|    ent_coef_loss   | -0.0801  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 80       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=810, episode_reward=-83.04 +/- 79.93\n",
      "Episode length: 5.80 +/- 5.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -83      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.7     |\n",
      "|    critic_loss     | 3.39e+03 |\n",
      "|    ent_coef        | 0.976    |\n",
      "|    ent_coef_loss   | -0.0804  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 81       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.7     |\n",
      "|    critic_loss   | 4.28e+03 |\n",
      "|    ent_coef      | 0.976    |\n",
      "|    ent_coef_loss | -0.0823  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 82       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=820, episode_reward=-80.02 +/- 45.72\n",
      "Episode length: 3.70 +/- 3.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -80      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830, episode_reward=-139.40 +/- 77.62\n",
      "Episode length: 3.80 +/- 3.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.1     |\n",
      "|    critic_loss     | 4.17e+03 |\n",
      "|    ent_coef        | 0.976    |\n",
      "|    ent_coef_loss   | -0.0836  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 83       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840, episode_reward=-86.51 +/- 70.84\n",
      "Episode length: 7.90 +/- 6.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | -86.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.2     |\n",
      "|    critic_loss     | 1.27e+03 |\n",
      "|    ent_coef        | 0.975    |\n",
      "|    ent_coef_loss   | -0.0839  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 84       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.5     |\n",
      "|    critic_loss   | 5.08e+03 |\n",
      "|    ent_coef      | 0.975    |\n",
      "|    ent_coef_loss | -0.0848  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 85       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=850, episode_reward=-133.80 +/- 57.69\n",
      "Episode length: 4.60 +/- 4.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860, episode_reward=-79.37 +/- 57.66\n",
      "Episode length: 6.40 +/- 5.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -79.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.9     |\n",
      "|    critic_loss     | 5.95e+03 |\n",
      "|    ent_coef        | 0.975    |\n",
      "|    ent_coef_loss   | -0.0858  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 86       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870, episode_reward=-58.14 +/- 62.07\n",
      "Episode length: 6.20 +/- 6.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -58.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.8     |\n",
      "|    critic_loss     | 3.8e+03  |\n",
      "|    ent_coef        | 0.975    |\n",
      "|    ent_coef_loss   | -0.0871  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 87       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.3     |\n",
      "|    critic_loss   | 2.92e+03 |\n",
      "|    ent_coef      | 0.974    |\n",
      "|    ent_coef_loss | -0.0875  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 88       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=880, episode_reward=-112.52 +/- 40.29\n",
      "Episode length: 3.50 +/- 3.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890, episode_reward=-41.45 +/- 84.99\n",
      "Episode length: 11.80 +/- 6.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | -41.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.9     |\n",
      "|    critic_loss     | 3.97e+03 |\n",
      "|    ent_coef        | 0.974    |\n",
      "|    ent_coef_loss   | -0.0887  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.1     |\n",
      "|    critic_loss   | 4.26e+03 |\n",
      "|    ent_coef      | 0.974    |\n",
      "|    ent_coef_loss | -0.0901  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 90       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=900, episode_reward=-72.46 +/- 72.06\n",
      "Episode length: 9.30 +/- 5.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.3      |\n",
      "|    mean_reward     | -72.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.3     |\n",
      "|    critic_loss   | 5.14e+03 |\n",
      "|    ent_coef      | 0.973    |\n",
      "|    ent_coef_loss | -0.0905  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 91       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=910, episode_reward=-85.47 +/- 60.58\n",
      "Episode length: 6.70 +/- 5.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -85.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.7     |\n",
      "|    critic_loss   | 3.59e+03 |\n",
      "|    ent_coef      | 0.973    |\n",
      "|    ent_coef_loss | -0.0913  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 92       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=920, episode_reward=-112.34 +/- 52.21\n",
      "Episode length: 3.60 +/- 2.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.3     |\n",
      "|    critic_loss   | 4.01e+03 |\n",
      "|    ent_coef      | 0.973    |\n",
      "|    ent_coef_loss | -0.093   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 93       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=930, episode_reward=-153.80 +/- 49.78\n",
      "Episode length: 2.00 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940, episode_reward=-100.99 +/- 58.86\n",
      "Episode length: 4.70 +/- 4.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 4.7e+03  |\n",
      "|    ent_coef        | 0.972    |\n",
      "|    ent_coef_loss   | -0.0942  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 94       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950, episode_reward=-77.69 +/- 62.30\n",
      "Episode length: 5.30 +/- 4.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -77.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 3.77e+03 |\n",
      "|    ent_coef        | 0.972    |\n",
      "|    ent_coef_loss   | -0.0948  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 95       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 20.8     |\n",
      "|    critic_loss   | 4.31e+03 |\n",
      "|    ent_coef      | 0.972    |\n",
      "|    ent_coef_loss | -0.0971  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 96       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=960, episode_reward=-83.74 +/- 64.82\n",
      "Episode length: 5.80 +/- 4.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -83.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970, episode_reward=-144.48 +/- 45.83\n",
      "Episode length: 2.00 +/- 1.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.2     |\n",
      "|    critic_loss     | 4.37e+03 |\n",
      "|    ent_coef        | 0.972    |\n",
      "|    ent_coef_loss   | -0.0966  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 97       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980, episode_reward=-104.91 +/- 58.86\n",
      "Episode length: 4.40 +/- 4.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.3     |\n",
      "|    critic_loss     | 2.57e+03 |\n",
      "|    ent_coef        | 0.971    |\n",
      "|    ent_coef_loss   | -0.0985  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 98       |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.2     |\n",
      "|    critic_loss   | 3.3e+03  |\n",
      "|    ent_coef      | 0.971    |\n",
      "|    ent_coef_loss | -0.098   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 99       |\n",
      "-------------------------------\n",
      "Eval num_timesteps=990, episode_reward=-98.08 +/- 70.46\n",
      "Episode length: 5.30 +/- 4.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.9     |\n",
      "|    critic_loss   | 4.58e+03 |\n",
      "|    ent_coef      | 0.971    |\n",
      "|    ent_coef_loss | -0.0985  |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 100      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-101.24 +/- 54.15\n",
      "Episode length: 5.00 +/- 3.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010, episode_reward=-120.78 +/- 63.07\n",
      "Episode length: 5.90 +/- 5.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.2     |\n",
      "|    critic_loss     | 4.05e+03 |\n",
      "|    ent_coef        | 0.97     |\n",
      "|    ent_coef_loss   | -0.101   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 101      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.2     |\n",
      "|    critic_loss   | 3.87e+03 |\n",
      "|    ent_coef      | 0.97     |\n",
      "|    ent_coef_loss | -0.1     |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 102      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1020, episode_reward=-108.37 +/- 65.25\n",
      "Episode length: 5.10 +/- 5.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.5     |\n",
      "|    critic_loss   | 4.12e+03 |\n",
      "|    ent_coef      | 0.97     |\n",
      "|    ent_coef_loss | -0.102   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 103      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1030, episode_reward=-137.93 +/- 50.70\n",
      "Episode length: 2.70 +/- 3.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040, episode_reward=-141.00 +/- 115.26\n",
      "Episode length: 3.70 +/- 3.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.4     |\n",
      "|    critic_loss     | 4.37e+03 |\n",
      "|    ent_coef        | 0.97     |\n",
      "|    ent_coef_loss   | -0.104   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 104      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050, episode_reward=-148.36 +/- 80.29\n",
      "Episode length: 4.00 +/- 3.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.5     |\n",
      "|    critic_loss     | 3.69e+03 |\n",
      "|    ent_coef        | 0.969    |\n",
      "|    ent_coef_loss   | -0.104   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 105      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060, episode_reward=-100.74 +/- 50.58\n",
      "Episode length: 4.90 +/- 4.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.9     |\n",
      "|    critic_loss     | 3.94e+03 |\n",
      "|    ent_coef        | 0.969    |\n",
      "|    ent_coef_loss   | -0.107   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 106      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.3     |\n",
      "|    critic_loss   | 1.36e+03 |\n",
      "|    ent_coef      | 0.969    |\n",
      "|    ent_coef_loss | -0.107   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 107      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1070, episode_reward=-63.51 +/- 71.59\n",
      "Episode length: 8.10 +/- 6.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.1      |\n",
      "|    mean_reward     | -63.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080, episode_reward=-86.81 +/- 54.39\n",
      "Episode length: 4.30 +/- 3.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -86.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 3.55e+03 |\n",
      "|    ent_coef        | 0.968    |\n",
      "|    ent_coef_loss   | -0.106   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 108      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090, episode_reward=-95.41 +/- 64.90\n",
      "Episode length: 5.50 +/- 4.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -95.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 2.71e+03 |\n",
      "|    ent_coef        | 0.968    |\n",
      "|    ent_coef_loss   | -0.109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.4     |\n",
      "|    critic_loss   | 4.75e+03 |\n",
      "|    ent_coef      | 0.968    |\n",
      "|    ent_coef_loss | -0.11    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 110      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1100, episode_reward=-109.03 +/- 95.87\n",
      "Episode length: 6.40 +/- 6.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.1     |\n",
      "|    critic_loss   | 3.01e+03 |\n",
      "|    ent_coef      | 0.968    |\n",
      "|    ent_coef_loss | -0.11    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 111      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1110, episode_reward=-70.72 +/- 86.11\n",
      "Episode length: 8.50 +/- 6.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.5      |\n",
      "|    mean_reward     | -70.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 17       |\n",
      "|    critic_loss   | 4.04e+03 |\n",
      "|    ent_coef      | 0.967    |\n",
      "|    ent_coef_loss | -0.112   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 112      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1120, episode_reward=-105.49 +/- 56.21\n",
      "Episode length: 3.00 +/- 2.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 18.3     |\n",
      "|    critic_loss   | 3.43e+03 |\n",
      "|    ent_coef      | 0.967    |\n",
      "|    ent_coef_loss | -0.114   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 113      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1130, episode_reward=-43.91 +/- 67.58\n",
      "Episode length: 9.50 +/- 4.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | -43.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140, episode_reward=-70.47 +/- 60.96\n",
      "Episode length: 6.30 +/- 4.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -70.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.2     |\n",
      "|    critic_loss     | 2.85e+03 |\n",
      "|    ent_coef        | 0.967    |\n",
      "|    ent_coef_loss   | -0.114   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 114      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\logger.py:218: UserWarning: Tried to write empty key-value dict\n",
      "  warnings.warn(\"Tried to write empty key-value dict\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1150, episode_reward=-104.78 +/- 54.36\n",
      "Episode length: 5.50 +/- 4.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.3     |\n",
      "|    critic_loss     | 3.13e+03 |\n",
      "|    ent_coef        | 0.966    |\n",
      "|    ent_coef_loss   | -0.115   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 115      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 18.5     |\n",
      "|    critic_loss   | 2.88e+03 |\n",
      "|    ent_coef      | 0.966    |\n",
      "|    ent_coef_loss | -0.115   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 116      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1160, episode_reward=-79.68 +/- 61.93\n",
      "Episode length: 5.10 +/- 5.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -79.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170, episode_reward=-90.56 +/- 66.91\n",
      "Episode length: 8.40 +/- 8.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | -90.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.6     |\n",
      "|    critic_loss     | 3.82e+03 |\n",
      "|    ent_coef        | 0.966    |\n",
      "|    ent_coef_loss   | -0.117   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 117      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180, episode_reward=-109.10 +/- 73.36\n",
      "Episode length: 4.00 +/- 5.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.5     |\n",
      "|    critic_loss     | 4.07e+03 |\n",
      "|    ent_coef        | 0.965    |\n",
      "|    ent_coef_loss   | -0.117   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 118      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.2     |\n",
      "|    critic_loss   | 3.92e+03 |\n",
      "|    ent_coef      | 0.965    |\n",
      "|    ent_coef_loss | -0.119   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 119      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1190, episode_reward=-125.98 +/- 66.57\n",
      "Episode length: 4.20 +/- 3.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200, episode_reward=-91.03 +/- 75.12\n",
      "Episode length: 6.10 +/- 5.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -91      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.8     |\n",
      "|    critic_loss     | 3.56e+03 |\n",
      "|    ent_coef        | 0.965    |\n",
      "|    ent_coef_loss   | -0.119   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 120      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.7     |\n",
      "|    critic_loss   | 5.84e+03 |\n",
      "|    ent_coef      | 0.965    |\n",
      "|    ent_coef_loss | -0.122   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 121      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1210, episode_reward=-87.91 +/- 94.87\n",
      "Episode length: 6.30 +/- 7.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -87.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220, episode_reward=-66.16 +/- 58.98\n",
      "Episode length: 7.60 +/- 7.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | -66.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.4     |\n",
      "|    critic_loss     | 4.57e+03 |\n",
      "|    ent_coef        | 0.964    |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 122      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 27.9     |\n",
      "|    critic_loss   | 4.96e+03 |\n",
      "|    ent_coef      | 0.964    |\n",
      "|    ent_coef_loss | -0.122   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 123      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1230, episode_reward=-137.77 +/- 58.91\n",
      "Episode length: 2.80 +/- 2.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29.4     |\n",
      "|    critic_loss   | 3.01e+03 |\n",
      "|    ent_coef      | 0.964    |\n",
      "|    ent_coef_loss | -0.124   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 124      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1240, episode_reward=-98.02 +/- 89.20\n",
      "Episode length: 5.20 +/- 7.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250, episode_reward=-130.73 +/- 62.59\n",
      "Episode length: 4.70 +/- 4.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.7     |\n",
      "|    critic_loss     | 2.78e+03 |\n",
      "|    ent_coef        | 0.963    |\n",
      "|    ent_coef_loss   | -0.125   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 125      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260, episode_reward=-115.27 +/- 48.10\n",
      "Episode length: 4.50 +/- 3.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.7     |\n",
      "|    critic_loss     | 4.09e+03 |\n",
      "|    ent_coef        | 0.963    |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 126      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.6     |\n",
      "|    critic_loss   | 5.71e+03 |\n",
      "|    ent_coef      | 0.963    |\n",
      "|    ent_coef_loss | -0.129   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 127      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1270, episode_reward=-100.67 +/- 53.19\n",
      "Episode length: 5.00 +/- 3.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280, episode_reward=-102.25 +/- 49.64\n",
      "Episode length: 3.40 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.2     |\n",
      "|    critic_loss     | 3.31e+03 |\n",
      "|    ent_coef        | 0.963    |\n",
      "|    ent_coef_loss   | -0.127   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 128      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.8     |\n",
      "|    critic_loss   | 4.83e+03 |\n",
      "|    ent_coef      | 0.962    |\n",
      "|    ent_coef_loss | -0.13    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 129      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1290, episode_reward=-77.93 +/- 72.24\n",
      "Episode length: 7.50 +/- 5.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | -77.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 27.7     |\n",
      "|    critic_loss   | 4.14e+03 |\n",
      "|    ent_coef      | 0.962    |\n",
      "|    ent_coef_loss | -0.131   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 130      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1300, episode_reward=-94.80 +/- 70.35\n",
      "Episode length: 5.50 +/- 4.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310, episode_reward=-149.43 +/- 65.68\n",
      "Episode length: 3.80 +/- 4.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.5     |\n",
      "|    critic_loss     | 1.37e+03 |\n",
      "|    ent_coef        | 0.962    |\n",
      "|    ent_coef_loss   | -0.132   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 131      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.3     |\n",
      "|    critic_loss   | 2.83e+03 |\n",
      "|    ent_coef      | 0.961    |\n",
      "|    ent_coef_loss | -0.133   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 132      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1320, episode_reward=-111.97 +/- 59.44\n",
      "Episode length: 5.80 +/- 4.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.2     |\n",
      "|    critic_loss   | 5.92e+03 |\n",
      "|    ent_coef      | 0.961    |\n",
      "|    ent_coef_loss | -0.133   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 133      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1330, episode_reward=-125.37 +/- 49.00\n",
      "Episode length: 3.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340, episode_reward=-144.52 +/- 50.38\n",
      "Episode length: 4.20 +/- 4.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.8     |\n",
      "|    critic_loss     | 3.12e+03 |\n",
      "|    ent_coef        | 0.961    |\n",
      "|    ent_coef_loss   | -0.135   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 134      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350, episode_reward=-137.15 +/- 103.85\n",
      "Episode length: 4.20 +/- 4.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.4     |\n",
      "|    critic_loss     | 2.01e+03 |\n",
      "|    ent_coef        | 0.961    |\n",
      "|    ent_coef_loss   | -0.135   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 135      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 18.3     |\n",
      "|    critic_loss   | 2.94e+03 |\n",
      "|    ent_coef      | 0.96     |\n",
      "|    ent_coef_loss | -0.136   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 136      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1360, episode_reward=-111.35 +/- 69.85\n",
      "Episode length: 6.10 +/- 5.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370, episode_reward=-96.26 +/- 57.10\n",
      "Episode length: 5.50 +/- 5.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.4     |\n",
      "|    critic_loss     | 2.09e+03 |\n",
      "|    ent_coef        | 0.96     |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 137      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 17.2     |\n",
      "|    critic_loss   | 4.05e+03 |\n",
      "|    ent_coef      | 0.96     |\n",
      "|    ent_coef_loss | -0.139   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 138      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1380, episode_reward=-117.26 +/- 57.69\n",
      "Episode length: 4.20 +/- 3.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 14.2     |\n",
      "|    critic_loss   | 4.04e+03 |\n",
      "|    ent_coef      | 0.959    |\n",
      "|    ent_coef_loss | -0.138   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 139      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1390, episode_reward=-123.05 +/- 43.84\n",
      "Episode length: 2.10 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.1      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.4     |\n",
      "|    critic_loss   | 4.04e+03 |\n",
      "|    ent_coef      | 0.959    |\n",
      "|    ent_coef_loss | -0.14    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 140      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1400, episode_reward=-70.19 +/- 58.06\n",
      "Episode length: 7.30 +/- 5.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | -70.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 17.2     |\n",
      "|    critic_loss   | 2.45e+03 |\n",
      "|    ent_coef      | 0.959    |\n",
      "|    ent_coef_loss | -0.142   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 141      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1410, episode_reward=-133.20 +/- 108.17\n",
      "Episode length: 5.40 +/- 6.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420, episode_reward=-137.45 +/- 38.62\n",
      "Episode length: 3.80 +/- 3.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.3     |\n",
      "|    critic_loss     | 4.16e+03 |\n",
      "|    ent_coef        | 0.959    |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 142      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430, episode_reward=-98.17 +/- 52.62\n",
      "Episode length: 3.80 +/- 3.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -98.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.2     |\n",
      "|    critic_loss     | 1.9e+03  |\n",
      "|    ent_coef        | 0.958    |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 143      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.1     |\n",
      "|    critic_loss   | 2.31e+03 |\n",
      "|    ent_coef      | 0.958    |\n",
      "|    ent_coef_loss | -0.142   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 144      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1440, episode_reward=-110.34 +/- 58.25\n",
      "Episode length: 3.80 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450, episode_reward=-128.06 +/- 55.04\n",
      "Episode length: 5.10 +/- 5.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 3.26e+03 |\n",
      "|    ent_coef        | 0.958    |\n",
      "|    ent_coef_loss   | -0.143   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 145      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460, episode_reward=-118.48 +/- 80.03\n",
      "Episode length: 6.20 +/- 4.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 1.55e+03 |\n",
      "|    ent_coef        | 0.957    |\n",
      "|    ent_coef_loss   | -0.146   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 146      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470, episode_reward=-108.30 +/- 60.31\n",
      "Episode length: 6.40 +/- 4.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.8     |\n",
      "|    critic_loss     | 2.79e+03 |\n",
      "|    ent_coef        | 0.957    |\n",
      "|    ent_coef_loss   | -0.146   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 147      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480, episode_reward=-91.81 +/- 65.32\n",
      "Episode length: 4.90 +/- 4.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 4.04e+03 |\n",
      "|    ent_coef        | 0.957    |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 148      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 22.8     |\n",
      "|    critic_loss   | 2.56e+03 |\n",
      "|    ent_coef      | 0.957    |\n",
      "|    ent_coef_loss | -0.147   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 149      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1490, episode_reward=-111.62 +/- 55.03\n",
      "Episode length: 6.00 +/- 4.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 17       |\n",
      "|    critic_loss   | 2.79e+03 |\n",
      "|    ent_coef      | 0.956    |\n",
      "|    ent_coef_loss | -0.148   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 150      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-141.59 +/- 44.84\n",
      "Episode length: 3.50 +/- 3.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510, episode_reward=-98.49 +/- 52.01\n",
      "Episode length: 5.30 +/- 4.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -98.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 4.38e+03 |\n",
      "|    ent_coef        | 0.956    |\n",
      "|    ent_coef_loss   | -0.151   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 151      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520, episode_reward=-110.45 +/- 105.88\n",
      "Episode length: 3.60 +/- 3.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 1.9e+03  |\n",
      "|    ent_coef        | 0.956    |\n",
      "|    ent_coef_loss   | -0.149   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 152      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530, episode_reward=-114.86 +/- 50.75\n",
      "Episode length: 4.30 +/- 3.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.5     |\n",
      "|    critic_loss     | 2.55e+03 |\n",
      "|    ent_coef        | 0.955    |\n",
      "|    ent_coef_loss   | -0.152   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 153      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 22.3     |\n",
      "|    critic_loss   | 2.49e+03 |\n",
      "|    ent_coef      | 0.955    |\n",
      "|    ent_coef_loss | -0.155   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 154      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1540, episode_reward=-104.69 +/- 53.44\n",
      "Episode length: 3.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550, episode_reward=-84.66 +/- 92.96\n",
      "Episode length: 8.10 +/- 6.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.1      |\n",
      "|    mean_reward     | -84.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 2.8e+03  |\n",
      "|    ent_coef        | 0.955    |\n",
      "|    ent_coef_loss   | -0.156   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 155      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 27       |\n",
      "|    critic_loss   | 3.58e+03 |\n",
      "|    ent_coef      | 0.955    |\n",
      "|    ent_coef_loss | -0.157   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 156      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1560, episode_reward=-145.90 +/- 43.10\n",
      "Episode length: 4.30 +/- 3.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570, episode_reward=-106.78 +/- 38.11\n",
      "Episode length: 5.10 +/- 4.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25       |\n",
      "|    critic_loss     | 4e+03    |\n",
      "|    ent_coef        | 0.954    |\n",
      "|    ent_coef_loss   | -0.155   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 157      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29.7     |\n",
      "|    critic_loss   | 2.43e+03 |\n",
      "|    ent_coef      | 0.954    |\n",
      "|    ent_coef_loss | -0.157   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 158      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1580, episode_reward=-134.88 +/- 46.43\n",
      "Episode length: 4.40 +/- 3.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590, episode_reward=-126.40 +/- 48.36\n",
      "Episode length: 4.10 +/- 3.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.9     |\n",
      "|    critic_loss     | 4.8e+03  |\n",
      "|    ent_coef        | 0.954    |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600, episode_reward=-132.33 +/- 67.71\n",
      "Episode length: 4.80 +/- 5.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.9     |\n",
      "|    critic_loss     | 2.92e+03 |\n",
      "|    ent_coef        | 0.953    |\n",
      "|    ent_coef_loss   | -0.157   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 160      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.7     |\n",
      "|    critic_loss   | 1.85e+03 |\n",
      "|    ent_coef      | 0.953    |\n",
      "|    ent_coef_loss | -0.159   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 161      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1610, episode_reward=-121.36 +/- 50.75\n",
      "Episode length: 3.60 +/- 2.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620, episode_reward=-122.16 +/- 77.08\n",
      "Episode length: 7.30 +/- 4.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.9     |\n",
      "|    critic_loss     | 4.52e+03 |\n",
      "|    ent_coef        | 0.953    |\n",
      "|    ent_coef_loss   | -0.161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 162      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630, episode_reward=-144.28 +/- 76.11\n",
      "Episode length: 4.30 +/- 3.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.6     |\n",
      "|    critic_loss     | 3.26e+03 |\n",
      "|    ent_coef        | 0.953    |\n",
      "|    ent_coef_loss   | -0.164   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 163      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.7     |\n",
      "|    critic_loss   | 4.41e+03 |\n",
      "|    ent_coef      | 0.952    |\n",
      "|    ent_coef_loss | -0.162   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 164      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1640, episode_reward=-132.61 +/- 70.92\n",
      "Episode length: 4.50 +/- 5.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650, episode_reward=-112.19 +/- 56.14\n",
      "Episode length: 3.50 +/- 4.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.4     |\n",
      "|    critic_loss     | 2.87e+03 |\n",
      "|    ent_coef        | 0.952    |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 165      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 22.5     |\n",
      "|    critic_loss   | 4.29e+03 |\n",
      "|    ent_coef      | 0.952    |\n",
      "|    ent_coef_loss | -0.163   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 166      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1660, episode_reward=-114.53 +/- 56.35\n",
      "Episode length: 5.70 +/- 3.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670, episode_reward=-132.87 +/- 53.37\n",
      "Episode length: 3.60 +/- 2.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.7     |\n",
      "|    critic_loss     | 5.78e+03 |\n",
      "|    ent_coef        | 0.951    |\n",
      "|    ent_coef_loss   | -0.166   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 167      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.3     |\n",
      "|    critic_loss   | 1.88e+03 |\n",
      "|    ent_coef      | 0.951    |\n",
      "|    ent_coef_loss | -0.172   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 168      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1680, episode_reward=-120.50 +/- 52.15\n",
      "Episode length: 4.00 +/- 3.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690, episode_reward=-90.60 +/- 36.04\n",
      "Episode length: 3.90 +/- 3.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -90.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23       |\n",
      "|    critic_loss     | 3.08e+03 |\n",
      "|    ent_coef        | 0.951    |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700, episode_reward=-131.79 +/- 48.68\n",
      "Episode length: 3.70 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.8     |\n",
      "|    critic_loss     | 3.75e+03 |\n",
      "|    ent_coef        | 0.951    |\n",
      "|    ent_coef_loss   | -0.169   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 170      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.6     |\n",
      "|    critic_loss   | 3.14e+03 |\n",
      "|    ent_coef      | 0.95     |\n",
      "|    ent_coef_loss | -0.166   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 171      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1710, episode_reward=-116.25 +/- 56.49\n",
      "Episode length: 5.10 +/- 5.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720, episode_reward=-162.19 +/- 56.42\n",
      "Episode length: 3.10 +/- 2.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.9     |\n",
      "|    critic_loss     | 4.84e+03 |\n",
      "|    ent_coef        | 0.95     |\n",
      "|    ent_coef_loss   | -0.165   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 172      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 27.7     |\n",
      "|    critic_loss   | 1.84e+03 |\n",
      "|    ent_coef      | 0.95     |\n",
      "|    ent_coef_loss | -0.171   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 173      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1730, episode_reward=-162.68 +/- 42.62\n",
      "Episode length: 2.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 32.7     |\n",
      "|    critic_loss   | 4.11e+03 |\n",
      "|    ent_coef      | 0.949    |\n",
      "|    ent_coef_loss | -0.17    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 174      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1740, episode_reward=-137.96 +/- 49.59\n",
      "Episode length: 1.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.7      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.4     |\n",
      "|    critic_loss   | 4.22e+03 |\n",
      "|    ent_coef      | 0.949    |\n",
      "|    ent_coef_loss | -0.162   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 175      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1750, episode_reward=-138.57 +/- 61.87\n",
      "Episode length: 2.50 +/- 2.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.1     |\n",
      "|    critic_loss   | 2.18e+03 |\n",
      "|    ent_coef      | 0.949    |\n",
      "|    ent_coef_loss | -0.164   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 176      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1760, episode_reward=-141.40 +/- 74.62\n",
      "Episode length: 3.70 +/- 4.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.3     |\n",
      "|    critic_loss   | 3.37e+03 |\n",
      "|    ent_coef      | 0.949    |\n",
      "|    ent_coef_loss | -0.168   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 177      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1770, episode_reward=-142.26 +/- 43.10\n",
      "Episode length: 4.80 +/- 3.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.9     |\n",
      "|    critic_loss   | 1.93e+03 |\n",
      "|    ent_coef      | 0.948    |\n",
      "|    ent_coef_loss | -0.171   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 178      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1780, episode_reward=-124.82 +/- 36.89\n",
      "Episode length: 5.50 +/- 3.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.3     |\n",
      "|    critic_loss   | 2.61e+03 |\n",
      "|    ent_coef      | 0.948    |\n",
      "|    ent_coef_loss | -0.165   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 179      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1790, episode_reward=-102.47 +/- 47.26\n",
      "Episode length: 5.60 +/- 4.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800, episode_reward=-127.26 +/- 52.35\n",
      "Episode length: 3.10 +/- 2.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 4.44e+03 |\n",
      "|    ent_coef        | 0.948    |\n",
      "|    ent_coef_loss   | -0.172   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810, episode_reward=-132.16 +/- 92.55\n",
      "Episode length: 3.40 +/- 3.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 4.76e+03 |\n",
      "|    ent_coef        | 0.947    |\n",
      "|    ent_coef_loss   | -0.174   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 181      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 14.1     |\n",
      "|    critic_loss   | 4.49e+03 |\n",
      "|    ent_coef      | 0.947    |\n",
      "|    ent_coef_loss | -0.178   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 182      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1820, episode_reward=-152.94 +/- 68.83\n",
      "Episode length: 4.50 +/- 5.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830, episode_reward=-102.23 +/- 53.72\n",
      "Episode length: 5.90 +/- 6.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 2.26e+03 |\n",
      "|    ent_coef        | 0.947    |\n",
      "|    ent_coef_loss   | -0.179   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 183      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 15.5     |\n",
      "|    critic_loss   | 3.82e+03 |\n",
      "|    ent_coef      | 0.947    |\n",
      "|    ent_coef_loss | -0.183   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 184      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1840, episode_reward=-100.83 +/- 48.92\n",
      "Episode length: 3.50 +/- 3.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850, episode_reward=-102.28 +/- 48.19\n",
      "Episode length: 4.60 +/- 3.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 2.73e+03 |\n",
      "|    ent_coef        | 0.946    |\n",
      "|    ent_coef_loss   | -0.183   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 185      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860, episode_reward=-100.58 +/- 48.42\n",
      "Episode length: 4.90 +/- 2.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 2.42e+03 |\n",
      "|    ent_coef        | 0.946    |\n",
      "|    ent_coef_loss   | -0.185   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 186      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.5     |\n",
      "|    critic_loss   | 2.37e+03 |\n",
      "|    ent_coef      | 0.946    |\n",
      "|    ent_coef_loss | -0.185   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 187      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1870, episode_reward=-130.73 +/- 64.97\n",
      "Episode length: 3.50 +/- 4.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 32.7     |\n",
      "|    critic_loss   | 4.32e+03 |\n",
      "|    ent_coef      | 0.946    |\n",
      "|    ent_coef_loss | -0.183   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 188      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1880, episode_reward=-97.67 +/- 68.40\n",
      "Episode length: 6.40 +/- 4.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -97.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890, episode_reward=-111.83 +/- 41.31\n",
      "Episode length: 4.90 +/- 3.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.6     |\n",
      "|    critic_loss     | 3.64e+03 |\n",
      "|    ent_coef        | 0.945    |\n",
      "|    ent_coef_loss   | -0.184   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 32.8     |\n",
      "|    critic_loss   | 2.76e+03 |\n",
      "|    ent_coef      | 0.945    |\n",
      "|    ent_coef_loss | -0.188   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 190      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1900, episode_reward=-123.37 +/- 99.97\n",
      "Episode length: 4.50 +/- 3.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 31.8     |\n",
      "|    critic_loss   | 2.9e+03  |\n",
      "|    ent_coef      | 0.945    |\n",
      "|    ent_coef_loss | -0.188   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 191      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1910, episode_reward=-117.89 +/- 50.65\n",
      "Episode length: 1.70 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.7      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 35.5     |\n",
      "|    critic_loss   | 4.45e+03 |\n",
      "|    ent_coef      | 0.944    |\n",
      "|    ent_coef_loss | -0.188   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 192      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1920, episode_reward=-145.45 +/- 71.71\n",
      "Episode length: 2.90 +/- 4.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930, episode_reward=-86.96 +/- 44.77\n",
      "Episode length: 4.20 +/- 3.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -87      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 30       |\n",
      "|    critic_loss     | 2.84e+03 |\n",
      "|    ent_coef        | 0.944    |\n",
      "|    ent_coef_loss   | -0.189   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 193      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29.9     |\n",
      "|    critic_loss   | 2.59e+03 |\n",
      "|    ent_coef      | 0.944    |\n",
      "|    ent_coef_loss | -0.19    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 194      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1940, episode_reward=-108.98 +/- 51.25\n",
      "Episode length: 2.90 +/- 3.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950, episode_reward=-156.75 +/- 72.59\n",
      "Episode length: 3.20 +/- 3.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 2.21e+03 |\n",
      "|    ent_coef        | 0.944    |\n",
      "|    ent_coef_loss   | -0.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 195      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 20.8     |\n",
      "|    critic_loss   | 2.79e+03 |\n",
      "|    ent_coef      | 0.943    |\n",
      "|    ent_coef_loss | -0.194   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 196      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1960, episode_reward=-116.21 +/- 46.60\n",
      "Episode length: 4.40 +/- 4.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.1     |\n",
      "|    critic_loss   | 2.92e+03 |\n",
      "|    ent_coef      | 0.943    |\n",
      "|    ent_coef_loss | -0.195   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 197      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1970, episode_reward=-133.86 +/- 94.31\n",
      "Episode length: 4.50 +/- 3.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980, episode_reward=-112.23 +/- 81.96\n",
      "Episode length: 5.60 +/- 6.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.2     |\n",
      "|    critic_loss     | 4.06e+03 |\n",
      "|    ent_coef        | 0.943    |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 198      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990, episode_reward=-137.94 +/- 53.57\n",
      "Episode length: 4.00 +/- 3.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.6     |\n",
      "|    critic_loss     | 2.15e+03 |\n",
      "|    ent_coef        | 0.942    |\n",
      "|    ent_coef_loss   | -0.194   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-124.11 +/- 74.12\n",
      "Episode length: 4.80 +/- 4.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.5     |\n",
      "|    critic_loss     | 3.82e+03 |\n",
      "|    ent_coef        | 0.942    |\n",
      "|    ent_coef_loss   | -0.196   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 200      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 26.1     |\n",
      "|    critic_loss   | 2.91e+03 |\n",
      "|    ent_coef      | 0.942    |\n",
      "|    ent_coef_loss | -0.2     |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 201      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2010, episode_reward=-122.01 +/- 72.40\n",
      "Episode length: 3.80 +/- 3.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.4     |\n",
      "|    critic_loss   | 1.71e+03 |\n",
      "|    ent_coef      | 0.942    |\n",
      "|    ent_coef_loss | -0.195   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 202      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2020, episode_reward=-112.87 +/- 67.66\n",
      "Episode length: 4.80 +/- 4.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.9     |\n",
      "|    critic_loss   | 2.37e+03 |\n",
      "|    ent_coef      | 0.941    |\n",
      "|    ent_coef_loss | -0.2     |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 203      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2030, episode_reward=-126.15 +/- 58.47\n",
      "Episode length: 5.30 +/- 4.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 20       |\n",
      "|    critic_loss   | 1.61e+03 |\n",
      "|    ent_coef      | 0.941    |\n",
      "|    ent_coef_loss | -0.2     |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 204      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2040, episode_reward=-119.69 +/- 78.76\n",
      "Episode length: 4.10 +/- 3.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.5     |\n",
      "|    critic_loss   | 5.03e+03 |\n",
      "|    ent_coef      | 0.941    |\n",
      "|    ent_coef_loss | -0.202   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 205      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2050, episode_reward=-141.98 +/- 61.99\n",
      "Episode length: 4.70 +/- 6.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060, episode_reward=-120.31 +/- 69.63\n",
      "Episode length: 8.60 +/- 5.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.6     |\n",
      "|    critic_loss     | 4.69e+03 |\n",
      "|    ent_coef        | 0.941    |\n",
      "|    ent_coef_loss   | -0.201   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 206      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070, episode_reward=-94.28 +/- 75.75\n",
      "Episode length: 5.90 +/- 5.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -94.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.5     |\n",
      "|    critic_loss     | 2.78e+03 |\n",
      "|    ent_coef        | 0.94     |\n",
      "|    ent_coef_loss   | -0.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 207      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.9     |\n",
      "|    critic_loss   | 2.12e+03 |\n",
      "|    ent_coef      | 0.94     |\n",
      "|    ent_coef_loss | -0.203   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 208      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2080, episode_reward=-103.03 +/- 73.37\n",
      "Episode length: 5.00 +/- 4.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090, episode_reward=-127.11 +/- 56.75\n",
      "Episode length: 5.20 +/- 6.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.2     |\n",
      "|    critic_loss     | 4.93e+03 |\n",
      "|    ent_coef        | 0.94     |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.1     |\n",
      "|    critic_loss   | 2.96e+03 |\n",
      "|    ent_coef      | 0.939    |\n",
      "|    ent_coef_loss | -0.213   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 210      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2100, episode_reward=-100.66 +/- 43.29\n",
      "Episode length: 8.80 +/- 5.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110, episode_reward=-85.79 +/- 60.85\n",
      "Episode length: 4.40 +/- 4.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -85.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 2.71e+03 |\n",
      "|    ent_coef        | 0.939    |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 211      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 23.8     |\n",
      "|    critic_loss   | 2.91e+03 |\n",
      "|    ent_coef      | 0.939    |\n",
      "|    ent_coef_loss | -0.208   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 212      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2120, episode_reward=-165.60 +/- 91.22\n",
      "Episode length: 2.00 +/- 2.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130, episode_reward=-115.91 +/- 78.02\n",
      "Episode length: 4.60 +/- 4.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 2.37e+03 |\n",
      "|    ent_coef        | 0.939    |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 213      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.9     |\n",
      "|    critic_loss   | 3.65e+03 |\n",
      "|    ent_coef      | 0.938    |\n",
      "|    ent_coef_loss | -0.209   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 214      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2140, episode_reward=-104.70 +/- 54.66\n",
      "Episode length: 5.40 +/- 6.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150, episode_reward=-100.98 +/- 73.03\n",
      "Episode length: 5.90 +/- 5.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.3     |\n",
      "|    critic_loss     | 2.73e+03 |\n",
      "|    ent_coef        | 0.938    |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 215      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34.5     |\n",
      "|    critic_loss   | 4.48e+03 |\n",
      "|    ent_coef      | 0.938    |\n",
      "|    ent_coef_loss | -0.218   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 216      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2160, episode_reward=-138.27 +/- 99.23\n",
      "Episode length: 4.20 +/- 4.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170, episode_reward=-116.93 +/- 58.95\n",
      "Episode length: 6.50 +/- 5.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.7     |\n",
      "|    critic_loss     | 2.28e+03 |\n",
      "|    ent_coef        | 0.937    |\n",
      "|    ent_coef_loss   | -0.212   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 217      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180, episode_reward=-96.27 +/- 34.74\n",
      "Episode length: 5.70 +/- 4.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 30       |\n",
      "|    critic_loss     | 4.13e+03 |\n",
      "|    ent_coef        | 0.937    |\n",
      "|    ent_coef_loss   | -0.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 218      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190, episode_reward=-102.03 +/- 48.83\n",
      "Episode length: 4.80 +/- 3.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.4     |\n",
      "|    critic_loss     | 3.87e+03 |\n",
      "|    ent_coef        | 0.937    |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34.1     |\n",
      "|    critic_loss   | 2.69e+03 |\n",
      "|    ent_coef      | 0.937    |\n",
      "|    ent_coef_loss | -0.215   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 220      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2200, episode_reward=-136.24 +/- 64.24\n",
      "Episode length: 6.40 +/- 6.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210, episode_reward=-123.44 +/- 55.83\n",
      "Episode length: 5.70 +/- 4.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33.3     |\n",
      "|    critic_loss     | 3.17e+03 |\n",
      "|    ent_coef        | 0.936    |\n",
      "|    ent_coef_loss   | -0.221   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 221      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 36.9     |\n",
      "|    critic_loss   | 3.04e+03 |\n",
      "|    ent_coef      | 0.936    |\n",
      "|    ent_coef_loss | -0.215   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 222      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2220, episode_reward=-128.41 +/- 67.56\n",
      "Episode length: 4.00 +/- 3.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230, episode_reward=-134.23 +/- 38.89\n",
      "Episode length: 4.40 +/- 4.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.5     |\n",
      "|    critic_loss     | 3.31e+03 |\n",
      "|    ent_coef        | 0.936    |\n",
      "|    ent_coef_loss   | -0.222   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 223      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 32.4     |\n",
      "|    critic_loss   | 1.38e+03 |\n",
      "|    ent_coef      | 0.936    |\n",
      "|    ent_coef_loss | -0.213   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 224      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2240, episode_reward=-141.47 +/- 103.84\n",
      "Episode length: 3.50 +/- 4.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250, episode_reward=-153.42 +/- 68.66\n",
      "Episode length: 3.30 +/- 3.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.9     |\n",
      "|    critic_loss     | 2.43e+03 |\n",
      "|    ent_coef        | 0.935    |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 225      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.6     |\n",
      "|    critic_loss   | 1.67e+03 |\n",
      "|    ent_coef      | 0.935    |\n",
      "|    ent_coef_loss | -0.218   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 226      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2260, episode_reward=-133.07 +/- 63.66\n",
      "Episode length: 4.30 +/- 4.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270, episode_reward=-82.87 +/- 43.07\n",
      "Episode length: 3.70 +/- 3.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -82.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21       |\n",
      "|    critic_loss     | 2.04e+03 |\n",
      "|    ent_coef        | 0.935    |\n",
      "|    ent_coef_loss   | -0.223   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 227      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21.4     |\n",
      "|    critic_loss   | 3.8e+03  |\n",
      "|    ent_coef      | 0.934    |\n",
      "|    ent_coef_loss | -0.227   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 228      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2280, episode_reward=-124.70 +/- 53.31\n",
      "Episode length: 4.70 +/- 4.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290, episode_reward=-102.74 +/- 42.94\n",
      "Episode length: 5.70 +/- 3.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.9     |\n",
      "|    critic_loss     | 4.1e+03  |\n",
      "|    ent_coef        | 0.934    |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 16.2     |\n",
      "|    critic_loss   | 2.35e+03 |\n",
      "|    ent_coef      | 0.934    |\n",
      "|    ent_coef_loss | -0.226   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 230      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2300, episode_reward=-150.34 +/- 47.87\n",
      "Episode length: 4.40 +/- 3.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310, episode_reward=-138.42 +/- 61.84\n",
      "Episode length: 5.10 +/- 3.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.6     |\n",
      "|    critic_loss     | 3.8e+03  |\n",
      "|    ent_coef        | 0.934    |\n",
      "|    ent_coef_loss   | -0.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 231      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 26.1     |\n",
      "|    critic_loss   | 4.97e+03 |\n",
      "|    ent_coef      | 0.933    |\n",
      "|    ent_coef_loss | -0.23    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 232      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2320, episode_reward=-104.37 +/- 37.55\n",
      "Episode length: 6.10 +/- 5.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330, episode_reward=-108.00 +/- 45.60\n",
      "Episode length: 5.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.9     |\n",
      "|    critic_loss     | 2.85e+03 |\n",
      "|    ent_coef        | 0.933    |\n",
      "|    ent_coef_loss   | -0.232   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 233      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340, episode_reward=-117.96 +/- 42.45\n",
      "Episode length: 5.30 +/- 4.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.1     |\n",
      "|    critic_loss     | 5.15e+03 |\n",
      "|    ent_coef        | 0.933    |\n",
      "|    ent_coef_loss   | -0.234   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 234      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350, episode_reward=-117.33 +/- 52.76\n",
      "Episode length: 5.50 +/- 4.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.9     |\n",
      "|    critic_loss     | 3.23e+03 |\n",
      "|    ent_coef        | 0.932    |\n",
      "|    ent_coef_loss   | -0.227   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 235      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360, episode_reward=-139.67 +/- 47.52\n",
      "Episode length: 2.50 +/- 3.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.2     |\n",
      "|    critic_loss     | 2.23e+03 |\n",
      "|    ent_coef        | 0.932    |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 236      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.5     |\n",
      "|    critic_loss   | 3.71e+03 |\n",
      "|    ent_coef      | 0.932    |\n",
      "|    ent_coef_loss | -0.238   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 237      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2370, episode_reward=-138.33 +/- 43.45\n",
      "Episode length: 4.90 +/- 3.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380, episode_reward=-137.98 +/- 48.63\n",
      "Episode length: 4.00 +/- 4.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.1     |\n",
      "|    critic_loss     | 2.12e+03 |\n",
      "|    ent_coef        | 0.932    |\n",
      "|    ent_coef_loss   | -0.236   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 238      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.9     |\n",
      "|    critic_loss   | 2.14e+03 |\n",
      "|    ent_coef      | 0.931    |\n",
      "|    ent_coef_loss | -0.235   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 239      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2390, episode_reward=-115.67 +/- 38.29\n",
      "Episode length: 2.90 +/- 3.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.1     |\n",
      "|    critic_loss   | 3.23e+03 |\n",
      "|    ent_coef      | 0.931    |\n",
      "|    ent_coef_loss | -0.233   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 240      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2400, episode_reward=-166.97 +/- 86.04\n",
      "Episode length: 3.30 +/- 2.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -167     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 32.7     |\n",
      "|    critic_loss   | 2.42e+03 |\n",
      "|    ent_coef      | 0.931    |\n",
      "|    ent_coef_loss | -0.231   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 241      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2410, episode_reward=-157.12 +/- 66.61\n",
      "Episode length: 2.80 +/- 2.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 27.9     |\n",
      "|    critic_loss   | 4.02e+03 |\n",
      "|    ent_coef      | 0.931    |\n",
      "|    ent_coef_loss | -0.224   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 242      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2420, episode_reward=-183.15 +/- 78.72\n",
      "Episode length: 3.30 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33.2     |\n",
      "|    critic_loss   | 4.02e+03 |\n",
      "|    ent_coef      | 0.93     |\n",
      "|    ent_coef_loss | -0.218   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 243      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2430, episode_reward=-140.06 +/- 60.84\n",
      "Episode length: 2.50 +/- 2.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29.1     |\n",
      "|    critic_loss   | 4.9e+03  |\n",
      "|    ent_coef      | 0.93     |\n",
      "|    ent_coef_loss | -0.236   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 244      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2440, episode_reward=-98.85 +/- 67.13\n",
      "Episode length: 4.50 +/- 4.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -98.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450, episode_reward=-128.69 +/- 56.76\n",
      "Episode length: 2.70 +/- 2.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.6     |\n",
      "|    critic_loss     | 2.97e+03 |\n",
      "|    ent_coef        | 0.93     |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 245      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.9     |\n",
      "|    critic_loss   | 1.53e+03 |\n",
      "|    ent_coef      | 0.929    |\n",
      "|    ent_coef_loss | -0.227   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 246      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2460, episode_reward=-89.51 +/- 52.82\n",
      "Episode length: 4.20 +/- 3.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -89.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470, episode_reward=-162.22 +/- 85.48\n",
      "Episode length: 3.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.2     |\n",
      "|    critic_loss     | 3.1e+03  |\n",
      "|    ent_coef        | 0.929    |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 247      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480, episode_reward=-121.81 +/- 61.00\n",
      "Episode length: 4.50 +/- 3.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.3     |\n",
      "|    critic_loss     | 3.02e+03 |\n",
      "|    ent_coef        | 0.929    |\n",
      "|    ent_coef_loss   | -0.243   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 248      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.8     |\n",
      "|    critic_loss   | 3.77e+03 |\n",
      "|    ent_coef      | 0.929    |\n",
      "|    ent_coef_loss | -0.251   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 249      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2490, episode_reward=-135.50 +/- 81.13\n",
      "Episode length: 3.40 +/- 4.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-139.68 +/- 45.94\n",
      "Episode length: 4.10 +/- 3.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.7     |\n",
      "|    critic_loss     | 2.4e+03  |\n",
      "|    ent_coef        | 0.928    |\n",
      "|    ent_coef_loss   | -0.246   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 250      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510, episode_reward=-80.01 +/- 69.14\n",
      "Episode length: 7.10 +/- 7.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -80      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.6     |\n",
      "|    critic_loss     | 4.01e+03 |\n",
      "|    ent_coef        | 0.928    |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 251      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520, episode_reward=-122.65 +/- 60.91\n",
      "Episode length: 4.50 +/- 4.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.7     |\n",
      "|    critic_loss     | 4.15e+03 |\n",
      "|    ent_coef        | 0.928    |\n",
      "|    ent_coef_loss   | -0.246   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 252      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 28.5     |\n",
      "|    critic_loss   | 3.22e+03 |\n",
      "|    ent_coef      | 0.928    |\n",
      "|    ent_coef_loss | -0.251   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 253      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2530, episode_reward=-132.71 +/- 49.69\n",
      "Episode length: 2.20 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.2      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540, episode_reward=-109.93 +/- 39.23\n",
      "Episode length: 7.10 +/- 5.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.2     |\n",
      "|    critic_loss     | 2.62e+03 |\n",
      "|    ent_coef        | 0.927    |\n",
      "|    ent_coef_loss   | -0.244   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 254      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.3     |\n",
      "|    critic_loss   | 1.77e+03 |\n",
      "|    ent_coef      | 0.927    |\n",
      "|    ent_coef_loss | -0.254   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 255      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2550, episode_reward=-124.04 +/- 66.32\n",
      "Episode length: 5.80 +/- 6.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29.5     |\n",
      "|    critic_loss   | 4.45e+03 |\n",
      "|    ent_coef      | 0.927    |\n",
      "|    ent_coef_loss | -0.247   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 256      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2560, episode_reward=-138.14 +/- 44.25\n",
      "Episode length: 1.60 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.6      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 26.3     |\n",
      "|    critic_loss   | 2.28e+03 |\n",
      "|    ent_coef      | 0.926    |\n",
      "|    ent_coef_loss | -0.249   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 257      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2570, episode_reward=-73.95 +/- 84.11\n",
      "Episode length: 8.00 +/- 9.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -74      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580, episode_reward=-123.80 +/- 63.44\n",
      "Episode length: 3.50 +/- 4.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.3     |\n",
      "|    critic_loss     | 3.69e+03 |\n",
      "|    ent_coef        | 0.926    |\n",
      "|    ent_coef_loss   | -0.256   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 258      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590, episode_reward=-89.34 +/- 70.08\n",
      "Episode length: 7.50 +/- 5.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | -89.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.7     |\n",
      "|    critic_loss     | 1.91e+03 |\n",
      "|    ent_coef        | 0.926    |\n",
      "|    ent_coef_loss   | -0.256   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33.2     |\n",
      "|    critic_loss   | 2.96e+03 |\n",
      "|    ent_coef      | 0.926    |\n",
      "|    ent_coef_loss | -0.259   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 260      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2600, episode_reward=-102.67 +/- 66.37\n",
      "Episode length: 4.90 +/- 3.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610, episode_reward=-121.79 +/- 56.01\n",
      "Episode length: 4.80 +/- 5.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.5     |\n",
      "|    critic_loss     | 2.79e+03 |\n",
      "|    ent_coef        | 0.925    |\n",
      "|    ent_coef_loss   | -0.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 261      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 41.8     |\n",
      "|    critic_loss   | 3.53e+03 |\n",
      "|    ent_coef      | 0.925    |\n",
      "|    ent_coef_loss | -0.258   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 262      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2620, episode_reward=-108.17 +/- 85.18\n",
      "Episode length: 6.60 +/- 6.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 35.3     |\n",
      "|    critic_loss   | 1.64e+03 |\n",
      "|    ent_coef      | 0.925    |\n",
      "|    ent_coef_loss | -0.265   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 263      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2630, episode_reward=-99.70 +/- 60.19\n",
      "Episode length: 6.40 +/- 4.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -99.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640, episode_reward=-113.60 +/- 46.75\n",
      "Episode length: 4.70 +/- 3.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -114     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.9     |\n",
      "|    critic_loss     | 2.15e+03 |\n",
      "|    ent_coef        | 0.925    |\n",
      "|    ent_coef_loss   | -0.261   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 264      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33.5     |\n",
      "|    critic_loss   | 3.31e+03 |\n",
      "|    ent_coef      | 0.924    |\n",
      "|    ent_coef_loss | -0.263   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 265      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2650, episode_reward=-162.12 +/- 40.95\n",
      "Episode length: 2.30 +/- 2.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.3      |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660, episode_reward=-121.90 +/- 42.09\n",
      "Episode length: 5.10 +/- 3.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.1     |\n",
      "|    critic_loss     | 5.46e+03 |\n",
      "|    ent_coef        | 0.924    |\n",
      "|    ent_coef_loss   | -0.265   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 266      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670, episode_reward=-145.97 +/- 43.73\n",
      "Episode length: 3.30 +/- 2.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33.8     |\n",
      "|    critic_loss     | 2.51e+03 |\n",
      "|    ent_coef        | 0.924    |\n",
      "|    ent_coef_loss   | -0.261   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 267      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 42.1     |\n",
      "|    critic_loss   | 3.86e+03 |\n",
      "|    ent_coef      | 0.923    |\n",
      "|    ent_coef_loss | -0.258   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 268      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2680, episode_reward=-127.31 +/- 54.10\n",
      "Episode length: 6.50 +/- 5.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690, episode_reward=-124.20 +/- 43.81\n",
      "Episode length: 6.60 +/- 5.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 37.5     |\n",
      "|    critic_loss     | 2.69e+03 |\n",
      "|    ent_coef        | 0.923    |\n",
      "|    ent_coef_loss   | -0.261   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700, episode_reward=-153.59 +/- 36.60\n",
      "Episode length: 3.30 +/- 2.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.4     |\n",
      "|    critic_loss     | 2.94e+03 |\n",
      "|    ent_coef        | 0.923    |\n",
      "|    ent_coef_loss   | -0.257   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 270      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710, episode_reward=-139.16 +/- 34.87\n",
      "Episode length: 5.60 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 34       |\n",
      "|    critic_loss     | 3.22e+03 |\n",
      "|    ent_coef        | 0.923    |\n",
      "|    ent_coef_loss   | -0.256   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 271      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 21       |\n",
      "|    critic_loss   | 2.43e+03 |\n",
      "|    ent_coef      | 0.922    |\n",
      "|    ent_coef_loss | -0.265   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 272      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2720, episode_reward=-140.03 +/- 57.87\n",
      "Episode length: 3.90 +/- 4.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730, episode_reward=-127.56 +/- 46.76\n",
      "Episode length: 6.20 +/- 3.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.6     |\n",
      "|    critic_loss     | 2.46e+03 |\n",
      "|    ent_coef        | 0.922    |\n",
      "|    ent_coef_loss   | -0.259   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 273      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.6     |\n",
      "|    critic_loss   | 4.67e+03 |\n",
      "|    ent_coef      | 0.922    |\n",
      "|    ent_coef_loss | -0.255   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 274      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2740, episode_reward=-128.46 +/- 59.51\n",
      "Episode length: 4.10 +/- 4.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33       |\n",
      "|    critic_loss   | 5e+03    |\n",
      "|    ent_coef      | 0.921    |\n",
      "|    ent_coef_loss | -0.258   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 275      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2750, episode_reward=-107.69 +/- 51.31\n",
      "Episode length: 5.30 +/- 4.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760, episode_reward=-115.23 +/- 36.76\n",
      "Episode length: 3.40 +/- 2.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26       |\n",
      "|    critic_loss     | 1.99e+03 |\n",
      "|    ent_coef        | 0.921    |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 276      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 30       |\n",
      "|    critic_loss   | 2.14e+03 |\n",
      "|    ent_coef      | 0.921    |\n",
      "|    ent_coef_loss | -0.263   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 277      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2770, episode_reward=-162.15 +/- 39.00\n",
      "Episode length: 3.40 +/- 3.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780, episode_reward=-121.76 +/- 47.64\n",
      "Episode length: 4.00 +/- 3.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31       |\n",
      "|    critic_loss     | 3.24e+03 |\n",
      "|    ent_coef        | 0.921    |\n",
      "|    ent_coef_loss   | -0.263   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 278      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.8     |\n",
      "|    critic_loss   | 4.16e+03 |\n",
      "|    ent_coef      | 0.92     |\n",
      "|    ent_coef_loss | -0.273   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 279      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2790, episode_reward=-113.49 +/- 34.62\n",
      "Episode length: 5.00 +/- 3.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33.2     |\n",
      "|    critic_loss   | 2.7e+03  |\n",
      "|    ent_coef      | 0.92     |\n",
      "|    ent_coef_loss | -0.266   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 280      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2800, episode_reward=-114.71 +/- 31.18\n",
      "Episode length: 6.60 +/- 4.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810, episode_reward=-191.15 +/- 105.96\n",
      "Episode length: 3.10 +/- 3.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.6     |\n",
      "|    critic_loss     | 5.13e+03 |\n",
      "|    ent_coef        | 0.92     |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 281      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820, episode_reward=-102.61 +/- 70.66\n",
      "Episode length: 6.20 +/- 4.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.3     |\n",
      "|    critic_loss     | 2.43e+03 |\n",
      "|    ent_coef        | 0.92     |\n",
      "|    ent_coef_loss   | -0.266   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 282      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830, episode_reward=-118.68 +/- 56.35\n",
      "Episode length: 4.20 +/- 3.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.8     |\n",
      "|    critic_loss     | 4.06e+03 |\n",
      "|    ent_coef        | 0.919    |\n",
      "|    ent_coef_loss   | -0.263   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 283      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 44.3     |\n",
      "|    critic_loss   | 2.51e+03 |\n",
      "|    ent_coef      | 0.919    |\n",
      "|    ent_coef_loss | -0.281   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 284      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2840, episode_reward=-109.41 +/- 44.78\n",
      "Episode length: 6.60 +/- 5.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850, episode_reward=-149.37 +/- 44.41\n",
      "Episode length: 2.50 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.9     |\n",
      "|    critic_loss     | 2.92e+03 |\n",
      "|    ent_coef        | 0.919    |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 285      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.4     |\n",
      "|    critic_loss   | 2.62e+03 |\n",
      "|    ent_coef      | 0.919    |\n",
      "|    ent_coef_loss | -0.269   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 286      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2860, episode_reward=-106.80 +/- 55.30\n",
      "Episode length: 5.80 +/- 4.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870, episode_reward=-157.25 +/- 41.03\n",
      "Episode length: 2.80 +/- 3.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.8     |\n",
      "|    critic_loss     | 4.29e+03 |\n",
      "|    ent_coef        | 0.918    |\n",
      "|    ent_coef_loss   | -0.257   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 287      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880, episode_reward=-121.34 +/- 53.56\n",
      "Episode length: 5.10 +/- 4.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.5     |\n",
      "|    critic_loss     | 3.14e+03 |\n",
      "|    ent_coef        | 0.918    |\n",
      "|    ent_coef_loss   | -0.281   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 288      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34       |\n",
      "|    critic_loss   | 1.44e+03 |\n",
      "|    ent_coef      | 0.918    |\n",
      "|    ent_coef_loss | -0.263   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 289      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2890, episode_reward=-147.50 +/- 43.49\n",
      "Episode length: 4.30 +/- 2.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900, episode_reward=-154.03 +/- 35.42\n",
      "Episode length: 2.90 +/- 2.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.8     |\n",
      "|    critic_loss     | 2.01e+03 |\n",
      "|    ent_coef        | 0.917    |\n",
      "|    ent_coef_loss   | -0.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 290      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 28.2     |\n",
      "|    critic_loss   | 2.03e+03 |\n",
      "|    ent_coef      | 0.917    |\n",
      "|    ent_coef_loss | -0.262   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 291      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2910, episode_reward=-167.82 +/- 21.06\n",
      "Episode length: 3.80 +/- 2.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -168     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920, episode_reward=-131.41 +/- 44.40\n",
      "Episode length: 2.60 +/- 1.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.6      |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.6     |\n",
      "|    critic_loss     | 4.27e+03 |\n",
      "|    ent_coef        | 0.917    |\n",
      "|    ent_coef_loss   | -0.274   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 292      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930, episode_reward=-143.63 +/- 28.08\n",
      "Episode length: 6.00 +/- 3.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.3     |\n",
      "|    critic_loss     | 3.25e+03 |\n",
      "|    ent_coef        | 0.917    |\n",
      "|    ent_coef_loss   | -0.284   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 293      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940, episode_reward=-116.80 +/- 72.71\n",
      "Episode length: 5.60 +/- 4.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.1     |\n",
      "|    critic_loss     | 2.39e+03 |\n",
      "|    ent_coef        | 0.916    |\n",
      "|    ent_coef_loss   | -0.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 294      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 19.4     |\n",
      "|    critic_loss   | 3.07e+03 |\n",
      "|    ent_coef      | 0.916    |\n",
      "|    ent_coef_loss | -0.284   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 295      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2950, episode_reward=-147.53 +/- 93.30\n",
      "Episode length: 4.10 +/- 3.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960, episode_reward=-114.65 +/- 64.84\n",
      "Episode length: 7.00 +/- 5.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.8     |\n",
      "|    critic_loss     | 3.36e+03 |\n",
      "|    ent_coef        | 0.916    |\n",
      "|    ent_coef_loss   | -0.288   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 296      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.9     |\n",
      "|    critic_loss   | 1.6e+03  |\n",
      "|    ent_coef      | 0.916    |\n",
      "|    ent_coef_loss | -0.286   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 297      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=2970, episode_reward=-131.53 +/- 44.46\n",
      "Episode length: 5.70 +/- 5.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980, episode_reward=-114.26 +/- 46.61\n",
      "Episode length: 3.30 +/- 3.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -114     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.2     |\n",
      "|    critic_loss     | 4.84e+03 |\n",
      "|    ent_coef        | 0.915    |\n",
      "|    ent_coef_loss   | -0.299   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 298      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990, episode_reward=-151.67 +/- 98.95\n",
      "Episode length: 7.00 +/- 7.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.6     |\n",
      "|    critic_loss     | 3.68e+03 |\n",
      "|    ent_coef        | 0.915    |\n",
      "|    ent_coef_loss   | -0.294   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 31.7     |\n",
      "|    critic_loss   | 1.76e+03 |\n",
      "|    ent_coef      | 0.915    |\n",
      "|    ent_coef_loss | -0.288   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 300      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-104.65 +/- 59.14\n",
      "Episode length: 5.90 +/- 4.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010, episode_reward=-137.91 +/- 94.10\n",
      "Episode length: 5.20 +/- 4.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.4     |\n",
      "|    critic_loss     | 1.33e+03 |\n",
      "|    ent_coef        | 0.915    |\n",
      "|    ent_coef_loss   | -0.294   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 301      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3020, episode_reward=-104.35 +/- 59.55\n",
      "Episode length: 5.50 +/- 4.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33.1     |\n",
      "|    critic_loss     | 3.89e+03 |\n",
      "|    ent_coef        | 0.914    |\n",
      "|    ent_coef_loss   | -0.289   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 302      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.2     |\n",
      "|    critic_loss   | 4.04e+03 |\n",
      "|    ent_coef      | 0.914    |\n",
      "|    ent_coef_loss | -0.305   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 303      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3030, episode_reward=-115.97 +/- 41.32\n",
      "Episode length: 5.10 +/- 4.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040, episode_reward=-118.95 +/- 50.79\n",
      "Episode length: 4.00 +/- 4.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.7     |\n",
      "|    critic_loss     | 1.53e+03 |\n",
      "|    ent_coef        | 0.914    |\n",
      "|    ent_coef_loss   | -0.307   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 304      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050, episode_reward=-105.94 +/- 64.52\n",
      "Episode length: 6.70 +/- 4.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.2     |\n",
      "|    critic_loss     | 5.56e+03 |\n",
      "|    ent_coef        | 0.913    |\n",
      "|    ent_coef_loss   | -0.302   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 305      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060, episode_reward=-114.61 +/- 48.60\n",
      "Episode length: 6.80 +/- 5.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.5     |\n",
      "|    critic_loss     | 2.87e+03 |\n",
      "|    ent_coef        | 0.913    |\n",
      "|    ent_coef_loss   | -0.296   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 306      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37       |\n",
      "|    critic_loss   | 1.39e+03 |\n",
      "|    ent_coef      | 0.913    |\n",
      "|    ent_coef_loss | -0.304   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 307      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3070, episode_reward=-122.80 +/- 48.55\n",
      "Episode length: 3.60 +/- 3.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080, episode_reward=-126.44 +/- 61.18\n",
      "Episode length: 6.50 +/- 4.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.3     |\n",
      "|    critic_loss     | 2.32e+03 |\n",
      "|    ent_coef        | 0.913    |\n",
      "|    ent_coef_loss   | -0.299   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 308      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090, episode_reward=-144.67 +/- 50.98\n",
      "Episode length: 2.10 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.1      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.6     |\n",
      "|    critic_loss     | 2.27e+03 |\n",
      "|    ent_coef        | 0.912    |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34.3     |\n",
      "|    critic_loss   | 3.83e+03 |\n",
      "|    ent_coef      | 0.912    |\n",
      "|    ent_coef_loss | -0.296   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 310      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3100, episode_reward=-171.84 +/- 60.13\n",
      "Episode length: 2.50 +/- 2.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | -172     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110, episode_reward=-131.13 +/- 51.09\n",
      "Episode length: 4.00 +/- 3.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.4     |\n",
      "|    critic_loss     | 1.43e+03 |\n",
      "|    ent_coef        | 0.912    |\n",
      "|    ent_coef_loss   | -0.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 311      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 31.5     |\n",
      "|    critic_loss   | 1.93e+03 |\n",
      "|    ent_coef      | 0.912    |\n",
      "|    ent_coef_loss | -0.294   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 312      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3120, episode_reward=-152.36 +/- 54.17\n",
      "Episode length: 3.90 +/- 2.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130, episode_reward=-145.70 +/- 38.74\n",
      "Episode length: 4.20 +/- 3.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 37.8     |\n",
      "|    critic_loss     | 4.47e+03 |\n",
      "|    ent_coef        | 0.911    |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140, episode_reward=-108.76 +/- 59.09\n",
      "Episode length: 6.60 +/- 4.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42       |\n",
      "|    critic_loss     | 3.11e+03 |\n",
      "|    ent_coef        | 0.911    |\n",
      "|    ent_coef_loss   | -0.312   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 314      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 35.5     |\n",
      "|    critic_loss   | 3.38e+03 |\n",
      "|    ent_coef      | 0.911    |\n",
      "|    ent_coef_loss | -0.303   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 315      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3150, episode_reward=-147.23 +/- 41.00\n",
      "Episode length: 3.70 +/- 3.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 27.1     |\n",
      "|    critic_loss   | 1.84e+03 |\n",
      "|    ent_coef      | 0.911    |\n",
      "|    ent_coef_loss | -0.295   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 316      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3160, episode_reward=-145.67 +/- 76.83\n",
      "Episode length: 5.40 +/- 6.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170, episode_reward=-164.49 +/- 58.49\n",
      "Episode length: 4.20 +/- 4.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -164     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.7     |\n",
      "|    critic_loss     | 1.66e+03 |\n",
      "|    ent_coef        | 0.91     |\n",
      "|    ent_coef_loss   | -0.304   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 317      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 31.5     |\n",
      "|    critic_loss   | 2.07e+03 |\n",
      "|    ent_coef      | 0.91     |\n",
      "|    ent_coef_loss | -0.32    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 318      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3180, episode_reward=-118.37 +/- 72.43\n",
      "Episode length: 5.50 +/- 4.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190, episode_reward=-118.82 +/- 41.85\n",
      "Episode length: 6.10 +/- 5.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.5     |\n",
      "|    critic_loss     | 2.66e+03 |\n",
      "|    ent_coef        | 0.91     |\n",
      "|    ent_coef_loss   | -0.312   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 26.7     |\n",
      "|    critic_loss   | 1.15e+03 |\n",
      "|    ent_coef      | 0.909    |\n",
      "|    ent_coef_loss | -0.311   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 320      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3200, episode_reward=-145.47 +/- 54.67\n",
      "Episode length: 3.00 +/- 2.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210, episode_reward=-161.28 +/- 60.78\n",
      "Episode length: 2.40 +/- 2.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.4      |\n",
      "|    mean_reward     | -161     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.8     |\n",
      "|    critic_loss     | 2.46e+03 |\n",
      "|    ent_coef        | 0.909    |\n",
      "|    ent_coef_loss   | -0.313   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 321      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220, episode_reward=-125.69 +/- 62.05\n",
      "Episode length: 4.10 +/- 3.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.8     |\n",
      "|    critic_loss     | 1.97e+03 |\n",
      "|    ent_coef        | 0.909    |\n",
      "|    ent_coef_loss   | -0.311   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 322      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 25.5     |\n",
      "|    critic_loss   | 2.86e+03 |\n",
      "|    ent_coef      | 0.909    |\n",
      "|    ent_coef_loss | -0.315   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 323      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3230, episode_reward=-121.83 +/- 68.16\n",
      "Episode length: 3.90 +/- 4.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29.3     |\n",
      "|    critic_loss   | 2.35e+03 |\n",
      "|    ent_coef      | 0.908    |\n",
      "|    ent_coef_loss | -0.302   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 324      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3240, episode_reward=-135.09 +/- 55.05\n",
      "Episode length: 5.40 +/- 4.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250, episode_reward=-141.99 +/- 61.81\n",
      "Episode length: 3.80 +/- 3.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33       |\n",
      "|    critic_loss     | 2.14e+03 |\n",
      "|    ent_coef        | 0.908    |\n",
      "|    ent_coef_loss   | -0.308   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 325      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 36       |\n",
      "|    critic_loss   | 3.52e+03 |\n",
      "|    ent_coef      | 0.908    |\n",
      "|    ent_coef_loss | -0.307   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 326      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3260, episode_reward=-152.00 +/- 63.83\n",
      "Episode length: 4.60 +/- 4.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270, episode_reward=-112.11 +/- 69.42\n",
      "Episode length: 8.00 +/- 5.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35       |\n",
      "|    critic_loss     | 4.28e+03 |\n",
      "|    ent_coef        | 0.908    |\n",
      "|    ent_coef_loss   | -0.298   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 327      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280, episode_reward=-103.30 +/- 69.00\n",
      "Episode length: 6.00 +/- 5.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.8     |\n",
      "|    critic_loss     | 3.4e+03  |\n",
      "|    ent_coef        | 0.907    |\n",
      "|    ent_coef_loss   | -0.322   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 328      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34.7     |\n",
      "|    critic_loss   | 2.73e+03 |\n",
      "|    ent_coef      | 0.907    |\n",
      "|    ent_coef_loss | -0.311   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 329      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3290, episode_reward=-142.32 +/- 52.74\n",
      "Episode length: 4.80 +/- 5.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300, episode_reward=-153.13 +/- 46.43\n",
      "Episode length: 3.30 +/- 3.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.9     |\n",
      "|    critic_loss     | 2.99e+03 |\n",
      "|    ent_coef        | 0.907    |\n",
      "|    ent_coef_loss   | -0.318   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 330      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34       |\n",
      "|    critic_loss   | 3.59e+03 |\n",
      "|    ent_coef      | 0.906    |\n",
      "|    ent_coef_loss | -0.317   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 331      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3310, episode_reward=-118.73 +/- 112.54\n",
      "Episode length: 7.80 +/- 5.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320, episode_reward=-136.00 +/- 47.05\n",
      "Episode length: 3.10 +/- 2.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32       |\n",
      "|    critic_loss     | 4.29e+03 |\n",
      "|    ent_coef        | 0.906    |\n",
      "|    ent_coef_loss   | -0.324   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 332      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34.5     |\n",
      "|    critic_loss   | 3.26e+03 |\n",
      "|    ent_coef      | 0.906    |\n",
      "|    ent_coef_loss | -0.321   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 333      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3330, episode_reward=-120.45 +/- 45.09\n",
      "Episode length: 6.60 +/- 4.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340, episode_reward=-144.63 +/- 46.88\n",
      "Episode length: 3.20 +/- 2.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.2     |\n",
      "|    critic_loss     | 2.85e+03 |\n",
      "|    ent_coef        | 0.906    |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 334      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 44.2     |\n",
      "|    critic_loss   | 1.58e+03 |\n",
      "|    ent_coef      | 0.905    |\n",
      "|    ent_coef_loss | -0.323   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 335      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3350, episode_reward=-118.81 +/- 35.36\n",
      "Episode length: 7.30 +/- 3.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 31.3     |\n",
      "|    critic_loss   | 1.61e+03 |\n",
      "|    ent_coef      | 0.905    |\n",
      "|    ent_coef_loss | -0.308   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 336      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3360, episode_reward=-169.92 +/- 99.81\n",
      "Episode length: 5.70 +/- 5.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -170     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 31       |\n",
      "|    critic_loss   | 889      |\n",
      "|    ent_coef      | 0.905    |\n",
      "|    ent_coef_loss | -0.316   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 337      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3370, episode_reward=-143.55 +/- 47.31\n",
      "Episode length: 6.00 +/- 4.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380, episode_reward=-112.74 +/- 38.16\n",
      "Episode length: 4.80 +/- 4.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.4     |\n",
      "|    critic_loss     | 2.82e+03 |\n",
      "|    ent_coef        | 0.905    |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 338      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 35.3     |\n",
      "|    critic_loss   | 3.07e+03 |\n",
      "|    ent_coef      | 0.904    |\n",
      "|    ent_coef_loss | -0.317   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 339      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3390, episode_reward=-139.78 +/- 51.41\n",
      "Episode length: 3.00 +/- 2.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400, episode_reward=-134.50 +/- 35.51\n",
      "Episode length: 3.60 +/- 3.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.9     |\n",
      "|    critic_loss     | 3.4e+03  |\n",
      "|    ent_coef        | 0.904    |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 340      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.8     |\n",
      "|    critic_loss   | 2.4e+03  |\n",
      "|    ent_coef      | 0.904    |\n",
      "|    ent_coef_loss | -0.314   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 341      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3410, episode_reward=-152.43 +/- 36.36\n",
      "Episode length: 3.90 +/- 3.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420, episode_reward=-134.03 +/- 37.58\n",
      "Episode length: 4.80 +/- 3.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.8     |\n",
      "|    critic_loss     | 1.14e+03 |\n",
      "|    ent_coef        | 0.904    |\n",
      "|    ent_coef_loss   | -0.311   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 342      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430, episode_reward=-138.96 +/- 74.91\n",
      "Episode length: 5.40 +/- 5.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.4     |\n",
      "|    critic_loss     | 1.72e+03 |\n",
      "|    ent_coef        | 0.903    |\n",
      "|    ent_coef_loss   | -0.314   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 343      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440, episode_reward=-133.92 +/- 58.00\n",
      "Episode length: 4.50 +/- 3.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.4     |\n",
      "|    critic_loss     | 2.92e+03 |\n",
      "|    ent_coef        | 0.903    |\n",
      "|    ent_coef_loss   | -0.322   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 344      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33.2     |\n",
      "|    critic_loss   | 1.49e+03 |\n",
      "|    ent_coef      | 0.903    |\n",
      "|    ent_coef_loss | -0.324   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 345      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3450, episode_reward=-139.30 +/- 104.25\n",
      "Episode length: 5.20 +/- 4.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460, episode_reward=-137.20 +/- 38.22\n",
      "Episode length: 5.60 +/- 4.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32       |\n",
      "|    critic_loss     | 1.39e+03 |\n",
      "|    ent_coef        | 0.903    |\n",
      "|    ent_coef_loss   | -0.333   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 346      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470, episode_reward=-148.16 +/- 49.95\n",
      "Episode length: 4.40 +/- 3.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.5     |\n",
      "|    critic_loss     | 2.97e+03 |\n",
      "|    ent_coef        | 0.902    |\n",
      "|    ent_coef_loss   | -0.337   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 347      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480, episode_reward=-158.72 +/- 46.04\n",
      "Episode length: 4.20 +/- 3.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.9     |\n",
      "|    critic_loss     | 5.54e+03 |\n",
      "|    ent_coef        | 0.902    |\n",
      "|    ent_coef_loss   | -0.332   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 348      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 38.9     |\n",
      "|    critic_loss   | 993      |\n",
      "|    ent_coef      | 0.902    |\n",
      "|    ent_coef_loss | -0.325   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 349      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3490, episode_reward=-161.75 +/- 31.63\n",
      "Episode length: 4.50 +/- 3.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-101.46 +/- 64.15\n",
      "Episode length: 6.30 +/- 4.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.3     |\n",
      "|    critic_loss     | 1.85e+03 |\n",
      "|    ent_coef        | 0.901    |\n",
      "|    ent_coef_loss   | -0.341   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 350      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 38.3     |\n",
      "|    critic_loss   | 5.23e+03 |\n",
      "|    ent_coef      | 0.901    |\n",
      "|    ent_coef_loss | -0.337   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 351      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3510, episode_reward=-153.98 +/- 61.61\n",
      "Episode length: 3.60 +/- 2.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520, episode_reward=-125.23 +/- 45.36\n",
      "Episode length: 1.90 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.9      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.9     |\n",
      "|    critic_loss     | 3e+03    |\n",
      "|    ent_coef        | 0.901    |\n",
      "|    ent_coef_loss   | -0.341   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 352      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3530, episode_reward=-132.43 +/- 42.12\n",
      "Episode length: 5.80 +/- 2.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 51.3     |\n",
      "|    critic_loss     | 4.2e+03  |\n",
      "|    ent_coef        | 0.901    |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 353      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 48.8     |\n",
      "|    critic_loss   | 2.92e+03 |\n",
      "|    ent_coef      | 0.9      |\n",
      "|    ent_coef_loss | -0.334   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 354      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3540, episode_reward=-116.62 +/- 38.39\n",
      "Episode length: 4.90 +/- 4.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 52.2     |\n",
      "|    critic_loss   | 1.61e+03 |\n",
      "|    ent_coef      | 0.9      |\n",
      "|    ent_coef_loss | -0.328   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 355      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3550, episode_reward=-153.40 +/- 92.57\n",
      "Episode length: 3.40 +/- 4.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 42.5     |\n",
      "|    critic_loss   | 2.23e+03 |\n",
      "|    ent_coef      | 0.9      |\n",
      "|    ent_coef_loss | -0.32    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 356      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3560, episode_reward=-138.22 +/- 41.31\n",
      "Episode length: 4.70 +/- 4.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570, episode_reward=-124.90 +/- 57.61\n",
      "Episode length: 5.20 +/- 4.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.5     |\n",
      "|    critic_loss     | 3.75e+03 |\n",
      "|    ent_coef        | 0.9      |\n",
      "|    ent_coef_loss   | -0.329   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 357      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 42.3     |\n",
      "|    critic_loss   | 3.32e+03 |\n",
      "|    ent_coef      | 0.899    |\n",
      "|    ent_coef_loss | -0.318   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 358      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3580, episode_reward=-118.59 +/- 57.12\n",
      "Episode length: 4.30 +/- 4.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590, episode_reward=-170.78 +/- 33.95\n",
      "Episode length: 3.40 +/- 3.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -171     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.3     |\n",
      "|    critic_loss     | 2.89e+03 |\n",
      "|    ent_coef        | 0.899    |\n",
      "|    ent_coef_loss   | -0.313   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29       |\n",
      "|    critic_loss   | 3.22e+03 |\n",
      "|    ent_coef      | 0.899    |\n",
      "|    ent_coef_loss | -0.318   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 360      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3600, episode_reward=-132.36 +/- 49.67\n",
      "Episode length: 6.70 +/- 5.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.4     |\n",
      "|    critic_loss   | 3.18e+03 |\n",
      "|    ent_coef      | 0.899    |\n",
      "|    ent_coef_loss | -0.314   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 361      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3610, episode_reward=-175.97 +/- 78.96\n",
      "Episode length: 3.00 +/- 2.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | -176     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620, episode_reward=-136.64 +/- 46.81\n",
      "Episode length: 6.10 +/- 5.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26       |\n",
      "|    critic_loss     | 2.84e+03 |\n",
      "|    ent_coef        | 0.898    |\n",
      "|    ent_coef_loss   | -0.332   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 362      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 32.7     |\n",
      "|    critic_loss   | 4.6e+03  |\n",
      "|    ent_coef      | 0.898    |\n",
      "|    ent_coef_loss | -0.327   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 363      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3630, episode_reward=-131.83 +/- 74.97\n",
      "Episode length: 6.90 +/- 7.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.9      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640, episode_reward=-116.44 +/- 43.90\n",
      "Episode length: 6.70 +/- 4.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 30.4     |\n",
      "|    critic_loss     | 3.24e+03 |\n",
      "|    ent_coef        | 0.898    |\n",
      "|    ent_coef_loss   | -0.339   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 364      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 31.2     |\n",
      "|    critic_loss   | 3.14e+03 |\n",
      "|    ent_coef      | 0.898    |\n",
      "|    ent_coef_loss | -0.358   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 365      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3650, episode_reward=-131.40 +/- 73.00\n",
      "Episode length: 4.70 +/- 4.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660, episode_reward=-134.00 +/- 55.52\n",
      "Episode length: 4.30 +/- 5.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.4     |\n",
      "|    critic_loss     | 3.72e+03 |\n",
      "|    ent_coef        | 0.897    |\n",
      "|    ent_coef_loss   | -0.357   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 366      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 38.8     |\n",
      "|    critic_loss   | 4.13e+03 |\n",
      "|    ent_coef      | 0.897    |\n",
      "|    ent_coef_loss | -0.348   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 367      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3670, episode_reward=-134.03 +/- 53.48\n",
      "Episode length: 3.10 +/- 3.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 50.7     |\n",
      "|    critic_loss   | 1.9e+03  |\n",
      "|    ent_coef      | 0.897    |\n",
      "|    ent_coef_loss | -0.355   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 368      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3680, episode_reward=-154.62 +/- 92.21\n",
      "Episode length: 3.20 +/- 2.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690, episode_reward=-129.33 +/- 66.58\n",
      "Episode length: 4.80 +/- 4.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50.2     |\n",
      "|    critic_loss     | 2.16e+03 |\n",
      "|    ent_coef        | 0.897    |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 56.2     |\n",
      "|    critic_loss   | 3.2e+03  |\n",
      "|    ent_coef      | 0.896    |\n",
      "|    ent_coef_loss | -0.347   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 370      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3700, episode_reward=-144.71 +/- 62.05\n",
      "Episode length: 4.20 +/- 3.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 49.5     |\n",
      "|    critic_loss   | 3.22e+03 |\n",
      "|    ent_coef      | 0.896    |\n",
      "|    ent_coef_loss | -0.363   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 371      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3710, episode_reward=-107.27 +/- 51.17\n",
      "Episode length: 5.00 +/- 5.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 48.6     |\n",
      "|    critic_loss   | 2.14e+03 |\n",
      "|    ent_coef      | 0.896    |\n",
      "|    ent_coef_loss | -0.349   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 372      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3720, episode_reward=-174.48 +/- 84.90\n",
      "Episode length: 3.10 +/- 4.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -174     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730, episode_reward=-164.47 +/- 106.28\n",
      "Episode length: 4.00 +/- 4.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -164     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 51.8     |\n",
      "|    critic_loss     | 4.2e+03  |\n",
      "|    ent_coef        | 0.896    |\n",
      "|    ent_coef_loss   | -0.361   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 373      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740, episode_reward=-123.50 +/- 52.82\n",
      "Episode length: 5.60 +/- 3.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.4     |\n",
      "|    critic_loss     | 2.32e+03 |\n",
      "|    ent_coef        | 0.895    |\n",
      "|    ent_coef_loss   | -0.366   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 374      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750, episode_reward=-150.86 +/- 38.72\n",
      "Episode length: 3.40 +/- 3.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.1     |\n",
      "|    critic_loss     | 2.48e+03 |\n",
      "|    ent_coef        | 0.895    |\n",
      "|    ent_coef_loss   | -0.357   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 375      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33.8     |\n",
      "|    critic_loss   | 1.79e+03 |\n",
      "|    ent_coef      | 0.895    |\n",
      "|    ent_coef_loss | -0.362   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 376      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3760, episode_reward=-129.06 +/- 33.70\n",
      "Episode length: 5.70 +/- 3.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 30.9     |\n",
      "|    critic_loss   | 1.59e+03 |\n",
      "|    ent_coef      | 0.894    |\n",
      "|    ent_coef_loss | -0.366   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 377      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3770, episode_reward=-134.54 +/- 53.22\n",
      "Episode length: 3.20 +/- 4.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 35.3     |\n",
      "|    critic_loss   | 1.17e+03 |\n",
      "|    ent_coef      | 0.894    |\n",
      "|    ent_coef_loss | -0.364   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 378      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3780, episode_reward=-154.91 +/- 49.82\n",
      "Episode length: 3.30 +/- 2.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3790, episode_reward=-124.64 +/- 33.51\n",
      "Episode length: 5.70 +/- 4.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 30.7     |\n",
      "|    critic_loss     | 3.17e+03 |\n",
      "|    ent_coef        | 0.894    |\n",
      "|    ent_coef_loss   | -0.366   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 22.5     |\n",
      "|    critic_loss   | 3.15e+03 |\n",
      "|    ent_coef      | 0.894    |\n",
      "|    ent_coef_loss | -0.363   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 380      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3800, episode_reward=-155.34 +/- 33.34\n",
      "Episode length: 2.90 +/- 1.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 27.7     |\n",
      "|    critic_loss   | 1.89e+03 |\n",
      "|    ent_coef      | 0.893    |\n",
      "|    ent_coef_loss | -0.374   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 381      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3810, episode_reward=-132.20 +/- 50.77\n",
      "Episode length: 5.00 +/- 4.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820, episode_reward=-122.38 +/- 73.01\n",
      "Episode length: 5.70 +/- 6.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.7     |\n",
      "|    critic_loss     | 2.69e+03 |\n",
      "|    ent_coef        | 0.893    |\n",
      "|    ent_coef_loss   | -0.366   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 382      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830, episode_reward=-121.01 +/- 50.98\n",
      "Episode length: 4.60 +/- 4.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33.4     |\n",
      "|    critic_loss     | 2.77e+03 |\n",
      "|    ent_coef        | 0.893    |\n",
      "|    ent_coef_loss   | -0.369   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 383      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 24.3     |\n",
      "|    critic_loss   | 1.49e+03 |\n",
      "|    ent_coef      | 0.893    |\n",
      "|    ent_coef_loss | -0.364   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 384      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3840, episode_reward=-136.05 +/- 30.66\n",
      "Episode length: 6.50 +/- 3.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.2     |\n",
      "|    critic_loss   | 2.97e+03 |\n",
      "|    ent_coef      | 0.892    |\n",
      "|    ent_coef_loss | -0.367   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 385      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3850, episode_reward=-141.17 +/- 49.97\n",
      "Episode length: 4.70 +/- 5.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 33.6     |\n",
      "|    critic_loss   | 3.42e+03 |\n",
      "|    ent_coef      | 0.892    |\n",
      "|    ent_coef_loss | -0.387   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 386      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3860, episode_reward=-157.12 +/- 90.71\n",
      "Episode length: 2.80 +/- 3.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870, episode_reward=-141.04 +/- 46.70\n",
      "Episode length: 5.60 +/- 5.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.3     |\n",
      "|    critic_loss     | 3.76e+03 |\n",
      "|    ent_coef        | 0.892    |\n",
      "|    ent_coef_loss   | -0.381   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 387      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880, episode_reward=-129.83 +/- 47.87\n",
      "Episode length: 3.80 +/- 3.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.5     |\n",
      "|    critic_loss     | 1.46e+03 |\n",
      "|    ent_coef        | 0.892    |\n",
      "|    ent_coef_loss   | -0.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 388      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 46.1     |\n",
      "|    critic_loss   | 3.13e+03 |\n",
      "|    ent_coef      | 0.891    |\n",
      "|    ent_coef_loss | -0.38    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 389      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3890, episode_reward=-160.65 +/- 37.90\n",
      "Episode length: 3.70 +/- 3.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -161     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900, episode_reward=-125.53 +/- 68.09\n",
      "Episode length: 4.10 +/- 5.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.1     |\n",
      "|    critic_loss     | 1.02e+03 |\n",
      "|    ent_coef        | 0.891    |\n",
      "|    ent_coef_loss   | -0.378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 390      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910, episode_reward=-141.35 +/- 50.93\n",
      "Episode length: 2.30 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.3      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39       |\n",
      "|    critic_loss     | 3.02e+03 |\n",
      "|    ent_coef        | 0.891    |\n",
      "|    ent_coef_loss   | -0.389   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 391      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 47.3     |\n",
      "|    critic_loss   | 3.52e+03 |\n",
      "|    ent_coef      | 0.891    |\n",
      "|    ent_coef_loss | -0.395   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 392      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3920, episode_reward=-140.82 +/- 43.96\n",
      "Episode length: 3.50 +/- 3.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.6     |\n",
      "|    critic_loss   | 1.97e+03 |\n",
      "|    ent_coef      | 0.89     |\n",
      "|    ent_coef_loss | -0.384   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 393      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3930, episode_reward=-130.48 +/- 39.88\n",
      "Episode length: 5.70 +/- 4.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940, episode_reward=-122.27 +/- 50.43\n",
      "Episode length: 4.60 +/- 3.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.3     |\n",
      "|    critic_loss     | 3.49e+03 |\n",
      "|    ent_coef        | 0.89     |\n",
      "|    ent_coef_loss   | -0.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 394      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 56       |\n",
      "|    critic_loss   | 3.2e+03  |\n",
      "|    ent_coef      | 0.89     |\n",
      "|    ent_coef_loss | -0.39    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 395      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3950, episode_reward=-129.25 +/- 49.85\n",
      "Episode length: 2.60 +/- 2.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.6      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960, episode_reward=-130.51 +/- 62.27\n",
      "Episode length: 5.30 +/- 5.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 3.11e+03 |\n",
      "|    ent_coef        | 0.889    |\n",
      "|    ent_coef_loss   | -0.373   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 396      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970, episode_reward=-142.10 +/- 66.04\n",
      "Episode length: 5.80 +/- 5.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.4     |\n",
      "|    critic_loss     | 2.29e+03 |\n",
      "|    ent_coef        | 0.889    |\n",
      "|    ent_coef_loss   | -0.381   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 397      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 38.1     |\n",
      "|    critic_loss   | 3.08e+03 |\n",
      "|    ent_coef      | 0.889    |\n",
      "|    ent_coef_loss | -0.382   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 398      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=3980, episode_reward=-144.53 +/- 39.96\n",
      "Episode length: 5.30 +/- 4.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990, episode_reward=-134.33 +/- 45.83\n",
      "Episode length: 4.40 +/- 4.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.2     |\n",
      "|    critic_loss     | 2.84e+03 |\n",
      "|    ent_coef        | 0.889    |\n",
      "|    ent_coef_loss   | -0.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 41.9     |\n",
      "|    critic_loss   | 2.73e+03 |\n",
      "|    ent_coef      | 0.888    |\n",
      "|    ent_coef_loss | -0.377   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 400      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-144.03 +/- 39.93\n",
      "Episode length: 4.70 +/- 4.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 30       |\n",
      "|    critic_loss   | 1.87e+03 |\n",
      "|    ent_coef      | 0.888    |\n",
      "|    ent_coef_loss | -0.389   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 401      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4010, episode_reward=-141.47 +/- 36.81\n",
      "Episode length: 3.30 +/- 2.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 38.2     |\n",
      "|    critic_loss   | 2.49e+03 |\n",
      "|    ent_coef      | 0.888    |\n",
      "|    ent_coef_loss | -0.387   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 402      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4020, episode_reward=-153.81 +/- 36.30\n",
      "Episode length: 3.20 +/- 2.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030, episode_reward=-145.40 +/- 45.34\n",
      "Episode length: 2.90 +/- 2.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.5     |\n",
      "|    critic_loss     | 2.1e+03  |\n",
      "|    ent_coef        | 0.888    |\n",
      "|    ent_coef_loss   | -0.385   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 403      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 47.8     |\n",
      "|    critic_loss   | 1.22e+03 |\n",
      "|    ent_coef      | 0.887    |\n",
      "|    ent_coef_loss | -0.38    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 404      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4040, episode_reward=-105.56 +/- 41.76\n",
      "Episode length: 4.50 +/- 2.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050, episode_reward=-116.22 +/- 65.37\n",
      "Episode length: 4.50 +/- 4.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 34.3     |\n",
      "|    critic_loss     | 1.66e+03 |\n",
      "|    ent_coef        | 0.887    |\n",
      "|    ent_coef_loss   | -0.377   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 405      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060, episode_reward=-117.78 +/- 56.85\n",
      "Episode length: 6.50 +/- 6.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.5     |\n",
      "|    critic_loss     | 2.92e+03 |\n",
      "|    ent_coef        | 0.887    |\n",
      "|    ent_coef_loss   | -0.386   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 406      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070, episode_reward=-111.91 +/- 42.30\n",
      "Episode length: 8.90 +/- 5.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.9      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44       |\n",
      "|    critic_loss     | 4.05e+03 |\n",
      "|    ent_coef        | 0.887    |\n",
      "|    ent_coef_loss   | -0.393   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 407      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 44.9     |\n",
      "|    critic_loss   | 2.48e+03 |\n",
      "|    ent_coef      | 0.886    |\n",
      "|    ent_coef_loss | -0.394   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 408      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4080, episode_reward=-140.36 +/- 61.57\n",
      "Episode length: 7.00 +/- 7.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37       |\n",
      "|    critic_loss   | 2.84e+03 |\n",
      "|    ent_coef      | 0.886    |\n",
      "|    ent_coef_loss | -0.399   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 409      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4090, episode_reward=-133.68 +/- 44.19\n",
      "Episode length: 6.90 +/- 4.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.9      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 32.7     |\n",
      "|    critic_loss   | 2.42e+03 |\n",
      "|    ent_coef      | 0.886    |\n",
      "|    ent_coef_loss | -0.418   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 410      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4100, episode_reward=-123.45 +/- 61.25\n",
      "Episode length: 5.50 +/- 4.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110, episode_reward=-144.50 +/- 93.33\n",
      "Episode length: 4.40 +/- 4.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.5     |\n",
      "|    critic_loss     | 3.48e+03 |\n",
      "|    ent_coef        | 0.886    |\n",
      "|    ent_coef_loss   | -0.395   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 411      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120, episode_reward=-116.68 +/- 59.48\n",
      "Episode length: 5.50 +/- 5.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39       |\n",
      "|    critic_loss     | 2.56e+03 |\n",
      "|    ent_coef        | 0.885    |\n",
      "|    ent_coef_loss   | -0.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 412      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130, episode_reward=-114.82 +/- 37.88\n",
      "Episode length: 4.20 +/- 4.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.3     |\n",
      "|    critic_loss     | 2.85e+03 |\n",
      "|    ent_coef        | 0.885    |\n",
      "|    ent_coef_loss   | -0.404   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 413      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 29.5     |\n",
      "|    critic_loss   | 1.97e+03 |\n",
      "|    ent_coef      | 0.885    |\n",
      "|    ent_coef_loss | -0.398   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 414      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4140, episode_reward=-117.08 +/- 43.34\n",
      "Episode length: 6.20 +/- 4.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 44.7     |\n",
      "|    critic_loss   | 3.26e+03 |\n",
      "|    ent_coef      | 0.884    |\n",
      "|    ent_coef_loss | -0.404   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 415      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4150, episode_reward=-143.56 +/- 38.58\n",
      "Episode length: 4.30 +/- 3.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160, episode_reward=-121.45 +/- 33.05\n",
      "Episode length: 5.60 +/- 4.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.6     |\n",
      "|    critic_loss     | 1.93e+03 |\n",
      "|    ent_coef        | 0.884    |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 416      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.7     |\n",
      "|    critic_loss   | 3.66e+03 |\n",
      "|    ent_coef      | 0.884    |\n",
      "|    ent_coef_loss | -0.401   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 417      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4170, episode_reward=-96.79 +/- 57.96\n",
      "Episode length: 6.40 +/- 5.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -96.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180, episode_reward=-113.47 +/- 58.43\n",
      "Episode length: 4.60 +/- 5.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.2     |\n",
      "|    critic_loss     | 1.73e+03 |\n",
      "|    ent_coef        | 0.884    |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 418      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 35.7     |\n",
      "|    critic_loss   | 2.67e+03 |\n",
      "|    ent_coef      | 0.883    |\n",
      "|    ent_coef_loss | -0.415   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 419      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4190, episode_reward=-139.77 +/- 94.11\n",
      "Episode length: 6.00 +/- 4.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200, episode_reward=-135.78 +/- 51.49\n",
      "Episode length: 5.10 +/- 4.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.4     |\n",
      "|    critic_loss     | 4.36e+03 |\n",
      "|    ent_coef        | 0.883    |\n",
      "|    ent_coef_loss   | -0.412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 420      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210, episode_reward=-131.92 +/- 68.99\n",
      "Episode length: 4.50 +/- 4.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50.6     |\n",
      "|    critic_loss     | 3.86e+03 |\n",
      "|    ent_coef        | 0.883    |\n",
      "|    ent_coef_loss   | -0.402   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 421      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220, episode_reward=-142.71 +/- 101.44\n",
      "Episode length: 4.60 +/- 4.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.8     |\n",
      "|    critic_loss     | 3.42e+03 |\n",
      "|    ent_coef        | 0.883    |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 422      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 38       |\n",
      "|    critic_loss   | 3.83e+03 |\n",
      "|    ent_coef      | 0.882    |\n",
      "|    ent_coef_loss | -0.407   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 423      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4230, episode_reward=-140.70 +/- 94.54\n",
      "Episode length: 5.80 +/- 4.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240, episode_reward=-134.61 +/- 41.28\n",
      "Episode length: 3.10 +/- 3.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.8     |\n",
      "|    critic_loss     | 2.64e+03 |\n",
      "|    ent_coef        | 0.882    |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 424      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250, episode_reward=-138.40 +/- 45.50\n",
      "Episode length: 5.00 +/- 4.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.8     |\n",
      "|    critic_loss     | 1.98e+03 |\n",
      "|    ent_coef        | 0.882    |\n",
      "|    ent_coef_loss   | -0.406   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 425      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 41.9     |\n",
      "|    critic_loss   | 1.87e+03 |\n",
      "|    ent_coef      | 0.882    |\n",
      "|    ent_coef_loss | -0.406   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 426      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4260, episode_reward=-95.18 +/- 45.32\n",
      "Episode length: 6.50 +/- 5.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -95.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 45       |\n",
      "|    critic_loss   | 5.14e+03 |\n",
      "|    ent_coef      | 0.881    |\n",
      "|    ent_coef_loss | -0.403   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 427      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4270, episode_reward=-134.89 +/- 50.50\n",
      "Episode length: 4.10 +/- 3.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4280, episode_reward=-148.21 +/- 44.86\n",
      "Episode length: 4.00 +/- 4.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.2     |\n",
      "|    critic_loss     | 3.64e+03 |\n",
      "|    ent_coef        | 0.881    |\n",
      "|    ent_coef_loss   | -0.398   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 428      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 44       |\n",
      "|    critic_loss   | 1.9e+03  |\n",
      "|    ent_coef      | 0.881    |\n",
      "|    ent_coef_loss | -0.418   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 429      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4290, episode_reward=-129.48 +/- 53.36\n",
      "Episode length: 5.90 +/- 6.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300, episode_reward=-147.46 +/- 99.14\n",
      "Episode length: 4.00 +/- 5.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 51.3     |\n",
      "|    critic_loss     | 1.77e+03 |\n",
      "|    ent_coef        | 0.88     |\n",
      "|    ent_coef_loss   | -0.416   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 430      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 49.9     |\n",
      "|    critic_loss   | 2.57e+03 |\n",
      "|    ent_coef      | 0.88     |\n",
      "|    ent_coef_loss | -0.393   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 431      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4310, episode_reward=-118.36 +/- 42.97\n",
      "Episode length: 3.60 +/- 4.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320, episode_reward=-127.14 +/- 57.79\n",
      "Episode length: 3.90 +/- 6.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.8     |\n",
      "|    critic_loss     | 2.92e+03 |\n",
      "|    ent_coef        | 0.88     |\n",
      "|    ent_coef_loss   | -0.408   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 432      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330, episode_reward=-106.00 +/- 38.98\n",
      "Episode length: 8.80 +/- 5.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.2     |\n",
      "|    critic_loss     | 4.39e+03 |\n",
      "|    ent_coef        | 0.88     |\n",
      "|    ent_coef_loss   | -0.415   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 433      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 53.3     |\n",
      "|    critic_loss   | 2.58e+03 |\n",
      "|    ent_coef      | 0.879    |\n",
      "|    ent_coef_loss | -0.42    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 434      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4340, episode_reward=-139.07 +/- 49.70\n",
      "Episode length: 2.60 +/- 3.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.6      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350, episode_reward=-141.96 +/- 46.43\n",
      "Episode length: 4.40 +/- 4.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.5     |\n",
      "|    critic_loss     | 1.92e+03 |\n",
      "|    ent_coef        | 0.879    |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 435      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 39.9     |\n",
      "|    critic_loss   | 2.25e+03 |\n",
      "|    ent_coef      | 0.879    |\n",
      "|    ent_coef_loss | -0.428   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 436      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4360, episode_reward=-151.09 +/- 90.12\n",
      "Episode length: 3.60 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370, episode_reward=-119.80 +/- 34.56\n",
      "Episode length: 6.10 +/- 4.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.9     |\n",
      "|    critic_loss     | 2.32e+03 |\n",
      "|    ent_coef        | 0.879    |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 437      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380, episode_reward=-134.78 +/- 47.53\n",
      "Episode length: 5.40 +/- 4.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 53.1     |\n",
      "|    critic_loss     | 3.01e+03 |\n",
      "|    ent_coef        | 0.878    |\n",
      "|    ent_coef_loss   | -0.412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 438      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390, episode_reward=-118.47 +/- 64.15\n",
      "Episode length: 9.30 +/- 7.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.3      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.4     |\n",
      "|    critic_loss     | 1.44e+03 |\n",
      "|    ent_coef        | 0.878    |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400, episode_reward=-121.48 +/- 71.91\n",
      "Episode length: 5.60 +/- 5.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.1     |\n",
      "|    critic_loss     | 1.96e+03 |\n",
      "|    ent_coef        | 0.878    |\n",
      "|    ent_coef_loss   | -0.435   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 440      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410, episode_reward=-129.66 +/- 70.42\n",
      "Episode length: 5.10 +/- 5.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.9     |\n",
      "|    critic_loss     | 3.1e+03  |\n",
      "|    ent_coef        | 0.878    |\n",
      "|    ent_coef_loss   | -0.427   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 441      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 47.3     |\n",
      "|    critic_loss   | 2.72e+03 |\n",
      "|    ent_coef      | 0.877    |\n",
      "|    ent_coef_loss | -0.421   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 442      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4420, episode_reward=-140.69 +/- 53.16\n",
      "Episode length: 4.90 +/- 5.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 40.3     |\n",
      "|    critic_loss   | 2.67e+03 |\n",
      "|    ent_coef      | 0.877    |\n",
      "|    ent_coef_loss | -0.419   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 443      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4430, episode_reward=-129.77 +/- 62.16\n",
      "Episode length: 3.80 +/- 3.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 43.3     |\n",
      "|    critic_loss   | 3.19e+03 |\n",
      "|    ent_coef      | 0.877    |\n",
      "|    ent_coef_loss | -0.404   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 444      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4440, episode_reward=-144.79 +/- 56.32\n",
      "Episode length: 5.60 +/- 5.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450, episode_reward=-113.43 +/- 73.22\n",
      "Episode length: 5.60 +/- 5.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.9     |\n",
      "|    critic_loss     | 2.02e+03 |\n",
      "|    ent_coef        | 0.877    |\n",
      "|    ent_coef_loss   | -0.397   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 445      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 43.9     |\n",
      "|    critic_loss   | 1.66e+03 |\n",
      "|    ent_coef      | 0.876    |\n",
      "|    ent_coef_loss | -0.423   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 446      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4460, episode_reward=-181.94 +/- 117.67\n",
      "Episode length: 3.70 +/- 5.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -182     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470, episode_reward=-130.20 +/- 76.49\n",
      "Episode length: 5.10 +/- 6.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.3     |\n",
      "|    critic_loss     | 3.09e+03 |\n",
      "|    ent_coef        | 0.876    |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 447      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4480, episode_reward=-160.12 +/- 65.74\n",
      "Episode length: 3.80 +/- 3.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33.9     |\n",
      "|    critic_loss     | 4.44e+03 |\n",
      "|    ent_coef        | 0.876    |\n",
      "|    ent_coef_loss   | -0.434   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 448      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 43.2     |\n",
      "|    critic_loss   | 2.58e+03 |\n",
      "|    ent_coef      | 0.876    |\n",
      "|    ent_coef_loss | -0.392   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 449      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4490, episode_reward=-116.74 +/- 46.65\n",
      "Episode length: 6.50 +/- 5.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-114.70 +/- 46.29\n",
      "Episode length: 3.30 +/- 4.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.8     |\n",
      "|    critic_loss     | 2.17e+03 |\n",
      "|    ent_coef        | 0.875    |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 450      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 42.4     |\n",
      "|    critic_loss   | 1.39e+03 |\n",
      "|    ent_coef      | 0.875    |\n",
      "|    ent_coef_loss | -0.419   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 451      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4510, episode_reward=-151.31 +/- 36.08\n",
      "Episode length: 4.60 +/- 3.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520, episode_reward=-132.56 +/- 41.66\n",
      "Episode length: 4.40 +/- 4.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50.5     |\n",
      "|    critic_loss     | 2.65e+03 |\n",
      "|    ent_coef        | 0.875    |\n",
      "|    ent_coef_loss   | -0.412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 452      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 51.9     |\n",
      "|    critic_loss   | 3.1e+03  |\n",
      "|    ent_coef      | 0.875    |\n",
      "|    ent_coef_loss | -0.411   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 453      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4530, episode_reward=-121.55 +/- 43.30\n",
      "Episode length: 6.10 +/- 3.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 38.7     |\n",
      "|    critic_loss   | 1.8e+03  |\n",
      "|    ent_coef      | 0.874    |\n",
      "|    ent_coef_loss | -0.444   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 454      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4540, episode_reward=-171.48 +/- 84.36\n",
      "Episode length: 3.40 +/- 3.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -171     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550, episode_reward=-130.33 +/- 72.15\n",
      "Episode length: 5.90 +/- 5.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.2     |\n",
      "|    critic_loss     | 2.19e+03 |\n",
      "|    ent_coef        | 0.874    |\n",
      "|    ent_coef_loss   | -0.426   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 455      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 40.1     |\n",
      "|    critic_loss   | 3.61e+03 |\n",
      "|    ent_coef      | 0.874    |\n",
      "|    ent_coef_loss | -0.429   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 456      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4560, episode_reward=-145.65 +/- 36.87\n",
      "Episode length: 4.30 +/- 3.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 42       |\n",
      "|    critic_loss   | 1.52e+03 |\n",
      "|    ent_coef      | 0.874    |\n",
      "|    ent_coef_loss | -0.423   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 457      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4570, episode_reward=-129.71 +/- 46.39\n",
      "Episode length: 3.80 +/- 4.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580, episode_reward=-138.63 +/- 52.82\n",
      "Episode length: 4.80 +/- 5.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.5     |\n",
      "|    critic_loss     | 2.54e+03 |\n",
      "|    ent_coef        | 0.873    |\n",
      "|    ent_coef_loss   | -0.425   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 458      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 41.2     |\n",
      "|    critic_loss   | 1.48e+03 |\n",
      "|    ent_coef      | 0.873    |\n",
      "|    ent_coef_loss | -0.437   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 459      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4590, episode_reward=-129.37 +/- 37.47\n",
      "Episode length: 5.90 +/- 4.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.4     |\n",
      "|    critic_loss   | 1.54e+03 |\n",
      "|    ent_coef      | 0.873    |\n",
      "|    ent_coef_loss | -0.445   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 460      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4600, episode_reward=-130.20 +/- 66.38\n",
      "Episode length: 6.30 +/- 6.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.7     |\n",
      "|    critic_loss   | 3.23e+03 |\n",
      "|    ent_coef      | 0.873    |\n",
      "|    ent_coef_loss | -0.433   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 461      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4610, episode_reward=-121.60 +/- 41.28\n",
      "Episode length: 5.90 +/- 4.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620, episode_reward=-141.12 +/- 44.00\n",
      "Episode length: 4.50 +/- 4.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.7     |\n",
      "|    critic_loss     | 3.07e+03 |\n",
      "|    ent_coef        | 0.872    |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 462      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.6     |\n",
      "|    critic_loss   | 2.35e+03 |\n",
      "|    ent_coef      | 0.872    |\n",
      "|    ent_coef_loss | -0.439   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 463      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4630, episode_reward=-133.94 +/- 47.69\n",
      "Episode length: 3.30 +/- 3.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 34.4     |\n",
      "|    critic_loss   | 1.98e+03 |\n",
      "|    ent_coef      | 0.872    |\n",
      "|    ent_coef_loss | -0.45    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 464      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4640, episode_reward=-121.06 +/- 71.48\n",
      "Episode length: 4.70 +/- 5.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 35.5     |\n",
      "|    critic_loss   | 2.54e+03 |\n",
      "|    ent_coef      | 0.872    |\n",
      "|    ent_coef_loss | -0.456   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 465      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4650, episode_reward=-125.02 +/- 85.57\n",
      "Episode length: 6.40 +/- 7.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660, episode_reward=-115.46 +/- 58.06\n",
      "Episode length: 5.20 +/- 5.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32       |\n",
      "|    critic_loss     | 4e+03    |\n",
      "|    ent_coef        | 0.871    |\n",
      "|    ent_coef_loss   | -0.455   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 466      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670, episode_reward=-151.56 +/- 58.35\n",
      "Episode length: 3.50 +/- 3.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50.1     |\n",
      "|    critic_loss     | 2.9e+03  |\n",
      "|    ent_coef        | 0.871    |\n",
      "|    ent_coef_loss   | -0.438   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 467      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680, episode_reward=-116.54 +/- 41.83\n",
      "Episode length: 5.40 +/- 5.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.1     |\n",
      "|    critic_loss     | 2.88e+03 |\n",
      "|    ent_coef        | 0.871    |\n",
      "|    ent_coef_loss   | -0.425   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 468      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 46.8     |\n",
      "|    critic_loss   | 1.57e+03 |\n",
      "|    ent_coef      | 0.87     |\n",
      "|    ent_coef_loss | -0.455   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 469      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4690, episode_reward=-137.65 +/- 27.53\n",
      "Episode length: 5.00 +/- 2.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700, episode_reward=-123.29 +/- 58.83\n",
      "Episode length: 5.60 +/- 5.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 53.5     |\n",
      "|    critic_loss     | 1.62e+03 |\n",
      "|    ent_coef        | 0.87     |\n",
      "|    ent_coef_loss   | -0.443   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 470      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710, episode_reward=-145.61 +/- 43.60\n",
      "Episode length: 2.80 +/- 3.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.8     |\n",
      "|    critic_loss     | 3.73e+03 |\n",
      "|    ent_coef        | 0.87     |\n",
      "|    ent_coef_loss   | -0.447   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 471      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 53.3     |\n",
      "|    critic_loss   | 1.17e+03 |\n",
      "|    ent_coef      | 0.87     |\n",
      "|    ent_coef_loss | -0.442   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 472      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4720, episode_reward=-92.96 +/- 38.31\n",
      "Episode length: 7.80 +/- 6.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -93      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 50.9     |\n",
      "|    critic_loss   | 1.97e+03 |\n",
      "|    ent_coef      | 0.869    |\n",
      "|    ent_coef_loss | -0.446   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 473      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4730, episode_reward=-107.54 +/- 50.64\n",
      "Episode length: 5.20 +/- 4.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 45.3     |\n",
      "|    critic_loss   | 3.91e+03 |\n",
      "|    ent_coef      | 0.869    |\n",
      "|    ent_coef_loss | -0.444   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 474      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4740, episode_reward=-134.67 +/- 101.96\n",
      "Episode length: 4.60 +/- 4.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 45.2     |\n",
      "|    critic_loss   | 1.68e+03 |\n",
      "|    ent_coef      | 0.869    |\n",
      "|    ent_coef_loss | -0.454   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 475      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4750, episode_reward=-127.48 +/- 48.26\n",
      "Episode length: 3.90 +/- 5.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760, episode_reward=-154.82 +/- 47.68\n",
      "Episode length: 4.40 +/- 5.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.6     |\n",
      "|    critic_loss     | 3.03e+03 |\n",
      "|    ent_coef        | 0.869    |\n",
      "|    ent_coef_loss   | -0.445   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 476      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 43.5     |\n",
      "|    critic_loss   | 1.94e+03 |\n",
      "|    ent_coef      | 0.868    |\n",
      "|    ent_coef_loss | -0.44    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 477      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4770, episode_reward=-75.42 +/- 56.57\n",
      "Episode length: 10.90 +/- 6.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.9     |\n",
      "|    mean_reward     | -75.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 49       |\n",
      "|    critic_loss   | 3.13e+03 |\n",
      "|    ent_coef      | 0.868    |\n",
      "|    ent_coef_loss | -0.432   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 478      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4780, episode_reward=-125.30 +/- 35.47\n",
      "Episode length: 3.00 +/- 2.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790, episode_reward=-118.22 +/- 45.81\n",
      "Episode length: 2.60 +/- 2.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.6      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.4     |\n",
      "|    critic_loss     | 1.91e+03 |\n",
      "|    ent_coef        | 0.868    |\n",
      "|    ent_coef_loss   | -0.456   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800, episode_reward=-101.92 +/- 62.47\n",
      "Episode length: 9.00 +/- 6.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.8     |\n",
      "|    critic_loss     | 2.06e+03 |\n",
      "|    ent_coef        | 0.868    |\n",
      "|    ent_coef_loss   | -0.458   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 480      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810, episode_reward=-135.01 +/- 44.81\n",
      "Episode length: 5.30 +/- 5.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 54.8     |\n",
      "|    critic_loss     | 1.5e+03  |\n",
      "|    ent_coef        | 0.867    |\n",
      "|    ent_coef_loss   | -0.479   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 481      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820, episode_reward=-155.96 +/- 73.77\n",
      "Episode length: 4.10 +/- 5.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43       |\n",
      "|    critic_loss     | 818      |\n",
      "|    ent_coef        | 0.867    |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 482      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.3     |\n",
      "|    critic_loss   | 2.31e+03 |\n",
      "|    ent_coef      | 0.867    |\n",
      "|    ent_coef_loss | -0.447   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 483      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4830, episode_reward=-169.01 +/- 86.23\n",
      "Episode length: 2.50 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840, episode_reward=-134.30 +/- 48.16\n",
      "Episode length: 5.30 +/- 5.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 52.8     |\n",
      "|    critic_loss     | 2.21e+03 |\n",
      "|    ent_coef        | 0.867    |\n",
      "|    ent_coef_loss   | -0.457   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 484      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850, episode_reward=-114.69 +/- 54.01\n",
      "Episode length: 6.50 +/- 4.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.6     |\n",
      "|    critic_loss     | 1.43e+03 |\n",
      "|    ent_coef        | 0.866    |\n",
      "|    ent_coef_loss   | -0.466   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 485      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.6     |\n",
      "|    critic_loss   | 3.75e+03 |\n",
      "|    ent_coef      | 0.866    |\n",
      "|    ent_coef_loss | -0.474   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 486      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4860, episode_reward=-116.78 +/- 39.56\n",
      "Episode length: 5.30 +/- 4.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870, episode_reward=-110.53 +/- 50.32\n",
      "Episode length: 6.30 +/- 5.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.5     |\n",
      "|    critic_loss     | 2.51e+03 |\n",
      "|    ent_coef        | 0.866    |\n",
      "|    ent_coef_loss   | -0.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 487      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4880, episode_reward=-144.38 +/- 44.79\n",
      "Episode length: 3.10 +/- 3.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.3     |\n",
      "|    critic_loss     | 1.36e+03 |\n",
      "|    ent_coef        | 0.866    |\n",
      "|    ent_coef_loss   | -0.458   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 488      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 45.7     |\n",
      "|    critic_loss   | 3.52e+03 |\n",
      "|    ent_coef      | 0.865    |\n",
      "|    ent_coef_loss | -0.481   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 489      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4890, episode_reward=-106.20 +/- 55.08\n",
      "Episode length: 9.10 +/- 7.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.1      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 36       |\n",
      "|    critic_loss   | 2.27e+03 |\n",
      "|    ent_coef      | 0.865    |\n",
      "|    ent_coef_loss | -0.474   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 490      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4900, episode_reward=-124.76 +/- 43.82\n",
      "Episode length: 3.20 +/- 2.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910, episode_reward=-144.56 +/- 93.79\n",
      "Episode length: 6.70 +/- 6.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.5     |\n",
      "|    critic_loss     | 3.68e+03 |\n",
      "|    ent_coef        | 0.865    |\n",
      "|    ent_coef_loss   | -0.478   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 491      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 46.5     |\n",
      "|    critic_loss   | 2.45e+03 |\n",
      "|    ent_coef      | 0.865    |\n",
      "|    ent_coef_loss | -0.481   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 492      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4920, episode_reward=-105.92 +/- 49.75\n",
      "Episode length: 9.80 +/- 4.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930, episode_reward=-147.30 +/- 49.66\n",
      "Episode length: 4.00 +/- 5.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 52       |\n",
      "|    critic_loss     | 1.16e+03 |\n",
      "|    ent_coef        | 0.864    |\n",
      "|    ent_coef_loss   | -0.466   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 493      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940, episode_reward=-120.37 +/- 53.88\n",
      "Episode length: 4.80 +/- 5.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 52.6     |\n",
      "|    critic_loss     | 2.29e+03 |\n",
      "|    ent_coef        | 0.864    |\n",
      "|    ent_coef_loss   | -0.465   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 494      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 52.2     |\n",
      "|    critic_loss   | 2.52e+03 |\n",
      "|    ent_coef      | 0.864    |\n",
      "|    ent_coef_loss | -0.483   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 495      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4950, episode_reward=-124.12 +/- 30.19\n",
      "Episode length: 4.40 +/- 3.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 50.9     |\n",
      "|    critic_loss   | 3.31e+03 |\n",
      "|    ent_coef      | 0.864    |\n",
      "|    ent_coef_loss | -0.456   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 496      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4960, episode_reward=-124.18 +/- 46.23\n",
      "Episode length: 5.40 +/- 5.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970, episode_reward=-123.47 +/- 48.25\n",
      "Episode length: 3.70 +/- 3.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.7     |\n",
      "|    critic_loss     | 2.47e+03 |\n",
      "|    ent_coef        | 0.863    |\n",
      "|    ent_coef_loss   | -0.503   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 497      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 40.6     |\n",
      "|    critic_loss   | 1.9e+03  |\n",
      "|    ent_coef      | 0.863    |\n",
      "|    ent_coef_loss | -0.463   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 498      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=4980, episode_reward=-82.39 +/- 64.15\n",
      "Episode length: 9.50 +/- 7.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | -82.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4990, episode_reward=-127.30 +/- 105.93\n",
      "Episode length: 10.00 +/- 8.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.8     |\n",
      "|    critic_loss     | 1.23e+03 |\n",
      "|    ent_coef        | 0.863    |\n",
      "|    ent_coef_loss   | -0.447   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-119.50 +/- 48.95\n",
      "Episode length: 7.30 +/- 5.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.4     |\n",
      "|    critic_loss     | 935      |\n",
      "|    ent_coef        | 0.863    |\n",
      "|    ent_coef_loss   | -0.451   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 500      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 36.2     |\n",
      "|    critic_loss   | 1.89e+03 |\n",
      "|    ent_coef      | 0.862    |\n",
      "|    ent_coef_loss | -0.463   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 501      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5010, episode_reward=-129.96 +/- 37.34\n",
      "Episode length: 3.90 +/- 2.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5010     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 46.5     |\n",
      "|    critic_loss   | 3.05e+03 |\n",
      "|    ent_coef      | 0.862    |\n",
      "|    ent_coef_loss | -0.44    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 502      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5020, episode_reward=-146.78 +/- 83.60\n",
      "Episode length: 6.30 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5020     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 44.6     |\n",
      "|    critic_loss   | 2.46e+03 |\n",
      "|    ent_coef      | 0.862    |\n",
      "|    ent_coef_loss | -0.478   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 503      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5030, episode_reward=-129.06 +/- 40.10\n",
      "Episode length: 6.10 +/- 4.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.1      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5040, episode_reward=-176.52 +/- 30.78\n",
      "Episode length: 1.70 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.7      |\n",
      "|    mean_reward     | -177     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.5     |\n",
      "|    critic_loss     | 1.98e+03 |\n",
      "|    ent_coef        | 0.862    |\n",
      "|    ent_coef_loss   | -0.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 504      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5050, episode_reward=-120.47 +/- 41.93\n",
      "Episode length: 6.00 +/- 5.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 57.7     |\n",
      "|    critic_loss     | 2.39e+03 |\n",
      "|    ent_coef        | 0.861    |\n",
      "|    ent_coef_loss   | -0.461   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 505      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 53.4     |\n",
      "|    critic_loss   | 3.94e+03 |\n",
      "|    ent_coef      | 0.861    |\n",
      "|    ent_coef_loss | -0.46    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 506      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5060, episode_reward=-119.64 +/- 54.48\n",
      "Episode length: 6.20 +/- 5.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5070, episode_reward=-120.16 +/- 43.05\n",
      "Episode length: 6.20 +/- 5.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 57.4     |\n",
      "|    critic_loss     | 1.08e+03 |\n",
      "|    ent_coef        | 0.861    |\n",
      "|    ent_coef_loss   | -0.484   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 507      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 47.2     |\n",
      "|    critic_loss   | 2.34e+03 |\n",
      "|    ent_coef      | 0.861    |\n",
      "|    ent_coef_loss | -0.469   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 508      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5080, episode_reward=-148.59 +/- 42.60\n",
      "Episode length: 4.90 +/- 4.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5080     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 53.9     |\n",
      "|    critic_loss   | 1.97e+03 |\n",
      "|    ent_coef      | 0.86     |\n",
      "|    ent_coef_loss | -0.463   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 509      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5090, episode_reward=-128.98 +/- 46.06\n",
      "Episode length: 6.20 +/- 5.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5100, episode_reward=-124.60 +/- 70.08\n",
      "Episode length: 6.50 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.9     |\n",
      "|    critic_loss     | 2.55e+03 |\n",
      "|    ent_coef        | 0.86     |\n",
      "|    ent_coef_loss   | -0.484   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 510      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 53.9     |\n",
      "|    critic_loss   | 3.48e+03 |\n",
      "|    ent_coef      | 0.86     |\n",
      "|    ent_coef_loss | -0.483   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 511      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5110, episode_reward=-125.59 +/- 73.89\n",
      "Episode length: 7.70 +/- 5.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.7      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5120, episode_reward=-137.05 +/- 51.31\n",
      "Episode length: 5.00 +/- 5.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.9     |\n",
      "|    critic_loss     | 3.03e+03 |\n",
      "|    ent_coef        | 0.86     |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 512      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5130, episode_reward=-99.74 +/- 36.28\n",
      "Episode length: 7.30 +/- 6.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | -99.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 51       |\n",
      "|    critic_loss     | 3.15e+03 |\n",
      "|    ent_coef        | 0.859    |\n",
      "|    ent_coef_loss   | -0.471   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 513      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5140, episode_reward=-161.92 +/- 32.78\n",
      "Episode length: 3.20 +/- 2.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.8     |\n",
      "|    critic_loss     | 1.78e+03 |\n",
      "|    ent_coef        | 0.859    |\n",
      "|    ent_coef_loss   | -0.486   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 514      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 53.5     |\n",
      "|    critic_loss   | 1.79e+03 |\n",
      "|    ent_coef      | 0.859    |\n",
      "|    ent_coef_loss | -0.488   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 515      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5150, episode_reward=-151.75 +/- 51.95\n",
      "Episode length: 4.40 +/- 5.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5150     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 43.1     |\n",
      "|    critic_loss   | 1.34e+03 |\n",
      "|    ent_coef      | 0.859    |\n",
      "|    ent_coef_loss | -0.491   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 516      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5160, episode_reward=-87.87 +/- 90.02\n",
      "Episode length: 8.40 +/- 7.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | -87.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5170, episode_reward=-132.48 +/- 40.96\n",
      "Episode length: 3.30 +/- 2.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.4     |\n",
      "|    critic_loss     | 1.99e+03 |\n",
      "|    ent_coef        | 0.858    |\n",
      "|    ent_coef_loss   | -0.503   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 517      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5180, episode_reward=-134.73 +/- 42.10\n",
      "Episode length: 6.30 +/- 4.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.3      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.2     |\n",
      "|    critic_loss     | 2.75e+03 |\n",
      "|    ent_coef        | 0.858    |\n",
      "|    ent_coef_loss   | -0.481   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 518      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 43.6     |\n",
      "|    critic_loss   | 1.49e+03 |\n",
      "|    ent_coef      | 0.858    |\n",
      "|    ent_coef_loss | -0.488   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 519      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5190, episode_reward=-148.05 +/- 36.60\n",
      "Episode length: 3.70 +/- 2.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5200, episode_reward=-118.19 +/- 55.40\n",
      "Episode length: 3.90 +/- 3.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 34.3     |\n",
      "|    critic_loss     | 1.4e+03  |\n",
      "|    ent_coef        | 0.858    |\n",
      "|    ent_coef_loss   | -0.479   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 520      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 56.7     |\n",
      "|    critic_loss   | 2.91e+03 |\n",
      "|    ent_coef      | 0.857    |\n",
      "|    ent_coef_loss | -0.474   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 521      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5210, episode_reward=-157.54 +/- 103.20\n",
      "Episode length: 5.20 +/- 6.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5220, episode_reward=-93.80 +/- 65.36\n",
      "Episode length: 6.70 +/- 7.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -93.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.2     |\n",
      "|    critic_loss     | 1.35e+03 |\n",
      "|    ent_coef        | 0.857    |\n",
      "|    ent_coef_loss   | -0.469   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 522      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 52       |\n",
      "|    critic_loss   | 2.48e+03 |\n",
      "|    ent_coef      | 0.857    |\n",
      "|    ent_coef_loss | -0.48    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 523      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5230, episode_reward=-124.46 +/- 44.96\n",
      "Episode length: 6.80 +/- 3.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5240, episode_reward=-135.21 +/- 42.09\n",
      "Episode length: 4.40 +/- 4.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50.6     |\n",
      "|    critic_loss     | 3.64e+03 |\n",
      "|    ent_coef        | 0.857    |\n",
      "|    ent_coef_loss   | -0.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 524      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 47.7     |\n",
      "|    critic_loss   | 2.11e+03 |\n",
      "|    ent_coef      | 0.856    |\n",
      "|    ent_coef_loss | -0.483   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 525      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5250, episode_reward=-145.78 +/- 54.59\n",
      "Episode length: 4.30 +/- 4.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5260, episode_reward=-121.42 +/- 65.79\n",
      "Episode length: 3.90 +/- 4.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50.8     |\n",
      "|    critic_loss     | 2.25e+03 |\n",
      "|    ent_coef        | 0.856    |\n",
      "|    ent_coef_loss   | -0.493   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 526      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 48.2     |\n",
      "|    critic_loss   | 1.76e+03 |\n",
      "|    ent_coef      | 0.856    |\n",
      "|    ent_coef_loss | -0.5     |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 527      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5270, episode_reward=-142.74 +/- 43.82\n",
      "Episode length: 3.30 +/- 2.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5270     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 51.9     |\n",
      "|    critic_loss   | 3.11e+03 |\n",
      "|    ent_coef      | 0.856    |\n",
      "|    ent_coef_loss | -0.521   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 528      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5280, episode_reward=-150.09 +/- 37.17\n",
      "Episode length: 3.60 +/- 2.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5280     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 49.7     |\n",
      "|    critic_loss   | 2.3e+03  |\n",
      "|    ent_coef      | 0.855    |\n",
      "|    ent_coef_loss | -0.511   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 529      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5290, episode_reward=-120.52 +/- 58.11\n",
      "Episode length: 8.20 +/- 6.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5300, episode_reward=-97.82 +/- 60.27\n",
      "Episode length: 6.80 +/- 6.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | -97.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.3     |\n",
      "|    critic_loss     | 2.79e+03 |\n",
      "|    ent_coef        | 0.855    |\n",
      "|    ent_coef_loss   | -0.492   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 530      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5310, episode_reward=-159.18 +/- 34.93\n",
      "Episode length: 3.70 +/- 3.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.5     |\n",
      "|    critic_loss     | 3.61e+03 |\n",
      "|    ent_coef        | 0.855    |\n",
      "|    ent_coef_loss   | -0.506   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 531      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 46.8     |\n",
      "|    critic_loss   | 2.41e+03 |\n",
      "|    ent_coef      | 0.855    |\n",
      "|    ent_coef_loss | -0.494   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 532      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5320, episode_reward=-184.70 +/- 46.24\n",
      "Episode length: 1.90 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.9      |\n",
      "|    mean_reward     | -185     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5330, episode_reward=-120.49 +/- 92.72\n",
      "Episode length: 7.10 +/- 7.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.9     |\n",
      "|    critic_loss     | 2.31e+03 |\n",
      "|    ent_coef        | 0.854    |\n",
      "|    ent_coef_loss   | -0.498   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 533      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 49.9     |\n",
      "|    critic_loss   | 2.6e+03  |\n",
      "|    ent_coef      | 0.854    |\n",
      "|    ent_coef_loss | -0.509   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 534      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5340, episode_reward=-107.67 +/- 48.58\n",
      "Episode length: 2.90 +/- 2.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5340     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 46.7     |\n",
      "|    critic_loss   | 1.97e+03 |\n",
      "|    ent_coef      | 0.854    |\n",
      "|    ent_coef_loss | -0.492   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 535      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5350, episode_reward=-118.54 +/- 75.84\n",
      "Episode length: 7.10 +/- 7.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5360, episode_reward=-133.30 +/- 67.71\n",
      "Episode length: 4.60 +/- 5.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.3     |\n",
      "|    critic_loss     | 1.72e+03 |\n",
      "|    ent_coef        | 0.854    |\n",
      "|    ent_coef_loss   | -0.517   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 536      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5370, episode_reward=-108.09 +/- 42.49\n",
      "Episode length: 7.00 +/- 5.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.5     |\n",
      "|    critic_loss     | 1e+03    |\n",
      "|    ent_coef        | 0.853    |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 537      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5380, episode_reward=-121.21 +/- 47.49\n",
      "Episode length: 3.50 +/- 4.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.4     |\n",
      "|    critic_loss     | 1.7e+03  |\n",
      "|    ent_coef        | 0.853    |\n",
      "|    ent_coef_loss   | -0.515   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 538      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 37.5     |\n",
      "|    critic_loss   | 1.62e+03 |\n",
      "|    ent_coef      | 0.853    |\n",
      "|    ent_coef_loss | -0.511   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 539      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5390, episode_reward=-110.12 +/- 47.12\n",
      "Episode length: 3.80 +/- 3.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5400, episode_reward=-103.02 +/- 43.34\n",
      "Episode length: 6.80 +/- 5.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.9     |\n",
      "|    critic_loss     | 1.47e+03 |\n",
      "|    ent_coef        | 0.853    |\n",
      "|    ent_coef_loss   | -0.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 540      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 45.9     |\n",
      "|    critic_loss   | 1.44e+03 |\n",
      "|    ent_coef      | 0.852    |\n",
      "|    ent_coef_loss | -0.521   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 541      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5410, episode_reward=-103.48 +/- 38.38\n",
      "Episode length: 5.90 +/- 5.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5420, episode_reward=-151.95 +/- 96.43\n",
      "Episode length: 4.70 +/- 4.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39       |\n",
      "|    critic_loss     | 1.55e+03 |\n",
      "|    ent_coef        | 0.852    |\n",
      "|    ent_coef_loss   | -0.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 542      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5430, episode_reward=-110.68 +/- 82.57\n",
      "Episode length: 9.40 +/- 8.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.4     |\n",
      "|    critic_loss     | 3.8e+03  |\n",
      "|    ent_coef        | 0.852    |\n",
      "|    ent_coef_loss   | -0.505   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 543      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5440, episode_reward=-71.82 +/- 46.24\n",
      "Episode length: 11.40 +/- 7.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | -71.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 53       |\n",
      "|    critic_loss     | 2.07e+03 |\n",
      "|    ent_coef        | 0.852    |\n",
      "|    ent_coef_loss   | -0.511   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 544      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5450, episode_reward=-102.89 +/- 77.89\n",
      "Episode length: 6.70 +/- 5.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.9     |\n",
      "|    critic_loss     | 1.42e+03 |\n",
      "|    ent_coef        | 0.851    |\n",
      "|    ent_coef_loss   | -0.497   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 545      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 50.2     |\n",
      "|    critic_loss   | 1.39e+03 |\n",
      "|    ent_coef      | 0.851    |\n",
      "|    ent_coef_loss | -0.51    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 546      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5460, episode_reward=-119.20 +/- 56.15\n",
      "Episode length: 6.00 +/- 4.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5470, episode_reward=-158.80 +/- 26.43\n",
      "Episode length: 4.70 +/- 3.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 51.5     |\n",
      "|    critic_loss     | 2.26e+03 |\n",
      "|    ent_coef        | 0.851    |\n",
      "|    ent_coef_loss   | -0.507   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 547      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 51.1     |\n",
      "|    critic_loss   | 1.51e+03 |\n",
      "|    ent_coef      | 0.851    |\n",
      "|    ent_coef_loss | -0.503   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 548      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5480, episode_reward=-116.28 +/- 57.84\n",
      "Episode length: 5.20 +/- 3.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5490, episode_reward=-150.62 +/- 43.46\n",
      "Episode length: 2.40 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.4      |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 55.5     |\n",
      "|    critic_loss     | 3.32e+03 |\n",
      "|    ent_coef        | 0.85     |\n",
      "|    ent_coef_loss   | -0.497   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-160.01 +/- 46.47\n",
      "Episode length: 5.10 +/- 3.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 55.5     |\n",
      "|    critic_loss     | 4.19e+03 |\n",
      "|    ent_coef        | 0.85     |\n",
      "|    ent_coef_loss   | -0.497   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 550      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 54.9     |\n",
      "|    critic_loss   | 2.32e+03 |\n",
      "|    ent_coef      | 0.85     |\n",
      "|    ent_coef_loss | -0.51    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 551      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5510, episode_reward=-114.42 +/- 65.14\n",
      "Episode length: 8.90 +/- 8.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.9      |\n",
      "|    mean_reward     | -114     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5510     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 50.8     |\n",
      "|    critic_loss   | 1.72e+03 |\n",
      "|    ent_coef      | 0.85     |\n",
      "|    ent_coef_loss | -0.543   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 552      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5520, episode_reward=-135.49 +/- 70.69\n",
      "Episode length: 7.50 +/- 6.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5520     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 50.6     |\n",
      "|    critic_loss   | 2.52e+03 |\n",
      "|    ent_coef      | 0.849    |\n",
      "|    ent_coef_loss | -0.51    |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 553      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5530, episode_reward=-128.60 +/- 62.12\n",
      "Episode length: 3.80 +/- 2.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5530     |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 54       |\n",
      "|    critic_loss   | 1.85e+03 |\n",
      "|    ent_coef      | 0.849    |\n",
      "|    ent_coef_loss | -0.527   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 554      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5540, episode_reward=-152.96 +/- 39.25\n",
      "Episode length: 5.80 +/- 4.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5550, episode_reward=-125.05 +/- 76.20\n",
      "Episode length: 6.70 +/- 4.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 59.3     |\n",
      "|    critic_loss     | 2.39e+03 |\n",
      "|    ent_coef        | 0.849    |\n",
      "|    ent_coef_loss   | -0.515   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 555      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5560, episode_reward=-155.94 +/- 59.13\n",
      "Episode length: 3.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.1     |\n",
      "|    critic_loss     | 3.87e+03 |\n",
      "|    ent_coef        | 0.849    |\n",
      "|    ent_coef_loss   | -0.534   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 556      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 47.5     |\n",
      "|    critic_loss   | 2.46e+03 |\n",
      "|    ent_coef      | 0.848    |\n",
      "|    ent_coef_loss | -0.514   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 557      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5570, episode_reward=-119.21 +/- 65.28\n",
      "Episode length: 6.40 +/- 4.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5580, episode_reward=-120.12 +/- 66.08\n",
      "Episode length: 5.90 +/- 6.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46       |\n",
      "|    critic_loss     | 4.21e+03 |\n",
      "|    ent_coef        | 0.848    |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 558      |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| train/           |          |\n",
      "|    actor_loss    | 46       |\n",
      "|    critic_loss   | 1.67e+03 |\n",
      "|    ent_coef      | 0.848    |\n",
      "|    ent_coef_loss | -0.506   |\n",
      "|    learning_rate | 0.0003   |\n",
      "|    n_updates     | 559      |\n",
      "-------------------------------\n",
      "Eval num_timesteps=5590, episode_reward=-134.86 +/- 40.16\n",
      "Episode length: 4.30 +/- 3.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5590     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m s\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:281\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 281\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\anaconda\\envs\\RL\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from stable_baselines3 import PPO,SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.commom.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import safe_mean\n",
    "\n",
    "env_raw = CustomEnv()\n",
    "save_path = \"logs/best_model_sac\"\n",
    "i=1\n",
    "while os.path.exists(save_path+str(i)):\n",
    "    i+=1\n",
    "save_path = f\"{save_path}{i}\"\n",
    "os.makedirs(save_path)\n",
    "\n",
    "tensorboard_dir = \"./sac_custom_env_tensorboard\"\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "tensorboard_log_dir = tensorboard_dir + '/'+ f'best_model_sac{i}'\n",
    "load_model = \"logs/best_model_sac44/best_model.zip\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env =DummyVecEnv([lambda: Monitor(env_raw)])  # Wrap the environment with Monitor for logging\n",
    "\n",
    "# env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128,256,256,128], qf=[128,256,256,128])],\n",
    "    # activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[128,256,256,128],\n",
    "    # activation_fn=torch.nn.ReLU  # 改为 ReLU，通常更适合稀疏奖励\n",
    ")\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1,policy_kwargs=policy_kwargs,tensorboard_log=\"./ppo_custom_env_tensorboard/\")\n",
    "# model = PPO.load(\"logs/best_model4/best_model.zip\",env=env)  # Load the best model\n",
    "# model.learning_rate = 0.0008\n",
    "# model.ent_coef = 0.02  # Set a lower learning rate for fine-tuning\n",
    "\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1,ent_coef='auto',policy_kwargs=policy_kwargs,tensorboard_log=tensorboard_log_dir)\n",
    "replay_buffer = model.replay_buffer\n",
    "# model = SAC.load(load_model, env, verbose=1,ent_coef='auto',learning_rate=0.0001,tensorboard_log=tensorboard_log_dir)\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=save_path,\n",
    "    log_path = './logs/',\n",
    "    eval_freq=10,  # 每1000步评估一次\n",
    "    deterministic=True,\n",
    "    render=True,\n",
    "    n_eval_episodes=10,  # 每次评估5个episode\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "debug_callback = DebugCallback(env=env,log_freq=10000, verbose=1)\n",
    "callback = CallbackList([eval_callback, debug_callback])\n",
    "\n",
    "# model._logger = configure(\"./logs/manual_sac_train/\", [\"stdout\", \"tensorboard\"])\n",
    "# model._current_progress_remaining = 1.0\n",
    "\n",
    "model._setup_learn(total_timesteps=400000,callback=callback)\n",
    "callback.on_training_start(locals(), globals())\n",
    "# model.learn(total_timesteps=400000,callback=callback)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "s = 0\n",
    "info_buffer = deque(maxlen=100)\n",
    "for step in range(1000000):\n",
    "    model.num_timesteps+=1\n",
    "\n",
    "\n",
    "    # get obs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    # next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "    #execute action\n",
    "\n",
    "    # compute reward,done,info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # get next obs\n",
    "\n",
    "\n",
    "    replay_buffer.add(obs,next_obs, action, reward, done, info)\n",
    "\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        model.train(batch_size=64, gradient_steps=1)\n",
    "    s+=1\n",
    "    if done:\n",
    "        model._episode_num += 1\n",
    "        info_buffer.append([s])\n",
    "        s = 0\n",
    "        if model._episode_num % 4 == 0:\n",
    "            # model.dump_logs()\n",
    "            # model.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in model.ep_info_buffer]))\n",
    "            # model.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[0] for ep_info in info_buffer]))\n",
    "            model.logger.dump(step=model.num_timesteps)\n",
    "        # obs = env.reset()\n",
    "        done = False\n",
    "    callback.on_step()\n",
    "callback.on_training_end()\n",
    "# env.save(\"vec_env.pkl\")  # Save the environment state\n",
    "\n",
    "settings = {\n",
    "    'load_model':load_model,\n",
    "    'tensorboard_log' :tensorboard_log_dir,\n",
    "           }\n",
    "env_raw.save_args(save_path)\n",
    "with open(os.path.join(save_path, \"settings.json\"), \"w\") as f:\n",
    "    json.dump(settings, f)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831e9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda\\envs\\RL\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResetting environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mmetric\u001b[49m(state_history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metric' is not defined"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import SAC,PPO\n",
    "# env =DummyVecEnv([lambda: Monitor(CustomEnv())])  # Wrap the environment with Monitor for logging\n",
    "\n",
    "# env = VecNormalize.load('vec_norm.pkl',env)\n",
    "# env = VecNormalize(env)  # Apply normalization to the environment\n",
    "# seed = np.random.randint(0,1000)\n",
    "# from custom_env import CustomEnv\n",
    "env = CustomEnv()\n",
    "env.random = False\n",
    "\n",
    "\n",
    "model= SAC.load(\"logs/best_model_sac59/best_model.zip\",env=env)  # Load the best model\n",
    "\n",
    "obs,_ = env.reset()\n",
    "env.stride_hand = 1\n",
    "env.stride_robot = 1.2\n",
    "state_history = []\n",
    "max_steps = 40000\n",
    "\n",
    "for i in range(max_steps):\n",
    "    obs = env._get_obs()\n",
    "    info = env._get_info()\n",
    "\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, teminated,_, info = env.step(action)\n",
    "    state_history.append(obs)\n",
    "    # print(f\"obs:{obs}\")\n",
    "    # print(f\"action:{action}\")\n",
    "\n",
    "\n",
    "\n",
    "    # print(info[\"robot_position\"],info[\"hand_position\"])\n",
    "    # print(\"Reward:\", reward)\n",
    "    # print(\"distance_arm:\",info['distance_arm'])\n",
    "    # env.render()\n",
    "    env.hand_position = render_environment(info[\"robot_position\"], info[\"hand_position\"],info[\"fix_point\"], trajectory_points=env.trajectory_points)\n",
    "    time.sleep(0.6)  # Control the frame rate\n",
    "    if teminated:\n",
    "        env.hand_position = render_environment(info[\"robot_position\"], info[\"hand_position\"],info[\"fix_point\"], trajectory_points=env.trajectory_points)\n",
    "        time.sleep(0.6)\n",
    "        env.close()\n",
    "        break\n",
    "        print(\"Resetting environment\")\n",
    "metric(state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb9ee739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['grid_size', 'pause', 'domain_randomization', 'render_mode', 'distance_threshold_penalty', 'distance_threshold_collision', 'distance_threshold_arm', 'penalty_factor', 'distance_reward_factor', 'smooth_action_penalty', 'steps', 'margin', 'reward_arm', 'reward_hand', 'reward_bound', 'reward_max_step', 'reward_step', 'stride_robot_random', 'stride_hand_random', 'hand_move_epsilon', 'current_distance', 'max_steps', 'action_space', 'observation_shape', 'observation_space', 'random', 'window', 'clock', 'cell_size', 'trajectory_points', 'dist_arm'])\n",
      "dict_keys(['grid_size', 'pause', 'domain_randomization', 'render_mode', 'distance_threshold_penalty', 'distance_threshold_collision', 'distance_threshold_arm', 'penalty_factor', 'distance_reward_factor', 'smooth_action_penalty', 'steps', 'margin', 'reward_arm', 'reward_hand', 'reward_bound', 'reward_max_step', 'reward_step', 'stride_robot_random', 'stride_hand_random', 'hand_move_epsilon', 'current_distance', 'max_steps', 'action_space', 'observation_shape', 'observation_space', 'random', 'window', 'clock', 'cell_size', 'trajectory_points', 'dist_arm', 'stride_hand'])\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "print(env.__dict__.keys())\n",
    "env.stride_hand = 1\n",
    "print(env.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143107ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b93ec99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric(state_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
